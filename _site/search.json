[
  {
    "objectID": "ewf/ewf-klausurvorbereitung.html",
    "href": "ewf/ewf-klausurvorbereitung.html",
    "title": "EWF Klausurvorbereitung",
    "section": "",
    "text": "Libraries\nlibrary(MASS)\nlibrary(lmtest)\nlibrary(Ecdat)\nlibrary(sur)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(gridExtra)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Klausurvorbereitung"
    ]
  },
  {
    "objectID": "ewf/ewf-klausurvorbereitung.html#einfache-regression",
    "href": "ewf/ewf-klausurvorbereitung.html#einfache-regression",
    "title": "EWF Klausurvorbereitung",
    "section": "Einfache Regression",
    "text": "Einfache Regression\n\nn &lt;- 40\nquadratmeter &lt;- seq(n)\npreis &lt;- rnorm(n,0+3*quadratmeter, 40)\ndata &lt;- data.frame(quadratmeter, preis)\n\na1 &lt;- lm(preis~quadratmeter)\nsummary(a1)\n\n\nCall:\nlm(formula = preis ~ quadratmeter)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-66.360 -33.446  -2.759  32.469  69.842 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -2.017     11.811  -0.171    0.865    \nquadratmeter    3.447      0.502   6.866 3.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.65 on 38 degrees of freedom\nMultiple R-squared:  0.5537,    Adjusted R-squared:  0.542 \nF-statistic: 47.15 on 1 and 38 DF,  p-value: 3.732e-08\n\ndata %&gt;% ggplot(aes(quadratmeter, preis))+\n  geom_point()+\n  stat_function(fun = function(x)  {a1$coefficients[1] + a1$coefficients[2]*x})",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Klausurvorbereitung"
    ]
  },
  {
    "objectID": "ewf/ewf-klausurvorbereitung.html#heteroskedastizität",
    "href": "ewf/ewf-klausurvorbereitung.html#heteroskedastizität",
    "title": "EWF Klausurvorbereitung",
    "section": "Heteroskedastizität",
    "text": "Heteroskedastizität\nGenerierung von Heteroskedastizität und Verteilung der Residuen\n\nn2 &lt;- 10000\nx &lt;- runif(n2, min = 0, max = 10)\ny &lt;- rnorm(n2, mean = 50+3*x, sd = 5+2*x)\ndf1 &lt;- data.frame(x, y)\ndf1 %&gt;%\n  ggplot(aes(x, y))+\n  geom_point(alpha = .5, size = .5)\n\n\n\n\n\n\n\na2 &lt;- lm(y~x)\nsummary(a2)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-73.393  -9.141   0.006   9.313  82.967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.07870    0.32421  154.46   &lt;2e-16 ***\nx            3.00503    0.05591   53.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.17 on 9998 degrees of freedom\nMultiple R-squared:  0.2242,    Adjusted R-squared:  0.2241 \nF-statistic:  2889 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\na2 %&gt;% ggplot(aes(x=a2$residuals))+\n  geom_density()+\n  stat_function(fun = function(x){dnorm(x, mean = mean(a2$residuals), sd = sd(a2$residuals))}, linetype = \"dashed\")\n\n\n\n\n\n\n\n\nDie Residuen (durchgezogene Linie) sind nicht normalverteilt (gestrichelte Linie)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Klausurvorbereitung"
    ]
  },
  {
    "objectID": "ewf/ewf-klausurvorbereitung.html#beispiel-einer-fehlehnden-binärvariable",
    "href": "ewf/ewf-klausurvorbereitung.html#beispiel-einer-fehlehnden-binärvariable",
    "title": "EWF Klausurvorbereitung",
    "section": "Beispiel einer fehlehnden Binärvariable",
    "text": "Beispiel einer fehlehnden Binärvariable\n\nn &lt;- 500\ndata &lt;- data.frame(x = runif(n, min = 0, max = 10),\n           group = c(1,2)) %&gt;%\n  mutate(group = as.factor(group),\n         y = if_else(group == 1, \n                     5 + 3*x + rnorm(n, 0, 2), \n                     20 + 3*x + rnorm(n, 0, 2)))\n\ndata %&gt;%\n  ggplot(aes(x = x, y = y, color = group))+\n  geom_point()\n\n\n\n\n\n\n\nreg1 &lt;- lm(y ~ x, data = data)\nreg1\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n     12.577        2.987  \n\ndata.frame(resid = resid(reg1)) %&gt;%\n  ggplot(aes(x = resid))+\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\nreg2 &lt;- lm(y ~ x + group, data = data)\nreg2\n\n\nCall:\nlm(formula = y ~ x + group, data = data)\n\nCoefficients:\n(Intercept)            x       group2  \n      5.006        3.015       14.875  \n\ndata.frame(resid = resid(reg2)) %&gt;%\n  ggplot(aes(x = resid))+\n  geom_histogram(bins = 30)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Klausurvorbereitung"
    ]
  },
  {
    "objectID": "ewf/ewf-klausurvorbereitung.html#chow-test-für-strukturbruch",
    "href": "ewf/ewf-klausurvorbereitung.html#chow-test-für-strukturbruch",
    "title": "EWF Klausurvorbereitung",
    "section": "Chow Test für Strukturbruch",
    "text": "Chow Test für Strukturbruch\nWir generieren zuerst einen Datensatz mit einem Strukturbruch bei \\(x=0\\). Es soll nicht zu offensichtlich sein, damit der F-Test am Ende noch interessant bleibt.\n\nn &lt;- 200\n\nx1 &lt;- runif(n/2, min = -5, max = 0)\nx2 &lt;- runif(n/2, min = 0, max = 5)\n\ny1 &lt;- 5 + 1*x1 + rnorm(n/2, 0, 3)\ny2 &lt;- 5 + 3*x2 + rnorm(n/2, 0, 3)\n\ndata &lt;- data.frame(x = c(x1, x2), y = c(y1, y2))\n\ndata %&gt;%\n  ggplot(aes(x, y))+\n  geom_point()\n\n\n\n\n\n\n\n\nWir sehen, dass eine “naive” lineare Regression hier kein gutes Ergebnis bringt.\n\nreg1 &lt;- lm(y ~ x, data = data)\nsummary(reg1)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8188 -2.0114 -0.0094  2.2146  8.5988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.61879    0.22851   33.34   &lt;2e-16 ***\nx            2.03469    0.07697   26.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.23 on 198 degrees of freedom\nMultiple R-squared:  0.7792,    Adjusted R-squared:  0.7781 \nF-statistic: 698.8 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\ndata %&gt;% ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = reg1$coefficients[1], slope = reg1$coefficients[2])\n\n\n\n\n\n\n\n\n\nreg2.1 &lt;- lm(y ~ x, data = data %&gt;% filter(x &lt; 0))\nreg2.2 &lt;- lm(y ~ x, data = data %&gt;% filter(x &gt; 0))\n\ndata %&gt;% ggplot(aes(x, y))+\n  geom_point()+\n  geom_abline(intercept = reg2.1$coefficients[1], slope = reg2.1$coefficients[2])+\n  geom_abline(intercept = reg2.2$coefficients[1], slope = reg2.2$coefficients[2])\n\n\n\n\n\n\n\n\nDer Chow Test zeigt ganz klar, dass ein Strukturbruch vorliegt.\n\nSSR.R &lt;- sum(resid(reg1)^2)\nSSR.U &lt;- sum(resid(reg2.1)^2) + sum(resid(reg2.2)^2)\n\nFstatistic &lt;- ((SSR.R-SSR.U)/reg1$rank)/(SSR.U/(n - 2*reg1$rank))\nFstatistic\n\n[1] 18.41022\n\n1-pf(Fstatistic, reg1$rank, n - 2*reg1$rank)\n\n[1] 4.710059e-08",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Klausurvorbereitung"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html",
    "href": "ewf/methoden-der-ewf.html",
    "title": "Methoden der EWF",
    "section": "",
    "text": "Die folgenden Packages wurden für das Erstellen dieses Dokuments verwendet:\nCode\nlibrary(pracma)\nlibrary(MASS)\nlibrary(lmtest)\nlibrary(numDeriv)\nlibrary(tidyverse)\nlibrary(L1pack)\nlibrary(ggpubr)\nlibrary(AER)\nlibrary(sem)\nlibrary(maxLik)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#gauss-markov-theorem",
    "href": "ewf/methoden-der-ewf.html#gauss-markov-theorem",
    "title": "Methoden der EWF",
    "section": "Gauss-Markov-Theorem",
    "text": "Gauss-Markov-Theorem\nDas Gauss-Markov-Theorem besagt, dass der OLS Schätzer \\(\\beta=(X^TX)^{-1}X^Ty\\) der best linear unbiased estimator (BLUE) ist. Das bedeutet, es gibt keinen anderen Schätzer in dieser Familie, welcher eine geringere Varianz aufweißt.\nACHTUNG: Der hier genutzte Least Absolute Deviations (LAD) Schätzer gehört nicht zu den linearen Schätzern und sollte daher eigentlich nicht zum Vergleich genutzt werden. Die folgenden Simulationen dienen also nur dazu, das Verhalten von OLS zu LAD bei verschiedenen Verteilungen der Residuen zu zeigen. Gleichzeitig ist auch zu erkennen, dass OLS nicht immer der beste Schätzer ist (auch wenn es manchmal so aussieht).\n\nNormalverteilte ResiduenExponentialverteilte Residuent-Verteilte Residuen\n\n\nSimulation mit normalverteilten Residuen.\n\n\nCode\nn &lt;- 100\nx &lt;- seq(from = 0, to = 10, length.out = n)\nn_runs &lt;- 10000\nbeta &lt;- cbind(numeric(n_runs), numeric(n_runs))\nfor(i in 1:n_runs){\n  u &lt;- rnorm(n, mean = 0, sd = 2)\n  y &lt;- x + u\n  \n  R_OLS &lt;- lm(y ~ x)\n  beta[i, 1] &lt;- R_OLS$coefficients[2]\n  \n  R_LAD &lt;- lad(y ~ x)\n  beta[i, 2] &lt;- R_LAD$coefficients[2]\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"OLS\" = X1, \"LAD\" = X2) %&gt;%\n  pivot_longer(cols = OLS:LAD) %&gt;%\n  ggplot(aes(x = value, color = name))+\n  geom_density()+\n  geom_vline(xintercept = 1, linetype = \"dashed\")+\n  coord_cartesian(xlim = c(.5,1.5), ylim = c(0, 8))+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nSimlation mit exponentialverteilten und mittelwertzentrierten Residuen.\n\n\nCode\nn &lt;- 100\nx &lt;- seq(from = 0, to = 10, length.out = n)\nn_runs &lt;- 10000\nbeta &lt;- cbind(numeric(n_runs), numeric(n_runs))\nfor(i in 1:n_runs){\n  u &lt;- scale(rexp(n, rate = .5), scale = FALSE)\n  y &lt;- x + u\n  \n  R_OLS &lt;- lm(y ~ x)\n  beta[i, 1] &lt;- R_OLS$coefficients[2]\n  \n  R_LAD &lt;- lad(y ~ x)\n  beta[i, 2] &lt;- R_LAD$coefficients[2]\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"OLS\" = X1, \"LAD\" = X2) %&gt;%\n  pivot_longer(cols = OLS:LAD) %&gt;%\n  ggplot(aes(x = value, color = name))+\n  geom_density()+\n  geom_vline(xintercept = 1, linetype = \"dashed\")+\n  coord_cartesian(xlim = c(.5,1.5), ylim = c(0, 8))+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nSimulation mit t-verteilten Residuen.\n\n\nCode\nn &lt;- 100\nx &lt;- seq(from = 0, to = 10, length.out = n)\nn_runs &lt;- 10000\nbeta &lt;- cbind(numeric(n_runs), numeric(n_runs))\nfor(i in 1:n_runs){\n  u &lt;- rt(n, 2)\n  y &lt;- x + u\n  \n  R_OLS &lt;- lm(y ~ x)\n  beta[i, 1] &lt;- R_OLS$coefficients[2]\n  \n  R_LAD &lt;- lad(y ~ x)\n  beta[i, 2] &lt;- R_LAD$coefficients[2]\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"OLS\" = X1, \"LAD\" = X2) %&gt;%\n  pivot_longer(cols = OLS:LAD) %&gt;%\n  ggplot(aes(x = value, color = name))+\n  geom_density()+\n  geom_vline(xintercept = 1, linetype = \"dashed\")+\n  coord_cartesian(xlim = c(.5,1.5), ylim = c(0, 8))+\n  theme_bw()+\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#konsistenz",
    "href": "ewf/methoden-der-ewf.html#konsistenz",
    "title": "Methoden der EWF",
    "section": "Konsistenz",
    "text": "Konsistenz\n\nKonsistenter, erwartungstreuer SchätzerKonsistenter, verzerrter SchätzerInkonsistenter, erwartungstreuer SchätzerInkonsistenter, verzerrter Schätzer\n\n\n\n\nCode\nns &lt;- c(10, 100, 500)\nn_runs &lt;- 10000\nbeta &lt;- matrix(rep(0, length(ns) * n_runs), ncol = length(ns))\n\nfor (j in 1:length(ns)) {\n  n &lt;- ns[j]\n  x &lt;- seq(from = 0, to = 10, length.out = n)\n  \n  for (i in 1:n_runs) {\n    u &lt;- rnorm(n, mean = 0, sd = 3)\n    y &lt;- 5 + 2 * x + u\n    \n    beta[i, j] &lt;- coef(lm(y ~ x))[2]\n  }\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"10\" = X1, \"100\" = X2, \"500\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Anzahl_Beobachtungen\", values_to = \"beta\") %&gt;%\n  ggplot(aes(x = beta, color = Anzahl_Beobachtungen))+\n  geom_density()+\n  geom_vline(xintercept = 2, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\")+\n  labs(color = \"Anzahl Beobachtungen\")\n\n\n\n\n\n\n\n\n\nEin erwartungstreuer und konsistenter Schätzer ist sozusagen der Musterschüler unter den Schätzern: Selbst für kleine Stichproben liefert er im Mittel den wahren Wert und mit größeren Stichproben sinkt noch die Streuung: \\(E(\\hat{\\beta}) = \\beta\\) und \\(Var(\\hat{\\beta}) = \\frac{\\sigma^2}{n} = 0\\) für \\(n \\rightarrow \\infty\\)\n\n\n\n\nCode\nns &lt;- c(10, 100, 500)\nn_runs &lt;- 10000\nbeta &lt;- matrix(rep(0, length(ns) * n_runs), ncol = length(ns))\n\nfor (j in 1:length(ns)) {\n  n &lt;- ns[j]\n  x &lt;- seq(from = 0, to = 10, length.out = n)\n  \n  for (i in 1:n_runs) {\n    u &lt;- rnorm(n, mean = 0, sd = 3)\n    y &lt;- 5 + 2 * x + u\n    \n    beta[i, j] &lt;- coef(lm(y ~ x))[2] - 5/n\n  }\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"10\" = X1, \"100\" = X2, \"500\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Anzahl_Beobachtungen\", values_to = \"beta\") %&gt;%\n  ggplot(aes(x = beta, color = Anzahl_Beobachtungen))+\n  geom_density()+\n  geom_vline(xintercept = 2, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\")+\n  labs(color = \"Anzahl Beobachtungen\")\n\n\n\n\n\n\n\n\n\nEin verzerrter, aber konsistenter Schätzer hat zwar einen verzerrten Erwartungswert, welcher aber gegen den wahren Erwartungswert strebt: \\(E(\\hat{\\beta}) = \\beta - \\frac{5}{n} = \\beta\\) für \\(n \\rightarrow \\infty\\). Zusätzlich geht auch die Varianz gegen Null: \\(Var(\\hat{\\beta}) = \\frac{\\sigma^2}{n} = 0\\) für \\(n \\rightarrow \\infty\\).\n\n\n\n\nCode\nns &lt;- c(10, 20, 30)\nn_runs &lt;- 10000\nbeta &lt;- matrix(rep(0, length(ns) * n_runs), ncol = length(ns))\n\nfor (j in 1:length(ns)) {\n  n &lt;- ns[j]\n  x &lt;- runif(n, min = 0, max = 10)\n  \n  for (i in 1:n_runs) {\n    u &lt;- rnorm(n, mean = 0, sd = 3)\n    y &lt;- 0 + u\n    \n    beta[i, j] &lt;- y[1]\n  }\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"10\" = X1, \"20\" = X2, \"30\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Anzahl_Beobachtungen\", values_to = \"beta\") %&gt;%\n  ggplot(aes(x = beta, color = Anzahl_Beobachtungen))+\n  geom_density()+\n  geom_vline(xintercept = 0, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\")+\n  labs(color = \"Anzahl Beobachtungen\")\n\n\n\n\n\n\n\n\n\nEin Beispiel für einen nicht-konsistenten (aber erwartungstruen) Mittelwertschätzer ist \\(\\bar{y} = y_1\\), es wird also einfach die erste Beobachtung als der Mittelwert angenommen. Logischerweise ändert sich die Varianz dieses Schätzers mit zunehmend großem Stichprobenumfang nicht: \\(Var(\\hat{\\beta}) = \\sigma^2\\) auch für \\(n \\rightarrow \\infty\\).\n\n\n\n\nCode\nns &lt;- c(10, 20, 30)\nn_runs &lt;- 10000\nbeta &lt;- matrix(rep(0, length(ns) * n_runs), ncol = length(ns))\n\nfor (j in 1:length(ns)) {\n  n &lt;- ns[j]\n  x &lt;- runif(n, min = 0, max = 10)\n  \n  for (i in 1:n_runs) {\n    u &lt;- rnorm(n, mean = 0, sd = 3)\n    y &lt;- 0 + u\n    \n    beta[i, j] &lt;- y[1] + 5\n  }\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"10\" = X1, \"20\" = X2, \"30\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Anzahl_Beobachtungen\", values_to = \"beta\") %&gt;%\n  ggplot(aes(x = beta, color = Anzahl_Beobachtungen))+\n  geom_density()+\n  geom_vline(xintercept = 0, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\")+\n  labs(color = \"Anzahl Beobachtungen\")\n\n\n\n\n\n\n\n\n\nEin Beispiel für einen nicht-konsistenten (aber erwartungstruen) Mittelwertschätzer ist \\(\\bar{y} = y_1 + 5\\), es wird also einfach die erste Beobachtung + 5 als der Mittelwert angenommen. Logischerweise ändert sich die Varianz dieses Schätzers mit zunehmend großem Stichprobenumfang nicht.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#skalierung-des-schätzers",
    "href": "ewf/methoden-der-ewf.html#skalierung-des-schätzers",
    "title": "Methoden der EWF",
    "section": "Skalierung des Schätzers",
    "text": "Skalierung des Schätzers\nUm die Konsistenz eines Schätzers besser zu zeigen, kann dieser mit \\(\\sqrt{n}\\) multipliziert werden. Dadurch bleibt die Varianz mit steigendem Stichprobenumfang konstant.\n\n\nCode\nns &lt;- c(10, 100, 500)\nn_runs &lt;- 10000\nbeta &lt;- matrix(rep(0, length(ns) * n_runs), ncol = length(ns))\nbeta_scaled &lt;- matrix(rep(0, length(ns) * n_runs), ncol = length(ns))\nbeta_true &lt;- 5\n\nfor (j in 1:length(ns)) {\n  n &lt;- ns[j]\n  \n  for (i in 1:n_runs) {\n    u &lt;- rnorm(n, mean = 0, sd = 3)\n    y &lt;- beta_true + u\n    \n    beta[i, j] &lt;- (coef(lm(y ~ 1))[1] - beta_true)\n    beta_scaled[i, j] &lt;- (coef(lm(y ~ 1))[1] - beta_true) * sqrt(n)\n  }\n}\n\ndata &lt;- beta %&gt;%\n  data.frame() %&gt;%\n  rename(\"10\" = X1, \"100\" = X2, \"500\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Anzahl_Beobachtungen\", values_to = \"beta\") %&gt;%\n  mutate(is_scaled = \"nicht skaliert\")\n\n\ndata_scaled &lt;- beta_scaled %&gt;%\n  data.frame() %&gt;%\n  rename(\"10\" = X1, \"100\" = X2, \"500\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Anzahl_Beobachtungen\", values_to = \"beta\") %&gt;%\n  mutate(is_scaled = \"skaliert\")\n\nbind_rows(data, data_scaled) %&gt;%\n  ggplot(aes(x = beta, color = Anzahl_Beobachtungen))+\n  geom_density()+\n  facet_wrap(~is_scaled, scales = \"free\")+\n  theme_bw()+\n  theme(legend.position = \"top\")+\n  labs(color = \"Anzahl Beobachtungen\")",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#beispiel-normalverteilung",
    "href": "ewf/methoden-der-ewf.html#beispiel-normalverteilung",
    "title": "Methoden der EWF",
    "section": "Beispiel: Normalverteilung",
    "text": "Beispiel: Normalverteilung\nFür eine Schätzung der Parameter einer Normalverteilung wird zuerst die Dichtefunktion aufgestellt \\[f(y_i | \\mu, \\sigma) = (2\\pi \\sigma^2)^{-1/2} \\text{exp}(-\\frac{1}{2}((y_i - \\mu)/\\sigma)^2)\\] und aus dieser ergibt sich dann die Dichte der Stichprobe \\[f(y_1, …, y_n | \\mu, \\sigma) = \\prod_{i=1}^n f(y_i | \\mu, \\sigma) = (2\\pi \\sigma^2)^{-1/2} \\text{exp}(-\\frac{1}{2} \\sum_{i=1}^n ((y_i - \\mu)/\\sigma)^2)\\] Da das Produkt schnell zu sehr niedrigen Werten führen kann, wird stattdessen der Logarithmus verwendet \\[LL(\\mu, \\sigma | y_1, …, y_n) = -\\frac{n}{2} \\ln(2\\pi) - n \\ln \\sigma - \\frac{1}{2}\\sigma^2 \\sum_{i=1}^n(y_i-\\mu)^2\\] Für die numerische Optimierung in R wird die optim() Funktion verwendet, welche ein Minimum sucht. Daher wird die negative Log-Likelihood Funktion verwendet.\n\n\nCode\nn &lt;- 1000\ny &lt;- rnorm(n, mean = 7, sd = 4)\n\nlog_L &lt;- function(theta, y) {\n  mu &lt;- theta[1]\n  sigma &lt;- theta[2]\n  -(n/2) * log(2*pi) - n * log(sigma) - (1/2) * sigma^(-2) * sum((y - mu)^2)\n}\n\nR &lt;- optim(par = c(mu = 5, sigma = 5), fn = log_L, y = y, control = list(fnscale = -1))\nR$par\n\n\n      mu    sigma \n7.020372 3.948059",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#lineare-regression",
    "href": "ewf/methoden-der-ewf.html#lineare-regression",
    "title": "Methoden der EWF",
    "section": "Lineare Regression",
    "text": "Lineare Regression\nEin Vorteil von Maximum Likelihood im Vergleich zu OLS ist, dass die Varianz direkt mitgeschätzt wird.\nDie Dichte einer Beobachtung in einem Linearen Regressionsmodell ergibt sich zu \\(y \\sim N(\\mathbf{x}_i^\\prime \\mathbf{\\beta}, \\sigma^2)\\). Daraus wird die (Log)-Likelihood Funktion aufgestellt.\n\n\nCode\nn &lt;- 100\nn_runs &lt;- 1000\nbeta &lt;- matrix(numeric(n_runs*3), ncol = 3)\n\nfor(i in 1:n_runs){\n  x &lt;- runif(n, min = 0, max = 10)\n  X &lt;- cbind(1, x)\n  y &lt;- 2 + 4*x + rnorm(n, mean = 0, sd = 2)\n  \n  LL &lt;- function(param) {\n      beta &lt;- param[1:2]\n      sigma &lt;- param[3]\n      -n/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((y - X %*% beta)^2)\n  }\n  \n  beta_start &lt;- c(1,1)\n  sigma_start &lt;- 1\n  R &lt;- maxLik(LL, start = c(beta_start, sigma_start))\n  beta[i,] &lt;- coef(R)\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"beta_1\" = X1, \"beta_2\" = X2, \"sigma\" = X3) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  ggplot(aes(x = value))+\n  geom_density()+\n  facet_wrap(~name, scales = \"free_x\")+\n  theme_bw()",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#likelihood-vs-log-likelihood",
    "href": "ewf/methoden-der-ewf.html#likelihood-vs-log-likelihood",
    "title": "Methoden der EWF",
    "section": "Likelihood vs Log-Likelihood",
    "text": "Likelihood vs Log-Likelihood\n\nn = 2n = 3n = 4n = 5\n\n\n\n\nCode\nset.seed(123)\nn &lt;- 2\nmu &lt;- 2\nsigma &lt;- 1.5\ndata &lt;- rnorm(n, mean = mu, sd = sigma)\n\ninit_params &lt;- c(mu = 0, sigma = 1)\n\nopt_likelihood &lt;- optim(par = init_params, fn = likelihood, data = data)\ncat(\"Likelihood: \", \"Funktionswert =\", opt_likelihood$value, \" mu = \", opt_likelihood$par[1], \" sigma = \", opt_likelihood$par[2], \"\\n\")\n\n\nLikelihood:  Funktionswert = -3.641176  mu =  1.40701  sigma =  0.1751672 \n\n\nCode\nopt_log_likelihood &lt;- optim(par = init_params, fn = log_likelihood, data = data)\ncat(\"Log-Likelihood: \", \"Funktionswert =\", opt_log_likelihood$value, \" mu = \", opt_log_likelihood$par[1], \" sigma = \", opt_log_likelihood$par[2], \"\\n\")\n\n\nLog-Likelihood:  Funktionswert = 0.04699389  mu =  1.407028  sigma =  0.2477029 \n\n\n\n\n\n\nCode\nset.seed(123)\nn &lt;- 3\nmu &lt;- 2\nsigma &lt;- 1.5\ndata &lt;- rnorm(n, mean = mu, sd = sigma)\n\ninit_params &lt;- c(mu = 0, sigma = 1)\n\nopt_likelihood &lt;- optim(par = init_params, fn = likelihood, data = data)\ncat(\"Likelihood: \", \"Funktionswert =\", opt_likelihood$value, \" mu = \", opt_likelihood$par[1], \" sigma = \", opt_likelihood$par[2], \"\\n\")\n\n\nLikelihood:  Funktionswert = -1.975685e-05  mu =  2.384028  sigma =  0.8062337 \n\n\nCode\nopt_log_likelihood &lt;- optim(par = init_params, fn = log_likelihood, data = data)\ncat(\"Log-Likelihood: \", \"Funktionswert =\", opt_log_likelihood$value, \" mu = \", opt_log_likelihood$par[1], \" sigma = \", opt_log_likelihood$par[2], \"\\n\")\n\n\nLog-Likelihood:  Funktionswert = 5.258589  mu =  2.383861  sigma =  1.396321 \n\n\n\n\n\n\nCode\nset.seed(123)\nn &lt;- 4\nmu &lt;- 2\nsigma &lt;- 1.5\ndata &lt;- rnorm(n, mean = mu, sd = sigma)\n\ninit_params &lt;- c(mu = 0, sigma = 1)\n\nopt_likelihood &lt;- optim(par = init_params, fn = likelihood, data = data)\ncat(\"Likelihood: \", \"Funktionswert =\", opt_likelihood$value, \" mu = \", opt_likelihood$par[1], \" sigma = \", opt_likelihood$par[2], \"\\n\")\n\n\nLikelihood:  Funktionswert = -3.995138e-07  mu =  2.314456  sigma =  0.6076679 \n\n\nCode\nopt_log_likelihood &lt;- optim(par = init_params, fn = log_likelihood, data = data)\ncat(\"Log-Likelihood: \", \"Funktionswert =\", opt_log_likelihood$value, \" mu = \", opt_log_likelihood$par[1], \" sigma = \", opt_log_likelihood$par[2], \"\\n\")\n\n\nLog-Likelihood:  Funktionswert = 6.455843  mu =  2.314621  sigma =  1.215455 \n\n\n\n\n\n\nCode\nset.seed(123)\nn &lt;- 5\nmu &lt;- 2\nsigma &lt;- 1.5\ndata &lt;- rnorm(n, mean = mu, sd = sigma)\n\ninit_params &lt;- c(mu = 0, sigma = 1)\n\nopt_likelihood &lt;- optim(par = init_params, fn = likelihood, data = data)\ncat(\"Likelihood: \", \"Funktionswert =\", opt_likelihood$value, \" mu = \", opt_likelihood$par[1], \" sigma = \", opt_likelihood$par[2], \"\\n\")\n\n\nLikelihood:  Funktionswert = -3.375142e-17  mu =  0.1  sigma =  1 \n\n\nCode\nopt_log_likelihood &lt;- optim(par = init_params, fn = log_likelihood, data = data)\ncat(\"Log-Likelihood: \", \"Funktionswert =\", opt_log_likelihood$value, \" mu = \", opt_log_likelihood$par[1], \" sigma = \", opt_log_likelihood$par[2], \"\\n\")\n\n\nLog-Likelihood:  Funktionswert = 7.516858  mu =  2.290556  sigma =  1.088041",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#hypothesentests",
    "href": "ewf/methoden-der-ewf.html#hypothesentests",
    "title": "Methoden der EWF",
    "section": "Hypothesentests",
    "text": "Hypothesentests\nAls Beispiel wird hier der Parameter einer Exponentialverteilung geschätzt. Diese bietet sich an, da nur ein einzelner Parameter geschätzt werden muss (im Gegensatz zu z.B. der Normalverteilung, welche zwei Parameter benötigt) und somit die (Log)-Likelihood Funktion schöner geplottet werden kann.\nAls Parameter wird \\(\\theta = 0.5\\) verwendet. Die Nullhypothese wird \\(H_0: \\theta_0 = 0.6\\) sein.\n\nWald TestLikelihood Ratio TestLagrange Multiplier Test / Score Test\n\n\nDer Wald Test betrachtet den horizontalen Abstand zwischen dem geschätzten Parameter(vektor) \\(\\hat{\\theta}\\) und dem Parameter(vektor) unter der Nullhypothese \\(\\theta_0\\): \\[t_W = \\frac{\\hat{\\theta}- \\theta_0}{\\sqrt{I(\\hat{\\theta})^{-1}}} \\sim N(0,1)\\] Für einzelne Parameter ist \\(I(\\hat{\\theta})^{-1}\\) gleich der Varianz.\n\n\nCode\nset.seed(4)\n\nn &lt;- 100\nrate &lt;- 0.5\ndata &lt;- rexp(n, rate)\n\n# Schätzen\n\nneg_log_likelihood &lt;- function(rate, data) {\n  -sum(log(dexp(data, rate)))\n}\n\nmle_result &lt;- optim(par = 1, fn = neg_log_likelihood, data = data, method = \"BFGS\", hessian = TRUE)\nestimated_rate &lt;- mle_result$par\n\n# Testen\nnull_value &lt;- 0.6\n\nestimated_se &lt;- sqrt(solve(mle_result$hessian[1, 1]))\nwald_test_statistic &lt;- (estimated_rate - null_value) / estimated_se\n\np_value &lt;- 2 * (1 - pnorm(abs(wald_test_statistic)))\ncat(\"Test Statistic:\", wald_test_statistic, \"\\n\", \"P-value:\", p_value, \"\\n\")\n\n\nTest Statistic: -1.34291 \n P-value: 0.1793012 \n\n\nCode\n# Plotten\nlog_likelihood &lt;- function(rate, data) {\n  sum(log(dexp(data, rate)))\n}\n\nrate_values &lt;- seq(0.4, 0.8, by = 0.01)\nlog_likelihood_values &lt;- sapply(rate_values, function(rate) log_likelihood(rate, data))\nlikelihood_plot &lt;- data.frame(rate = rate_values, log_likelihood = log_likelihood_values)\n\nggplot(likelihood_plot, aes(x = rate, y = log_likelihood)) +\n  geom_line() +\n  labs(x = \"Rate\", y = \"Log-Likelihood\") +\n  geom_vline(xintercept = null_value, linetype = \"dashed\")+\n  annotate(\"text\", x = (null_value + 0.01), y = -170, label = paste(\"Nullhypothese =\", null_value), angle = 90, size = 3)+\n  geom_vline(xintercept = estimated_rate, linetype = \"dotted\")+\n  annotate(\"text\", x = (estimated_rate - 0.01), y = -170, label = paste(\"geschätzter Wert =\", round(estimated_rate, 2)), angle = 90, size = 3)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIn diesem Fall kann die Nullhypothese zu einem Konfidenzlevel von 5% also nicht verworfen werden. Der geschätzte Wert liegt zu nah an dem Wert unter der Nullhypothese und es gibt zu wenige Beobachtungen.\n\n\nDer Likelihood Ratio Test vergleicht den (Log)-Likelihood Wert des geschätzten Modells mit dem Wert unter der Nullhypothese: \\[t_{LR} = 2(LL(\\hat{\\theta}) - LL(\\theta_0)) \\sim \\chi^2(1)\\]\n\n\nCode\nset.seed(4)\n\nn &lt;- 100\nrate &lt;- 0.5\ndata &lt;- rexp(n, rate)\n\n# Schätzen\n\nneg_log_likelihood &lt;- function(rate, data) {\n  -sum(log(dexp(data, rate)))\n}\n\nlog_likelihood &lt;- function(rate, data) {\n  sum(log(dexp(data, rate)))\n}\n\nmle_result &lt;- optim(par = 1, fn = neg_log_likelihood, data = data, method = \"BFGS\", hessian = TRUE)\nestimated_rate &lt;- mle_result$par\n\n# Testen\n\nnull_value &lt;- 0.6\n\nlog_likelihood_full &lt;- log_likelihood(estimated_rate, data)\nlog_likelihood_reduced &lt;- log_likelihood(null_value, data)\nlrt_test_statistic &lt;- 2 * (log_likelihood_full - log_likelihood_reduced)\n\np_value &lt;- 1 - pchisq(lrt_test_statistic, df = 1)\ncat(\"Test Statistic:\", lrt_test_statistic, \"\\n\", \"P-value:\", p_value, \"\\n\")\n\n\nTest Statistic: 1.656631 \n P-value: 0.1980588 \n\n\nCode\n# Plotten\nrate_values &lt;- seq(0.4, 0.8, by = 0.01)\nlog_likelihood_values &lt;- sapply(rate_values, function(rate) log_likelihood(rate, data))\nlikelihood_plot &lt;- data.frame(rate = rate_values, log_likelihood = log_likelihood_values)\n\nggplot(likelihood_plot, aes(x = rate, y = log_likelihood)) +\n  geom_line() +\n  geom_hline(yintercept = -mle_result$value, linetype = \"dotted\")+\n  geom_hline(yintercept = log_likelihood_reduced, linetype = \"dashed\")+\n  annotate(\"text\", y = (-mle_result$value + .3), x = .7, label = paste(\"geschätzter Likelihood-Wert =\", round(-mle_result$value, 2)), size = 3)+\n  annotate(\"text\", y = (log_likelihood_reduced - .3), x = .7, label = paste(\"Likelihood-Wert bei Nullhypothese =\", round(log_likelihood_reduced, 2)), size = 3)+\n  labs(x = \"Rate\", y = \"Log-Likelihood\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nDer Lagrange Multiplier oder Score Test betrachtet die Steigung der (Log)-Likelihood Funktion unter der Nullhypothese: \\[t_{LM} = \\frac{g(\\theta_0)}{I(\\theta_0)} \\sim \\chi^2(1)\\]\n\n\nCode\nset.seed(4)\n\nn &lt;- 100\nrate &lt;- 0.5\n\ndata &lt;- rexp(n, rate)\n\n# Schätzen\nneg_log_likelihood &lt;- function(rate, data) {\n  -sum(log(dexp(data, rate)))\n}\n\nlog_likelihood &lt;- function(rate, data) {\n  sum(log(dexp(data, rate)))\n}\n\nmle_result &lt;- optim(par = 1, fn = neg_log_likelihood, data = data, method = \"BFGS\", hessian = TRUE)\nestimated_rate &lt;- mle_result$par\n\n# Testen\nnull_value &lt;- 0.6\n\ngrad_null &lt;- grad(func = function(rate) log_likelihood(rate, data), x = null_value)\nintercept &lt;- log_likelihood_reduced - grad_null * null_value\n# hier fehlt jetzt die Fisher Informationsmatrix unter der Nullhypothese\n\n\n# Plotten\nrate_values &lt;- seq(0.4, 0.8, by = 0.01)\nlog_likelihood_values &lt;- sapply(rate_values, function(rate) log_likelihood(rate, data))\nlikelihood_plot &lt;- data.frame(rate = rate_values, log_likelihood = log_likelihood_values)\n\nggplot(likelihood_plot, aes(x = rate, y = log_likelihood)) +\n  geom_line() +\n  geom_abline(slope = grad_null, intercept = intercept, linetype = \"dashed\")+\n  annotate(\"text\", y = (intercept+grad_null*0.7) + .3, x = .7, label = paste(\"Steigung =\", round(grad_null, 2)), size = 3, angle = -30)+\n  labs(x = \"Rate\", y = \"Log-Likelihood\") +\n  theme_bw()",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#nonlinear-least-squares",
    "href": "ewf/methoden-der-ewf.html#nonlinear-least-squares",
    "title": "Methoden der EWF",
    "section": "Nonlinear Least Squares",
    "text": "Nonlinear Least Squares\nDas nichtlineare pendant zum OLS Schätzer: \\(\\text{min}_{\\beta}SSR(\\beta) = (\\mathbf{y} - \\mathbf{x}(\\mathbf{\\beta}))^\\prime(\\mathbf{y} - \\mathbf{x}(\\mathbf{\\beta}))\\). Man beachte den Unterschied zu OLS: \\(\\mathbf{y} = \\mathbf{x}(\\mathbf{\\beta}) + \\mathbf{u}\\) vs \\(\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{u}\\).\nNach der Ableitung bleibt das folgende nichtlineare Gleichungssstem: \\(\\mathbf{X}(\\mathbf{\\beta})^\\prime(\\mathbf{y}-\\mathbf{x}(\\mathbf{\\beta}))=\\mathbf{0}\\)\n\n\nCode\nR &lt;- nls(y ~ f(x, a, b), start = list(a = 1, b = 3.5))\nsummary(R)\n\n\n\nFormula: y ~ f(x, a, b)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 1.983842   0.029283   67.75   &lt;2e-16 ***\nb 3.995111   0.004891  816.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2964 on 198 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 1.213e-06\n\n\nCode\ndata.frame(x,y) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point(alpha = .2)+\n  geom_function(fun = function(x) coef(R)[1]*sin(coef(R)[2]*x))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNichtlineare Schätzer, auch NLS, sind sensibel was die Startwerte der numerischen Optimierung angeht. Mehr dazu folgt unten.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#maximum-likelihood-schätzung",
    "href": "ewf/methoden-der-ewf.html#maximum-likelihood-schätzung",
    "title": "Methoden der EWF",
    "section": "Maximum-Likelihood-Schätzung",
    "text": "Maximum-Likelihood-Schätzung\nFür eine ML Schätzung wird die folgende Likelihood Funktion maximiert: \\[L(a, b, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_i - f(x_i, a, b))^2}{2\\sigma^{2}}\\right)\\] Für leichtere Ableitungen wird die Funktion typischerweise log-Transformiert. Da der Logarithmus eine streng monotone Funktion ist, bleiben Extremstellen nach einer log-Transformation gleich. \\[LL(a, b, \\sigma) = \\sum_{i=1}^{n} \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^{2}) - \\frac{(y_i - f(x_i, a, b))^2}{2\\sigma^{2}}\\right)\\] Da die hier für die numerische Optimierung genutzte optim() Funktion ein Minimum sucht, wird die negative LL Funktion verwendet.\n\n\nCode\nneg_log_likelihood &lt;- function(params, x, y) {\n  a &lt;- params[1]\n  b &lt;- params[2]\n  sigma &lt;- params[3]\n  \n  y_pred &lt;- f(x, a, b)\n  \n  neg_ll &lt;- -sum(-0.5*log(2*pi*sigma^2) - 0.5*((y - y_pred)^2)/(sigma^2))\n  #neg_ll &lt;- -sum(dnorm(y, mean = y_pred, sd = sigma, log = TRUE)) # Alternative\n  return(neg_ll)\n}\n\ninit_params &lt;- c(a = 1, b = 3.5, sigma = 0.5)\n\nopt_result &lt;- optim(par = init_params, fn = neg_log_likelihood, x = x, y = y)\nopt_result$par\n\n\n        a         b     sigma \n1.9838290 3.9951124 0.2949199 \n\n\nFür die hier gewählten Startwerte \\(a=1\\) und \\(b=3.5\\) findet der Schätzer die wahren Werte ziemlich gut. Für beispielsweise \\(a=1\\) und \\(b=2\\) jedoch werden falsche Werte gefunden:\n\n\nCode\ninit_params &lt;- c(a = 1, b = 2, sigma = 0.5)\n\nopt_result &lt;- optim(par = init_params, fn = neg_log_likelihood, x = x, y = y)\nopt_result$par\n\n\n        a         b     sigma \n0.1504887 2.4726679 1.4475354 \n\n\nDagegen scheinen verschiedene Startwerte für \\(a\\) kein Problem zu sein. Das liegt wohl daran, dass \\(a\\) in dem Modell linear ist. Ein Beispiel mit \\(a=10\\) und \\(b=3.5\\):\n\n\nCode\ninit_params &lt;- c(a = 10, b = 3.5, sigma = 0.5)\n\nopt_result &lt;- optim(par = init_params, fn = neg_log_likelihood, x = x, y = y)\nopt_result$par\n\n\n        a         b     sigma \n1.9838073 3.9951363 0.2949901 \n\n\nDer folgende Plot zeigt das Problem. Es wird die resultierende negative Log-Likelihood Funktion dargestellt (für \\(a = 2\\) und \\(\\sigma = 0.3\\). Für diese wird von der optim() Funktion ausgehend von den Startwerten numerisch ein Minimum gesucht. Da die Funktion jedoch viele lokale Minima hat, sind die Startwerte essentiell. Startet man beispielsweise von \\(b = 2\\), so wird nur das lokale Minimum bei \\(b \\approx 2.5\\) gefunden.\n\n\nCode\nb_values &lt;- seq(-1, 5, by = 0.01)\nneg_ll_values &lt;- numeric(length(b_values))\na &lt;- 2\nsigma &lt;- 0.3\n\nfor (i in seq_along(b_values)) {\n  neg_ll_values[i] &lt;- neg_log_likelihood(c(a, b_values[i], sigma), x, y)\n}\n\ndata.frame(b_values, neg_ll_values) %&gt;%\n  ggplot(aes(x = b_values, y = neg_ll_values))+\n  geom_line()+\n  geom_vline(xintercept = 4, linetype = \"dashed\")+\n  theme_bw()+\n  labs(x = \"b\",\n       y = \"Negative LL Wert\")",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#gauss-newton-regression",
    "href": "ewf/methoden-der-ewf.html#gauss-newton-regression",
    "title": "Methoden der EWF",
    "section": "Gauss-Newton-Regression",
    "text": "Gauss-Newton-Regression\nDie Gauss-Newton-Regression ist ein iteratives Verfahren zur Schätzung der Parameter in nichtlinearen Regressionsmodellen. Es basiert auf einer linearen Approximation der nichtlinearen Funktion und verwendet die Methode der kleinsten Quadrate, um die Parameter zu schätzen.\n\nModell\nDas nichtlineare Regressionsmodell kann wie folgt definiert werden:\n\\[\ny_i = f(x_i, \\mathbf{\\beta}) + \\epsilon_i\n\\]\nwo \\(y_i\\) die beobachtete abhängige Variable ist, \\(x_i\\) der Vektor der unabhängigen Variablen, \\(\\mathbf{\\beta}\\) der Vektor der zu schätzenden Parameter, \\(f(x_i, \\mathbf{\\beta})\\) die nichtlineare Funktion und \\(\\epsilon_i\\) der Fehlerterm.\nEs wird weiterhin das Beispiel von oben verwendet: \\(y_i = a \\cdot sin(b x_i) + u_i\\) mit \\(a=2\\) und \\(b=4\\).\n\n\nGauss-Newton-Algorithmus\nDer Gauss-Newton-Algorithmus verwendet eine iterative Methode, um die Parameter \\(\\mathbf{\\beta}\\) schrittweise zu verbessern. Der Algorithmus basiert auf einer linearen Approximation der Funktion \\(f(x_i, \\mathbf{\\beta})\\) um die aktuellen Schätzwerte \\(\\mathbf{\\beta}^{(k)}\\). Die lineare Approximation kann mit Hilfe des Taylor-Polynoms erster Ordnung durchgeführt werden: \\[\nf(x_i, \\mathbf{\\beta}) \\approx f(x_i, \\mathbf{\\beta}^{(k)}) + \\mathbf{J} \\cdot (\\mathbf{\\beta} - \\mathbf{\\beta}^{(k)})\n\\]\nwobei \\(\\mathbf{J}\\) die Jacobimatrix der Funktion \\(f(x_i, \\mathbf{\\beta})\\) ist. Die Jacobimatrix enthält die partiellen Ableitungen von \\(f\\) nach den Parametern \\(\\mathbf{\\beta}\\).\nDie Schätzung der Parameter \\(\\mathbf{\\beta}\\) wird dann durch Minimierung der Fehlerquadrate erreicht: \\[\n\\min_{\\mathbf{\\beta}} \\sum_{i=1}^{n} \\left( y_i - f(x_i, \\mathbf{\\beta}^{(k)}) - \\mathbf{J} \\cdot (\\mathbf{\\beta} - \\mathbf{\\beta}^{(k)}) \\right)^2\n\\]\nDer Gauss-Newton-Algorithmus verwendet die Methode der kleinsten Quadrate, um die Parameter \\(\\mathbf{\\beta}\\) zu schätzen, indem er die obige Zielfunktion iterativ minimiert. Die Iteration erfolgt wie folgt:\n\nInitialisierung der Parameter \\(\\mathbf{\\beta}^{(0)}\\)\nBerechnung der Jacobimatrix \\(\\mathbf{J}\\) und des Residuals \\(\\mathbf{r} = \\mathbf{y} - f(\\mathbf{x}, \\mathbf{\\beta}^{(k)})\\)\nBerechnung der Aktualisierung der Parameter \\(\\Delta \\mathbf{\\beta}\\): \\[\n\\Delta \\mathbf{\\beta} = (\\mathbf{J}^T \\cdot \\mathbf{J})^{-1} \\cdot \\mathbf{J}^T \\cdot \\mathbf{r}\n\\]\nAktualisierung der Parameter: \\[\n\\mathbf{\\beta}^{(k+1)} = \\mathbf{\\beta}^{(k)} + \\Delta \\mathbf{\\beta}\n\\]\nWiederholung der Schritte 2-4 bis ein Abbruchkriterium erfüllt ist (z. B. Konvergenz oder maximale Anzahl an Iterationen)\n\n\n\nCode\nset.seed(4)\nn &lt;- 200\na &lt;- 2\nb &lt;- 4\n\n# Definition Funktion und Ableitungen nach a und b\nf &lt;- function(x, a, b) a*sin(b*x)\nf_a &lt;- function(x, a, b) sin(b*x)\nf_b &lt;- function(x, a, b) a*x*sin(b*x)\n\nx &lt;- runif(n, min = 0, max = 5)\ny &lt;- f(x, a, b) + rnorm(n, mean = 0, sd = .3)\n\nbetahat &lt;- c(1, 3.5) # Startwerte\niter &lt;- 0\nrepeat\n{\n beta1 &lt;- betahat[1]\n beta2 &lt;- betahat[2]\n \n J &lt;- cbind(f_a(x, beta1, beta2), f_b(x, beta1, beta2))\n r &lt;- y - f(x, beta1, beta2)\n new_betahat &lt;- betahat + solve(t(J) %*% J) %*% t(J) %*% r\n \n iter &lt;- iter + 1\n \n # Abbruchkriterien\n if (sum((betahat - new_betahat)^2) &lt; 1e-5){\n   cat(\"Abbruch wegen Konvergenz \\n\")\n   break\n }\n if (iter &gt; 50){\n   cat(\"Abbruch wegen Iterationen \\n\")\n   break\n }\n \n betahat &lt;- new_betahat\n} \n\n\nAbbruch wegen Konvergenz \n\n\nCode\nbetahat\n\n\n         [,1]\n[1,] 1.972976\n[2,] 4.024437\n\n\nCode\ndata.frame(x,y) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point(alpha = .2)+\n  geom_function(fun = function(x) betahat[1]*sin(betahat[2]*x))+\n  theme_bw()",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#beispiel-messfehler-in-x",
    "href": "ewf/methoden-der-ewf.html#beispiel-messfehler-in-x",
    "title": "Methoden der EWF",
    "section": "Beispiel: Messfehler in x",
    "text": "Beispiel: Messfehler in x\nDurch Messfehler in der erklärenden Variable \\(x_\\text{observed} = x_\\text{true} + u_x\\) entsteht ein Bias gegen Null in einer einfachen OLS Schätzung. Aushelfen kann dabei ein Instrument \\(w\\). Dieses darf nicht mit den Fehlertermen \\(u_\\text{hat}\\) aus der Regression korrelieren (Validität) und muss mit \\(x_\\text{observed}\\) korrelieren (Relevanz).\nEin Beispiel hierfür wäre\n\n\\(x_\\text{true}\\): Humankapital\n\\(x_\\text{observed}\\): Angabe zu Schuljahren, mit “Messfehlern”\n\\(y\\): Gehalt (abhängig von \\(x_{true}\\))\n\nEine Regression nur mit \\(x_\\text{observed}\\) und \\(y\\) wäre verzerrt, da der Fehlerterm \\(v\\) nun auch \\(\\beta_2\\) enthält: \\[\n\\begin{align*}\ny &= \\beta_1 + \\beta_2 x_\\text{true} + u \\qquad \\text{mit } x_\\text{observed} = x_\\text{true} + v \\\\\n&= \\beta_1 + \\beta_2 (x_\\text{observed} - v) + u \\\\\n&= \\beta_1 + \\beta_2 x_\\text{observed} + (u - \\beta_2v) \\qquad \\text{mit } \\epsilon = u - \\beta_2v \\\\\n&= \\beta_1 + \\beta_2 x_\\text{observed} + \\epsilon \\\\\n\\end{align*}\n\\]\nDaher kann ein Instrument \\(w\\) eingeführt werden, z.B. die Anzahl der Schuljahre der Eltern. \\(w\\) dürfte in diesem Fall nicht mit dem Fehler in der Umfrage zusammenhängen und ist wahrscheinlich hoch korreliert mit dem Humankapital \\(x\\).\n\n\nCode\nn &lt;- 100\nn_runs &lt;- 10000\nbeta &lt;- matrix(rep(0, 3*n_runs), ncol = 3)\n\nfor(i in 1:n_runs){\n  x_true &lt;- seq(from = 0, to = 10, length.out = n)\n  u_x &lt;- rnorm(n, mean = 0, sd = 1)\n  x_observed &lt;- x_true + u_x\n  \n  u_y &lt;- rnorm(n, mean = 0, sd = 1)\n  y &lt;- 2 + 4*x_true + u_y\n  \n  u_w &lt;- rnorm(n, mean = 0, sd = 1)\n  w &lt;- 0.8*x_true + u_w\n  \n  beta[i,1] &lt;- coef(lm(y ~ x_true))[2] # Wahres Modell\n  beta[i,2] &lt;- coef(lm(y ~ x_observed))[2] # Falsches Modell\n  beta[i,3] &lt;- coef(tsls(y ~ x_observed, ~w))[2] # IV\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"Wahres Modell\" = X1, \"Falsches Modell\" = X2, \"IV Schätzung\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"modell\", values_to = \"beta2\") %&gt;%\n  ggplot(aes(x = beta2, color = modell))+\n  geom_density()+\n  geom_vline(xintercept = 4, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nWie man sehen kann ist die einfache OLS Schätzung zu Null verzerrt. Das ist immer der Fall, auch wenn \\(\\beta_{2, \\text{wahr}}\\) negativ wäre.\nDie Instrumentvariablenschätzung trifft zwar im Mittel den wahren Wert von \\(\\beta_2\\), weißt aber eine höhere Varianz auf als die OLS Schätzung des wahren Modells (ohne Messfehler).\nACHTUNG: Das hier vorgestellte Beispiel bezieht sich nur auf Messfehler in \\(x\\), nicht auf andere unberücksichtigte Einflüsse auf das Gehalt wie z.B. Intelligenz oder Durchhaltevermögen, welche ebenfalls durch \\(u_y\\) aufgefangen werden. Dafür wäre die Anzahl der Schuljahre der Eltern wahrscheinlich kein gutes Instrument, weil die Anzahl der Schuljahre der Eltern und die Intelligenz der beobachteten Person wahrscheinlich korrelieren.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#beispiel-endogene-regressoren",
    "href": "ewf/methoden-der-ewf.html#beispiel-endogene-regressoren",
    "title": "Methoden der EWF",
    "section": "Beispiel: Endogene Regressoren",
    "text": "Beispiel: Endogene Regressoren\nWahres Modell \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 z_i\\) wobei\n\n\\(y_i\\): Einkommen\n\\(x_i\\): Schuliche & Akademische Ausbildung (Jahre)\n\\(z_i\\): Allgemeine Fähigkeit\n\nAllerdings wird die Fähigkeit \\(z\\) nicht beobachtet und \\(\\text{Cov}(x,z) = 0.7\\). Bei einer “naiven” Regression von \\(y\\) auf \\(x\\) fällt somit \\(z\\) in den Fehlerterm \\(u\\). Das führt zu einer Verzerrung von \\(\\beta_2\\).\nEin Instrument \\(w\\) könnte die Entfernung zwischen dem Wohnort und der nächsten Universität sein. Die Idee hier ist, dass Personen, die näher an einer Universität wohnen diese auch eher besuchen (z.B. weil es günstiger ist, da sie bei den Eltern wohnen bleiben können) und somit eine längere Ausbildung haben. Mit der Fähigkeit oder dem Lohn sollte diese Entfernung jedoch nicht zusammenhängen.\n\n\nCode\nn &lt;- 100\nn_runs &lt;- 1000\nbeta &lt;- matrix(rep(0, 3*n_runs), ncol = 3)\n\nfor(i in 1:n_runs){\n  Sigma &lt;- rbind(c(3, 0.7), \n                 c(0.7, 3))\n  mu &lt;- c(10, 20)\n  \n  x_z &lt;- MASS::mvrnorm(n, mu, Sigma)\n  x &lt;- x_z[,1]\n  z &lt;- x_z[,2]\n  u &lt;- rnorm(n, mean = 0, sd = 2)\n  \n  y &lt;- 2 + 4*x + 6*z + u\n  \n  w &lt;- residuals(lm(x ~ z)) + rnorm(n, mean = 0, sd = 1)\n  \n  beta[i,1] &lt;- coef(lm(y ~ x+z))[2] # True model\n  beta[i,2] &lt;- coef(lm(y ~ x))[2] # Model with endogenous regressor\n  beta[i,3] &lt;- coef(tsls(y ~ x, ~w))[2] # IV regression\n}\n\nbeta %&gt;%\n  data.frame() %&gt;%\n  rename(\"Wahres Modell\" = X1, \"Falsches Modell\" = X2, \"IV Schätzung\" = X3) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"modell\", values_to = \"beta2\") %&gt;%\n  #filter(modell != \"Wahres Modell\") %&gt;%\n  ggplot(aes(x = beta2, color = modell))+\n  geom_density()+\n  geom_vline(xintercept = 4, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nIn dem falschen Regression von \\(y\\) (nur) auf \\(x\\) entsteht eine Korrelation zwischen \\(y\\) und \\(u\\): \\(\\text{Cov}(y,u) \\neq 0\\). \\(w\\) und \\(x\\) sind hoch korreliert \\(\\text{Cov}(y,u) \\neq 0\\).\nWieso ist cor(y, resid(tsls(y ~ x, ~w)) so hoch?",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#klassische-momentenmethode-cmm",
    "href": "ewf/methoden-der-ewf.html#klassische-momentenmethode-cmm",
    "title": "Methoden der EWF",
    "section": "Klassische Momentenmethode (CMM)",
    "text": "Klassische Momentenmethode (CMM)\nFür CMM gilt \\(r = k\\).\nIn einem Beispiel mit der Normalverteilung wären sind der Erwartungswert als das erste Moment \\(\\mu = E(x)\\) und die Varinaz als das zweite Moment \\(\\sigma^2 = E((x-\\mu)^2)\\) interessant:\n\n\nCode\nset.seed(1)\n\nn &lt;- 100\nx &lt;- rnorm(n, mean = 2, sd = 1)\n\nm1 &lt;- 1/n * sum(x)\nm2 &lt;- 1/n * sum((x - m1)^2)\n\ncat(\"Erwartungswert (1. Moment) =\", round(m1, 2),\n    \"\\nVarianz (2. zentriertes Moment) =\", round(m2, 2), \"\\n\")\n\n\nErwartungswert (1. Moment) = 2.11 \nVarianz (2. zentriertes Moment) = 0.8 \n\n\nCode\ndata.frame(x) %&gt;%\n  ggplot(aes(x = x))+\n  geom_line(stat = \"density\", alpha = .6, color = \"blue\")+\n  geom_vline(xintercept = m1, linetype = \"dashed\")+\n  geom_vline(xintercept = c(m1+m2, m1-m2), linetype = \"dotted\")+\n  geom_function(fun = function(x) dnorm(x, mean = m1, sd = sqrt(m2)))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nDie gestrichelte Linie stellt die Schätzung für \\(E(x)\\) dar, die gepunkteten stellen \\(E(x^2)\\) dar.\nAllgemein wird mit \\(\\mu_j(\\mathbf{\\theta})\\) das \\(j\\)te Polulationsmoment und \\(\\frac{1}{n}\\sum_{i=1}^n x^j\\) das \\(j\\)te Stichprobenmoment betrachtet. Der Parametervektor \\(\\theta\\) soll so gewählt werden, dass \\[\n\\mu_j(\\mathbf{\\theta}) = \\frac{1}{n}\\sum x^j \\Leftrightarrow \\mu_j(\\mathbf{\\theta}) - \\frac{1}{n}\\sum x^j = 0\n\\] Das kann nun als Gleichungssystem für die \\(k\\) Parameter geschrieben werden: \\[\n\\mathbf{m}(\\mathbf{\\theta}) = \\begin{bmatrix} m_1(\\mathbf{\\theta}) \\\\ m_2(\\mathbf{\\theta}) \\\\ \\vdots \\\\ m_k(\\mathbf{\\theta}) \\end{bmatrix} = \\begin{bmatrix} \\mu_1(\\mathbf{\\theta}) - \\frac{1}{n}\\sum x^1 \\\\ \\mu_2(\\mathbf{\\theta}) - \\frac{1}{n}\\sum x^2 \\\\ \\vdots \\\\ \\mu_k(\\mathbf{\\theta}) - \\frac{1}{n}\\sum x^k \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\mathbf{0}\n\\]\nEin solches Gleichungssystem kann exakt gelöst werden, da die Anzahl der Unbekannten gleich der Anzahl der Gleichungen ist.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/methoden-der-ewf.html#verallgemeinerte-momentenmethode-gmm",
    "href": "ewf/methoden-der-ewf.html#verallgemeinerte-momentenmethode-gmm",
    "title": "Methoden der EWF",
    "section": "Verallgemeinerte Momentenmethode (GMM)",
    "text": "Verallgemeinerte Momentenmethode (GMM)\nIn GMM gilt \\(r &gt; k\\).\nEs wird ebenfalls das Gleichungssystem \\(\\mathbf{m}(\\mathbf{\\theta}) = \\mathbf{0}\\) aufgestellt. Nur dieses lässt sich nicht exakt lösen, da die Anzahl der Gleichungen die Zahl der zu schätzenden Parameter übersteigt. Daher muss eine Zielfunktion her, welche die Summe der gewichteten Momentengleichungen minimiert: \\[\n\\hat{\\mathbf{\\theta}} = \\text{argmin}_\\mathbf{\\theta} Q(\\mathbf{\\theta}) = \\mathbf{m}(\\mathbf{\\theta})^\\prime \\mathbf{A} \\mathbf{m}(\\mathbf{\\theta})\n\\]\nWie das funktioniert wird nun Anhand des Beispiels mit der Normalverteilung gezeigt. Diesmal werden jedoch die ersten 3 Momente verwendet (\\(r=3 &gt; k=2\\)).\n\n\nCode\nn &lt;- 1000\nx &lt;- rnorm(n, mean = 4, sd = 2)\n\n# Stichprobenmomente\nm1 &lt;- 1/n * sum(x)\nm2 &lt;- 1/n * sum((x - m1)^2)\nm3 &lt;- 1/n * sum((x - m1)^3)\n\n# Gewichtungsmatrix\nA &lt;- rbind(c(1,0,0),\n           c(0,1,0),\n           c(0,0,1))\n\ngmm_criterion &lt;- function(params) {\n    mu &lt;- params[1]\n    sigma &lt;- params[2]\n\n    # Polulationsmomente\n    t1 &lt;- mu\n    t2 &lt;- sigma^2\n    t3 &lt;- 0 # Schiefe (3. Moment) der Normalverteilung ist 0\n\n    # Momentengleichungen\n    m &lt;- c(m1 - t1, \n           m2 - t2, \n           m3 - t3)\n\n    # Zielfunktion mit Gewichtung A\n    gmm_obj &lt;- t(m) %*% A %*% m\n    return(gmm_obj)\n}\n\ninit_params &lt;- c(mean = 1, sd = 1) # sd muss größer Null sein\nres &lt;- optim(init_params, gmm_criterion, method = \"BFGS\")\n\nres$par\n\n\n    mean       sd \n3.952863 2.091397 \n\n\n\nVergleich GMM und ML\n\nGewichtung mit \\(A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)Gewichtung mit \\(A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 0.8 & 0 \\\\ 0 & 0 & 0.2 \\end{bmatrix}\\)Gewichtung mit \\(A = \\begin{bmatrix} 2.7 & 0 & 0 \\\\ 0 & 0.2 & 0 \\\\ 0 & 0 & 0.1 \\end{bmatrix}\\)\n\n\n\n\nCode\nn &lt;- 100\nn_runs &lt;- 1000\nresults &lt;- cbind(numeric(n_runs), numeric(n_runs))\n\nexp_rate &lt;- 2\n\ninit_param_gmm &lt;- 1\ninit_param_ml &lt;- 1\n\n# Gewichtungsmatrix\nA &lt;- rbind(c(1,0,0),\n           c(0,1,0),\n           c(0,0,1))\n\nfor(i in 1:n_runs){\n  x &lt;- rexp(n, rate = exp_rate)\n\n  # GMM\n  # Stichprobenmomente\n  m1 &lt;- 1/n * sum(x)\n  m2 &lt;- 1/n * sum((x - m1)^2)\n  m3 &lt;- 1/n * sum((x - m1)^3)\n  \n  gmm_criterion &lt;- function(param) {\n      lambda &lt;- param[1]\n  \n      # Polulationsmomente\n      t1 &lt;- 1/lambda\n      t2 &lt;- 1/lambda^2\n      t3 &lt;- 2\n  \n      # Momentengleichungen\n      diff &lt;- c(m1 - t1, \n                m2 - t2, \n                m3 - t3)\n  \n      # Zielfunktion mit Gewichtung A\n      gmm_obj &lt;- t(diff) %*% A %*% diff\n      return(gmm_obj)\n  }\n  \n  res_gmm &lt;- optim(init_param_gmm, gmm_criterion, method = \"BFGS\")\n  results[i,1] &lt;- res_gmm$par\n  \n  \n  # ML\n  log_likelihood &lt;- function(param) {\n    lambda &lt;- param[1]\n\n    logL &lt;- n * log(lambda) - lambda * sum(x)\n    return(-logL)  # Negative, weil optim() Minimum sucht\n  }\n  \n  res_ml &lt;- optim(init_param_ml, log_likelihood, method = \"BFGS\")\n  results[i,2] &lt;- res_ml$par\n}\n\nresults %&gt;%\n  data.frame() %&gt;%\n  rename(\"GMM\" = X1, \"ML\" = X2) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  ggplot(aes(x = value, color = name))+\n  geom_density()+\n  geom_vline(xintercept = exp_rate, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nn &lt;- 100\nn_runs &lt;- 1000\nresults &lt;- cbind(numeric(n_runs), numeric(n_runs))\n\nexp_rate &lt;- 2\n\ninit_param_gmm &lt;- 1\ninit_param_ml &lt;- 1\n\n# Gewichtungsmatrix\nA &lt;- rbind(c(2,0,0),\n           c(0,0.8,0),\n           c(0,0,0.2))\n\nfor(i in 1:n_runs){\n  x &lt;- rexp(n, rate = exp_rate)\n\n  # GMM\n  # Stichprobenmomente\n  m1 &lt;- 1/n * sum(x)\n  m2 &lt;- 1/n * sum((x - m1)^2)\n  m3 &lt;- 1/n * sum((x - m1)^3)\n  \n  gmm_criterion &lt;- function(param) {\n      lambda &lt;- param[1]\n  \n      # Polulationsmomente\n      t1 &lt;- 1/lambda\n      t2 &lt;- 1/lambda^2\n      t3 &lt;- 2\n  \n      # Momentengleichungen\n      diff &lt;- c(m1 - t1, \n                m2 - t2, \n                m3 - t3)\n  \n      # Zielfunktion mit Gewichtung A\n      gmm_obj &lt;- t(diff) %*% A %*% diff\n      return(gmm_obj)\n  }\n  \n  res_gmm &lt;- optim(init_param_gmm, gmm_criterion, method = \"BFGS\")\n  results[i,1] &lt;- res_gmm$par\n  \n  \n  # ML\n  log_likelihood &lt;- function(param) {\n    lambda &lt;- param[1]\n\n    logL &lt;- n * log(lambda) - lambda * sum(x)\n    return(-logL)  # Negative, weil optim() Minimum sucht\n  }\n  \n  res_ml &lt;- optim(init_param_ml, log_likelihood, method = \"BFGS\")\n  results[i,2] &lt;- res_ml$par\n}\n\nresults %&gt;%\n  data.frame() %&gt;%\n  rename(\"GMM\" = X1, \"ML\" = X2) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  ggplot(aes(x = value, color = name))+\n  geom_density()+\n  geom_vline(xintercept = exp_rate, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nn &lt;- 100\nn_runs &lt;- 1000\nresults &lt;- cbind(numeric(n_runs), numeric(n_runs))\n\nexp_rate &lt;- 2\n\ninit_param_gmm &lt;- 1\ninit_param_ml &lt;- 1\n\n# Gewichtungsmatrix\nA &lt;- rbind(c(2.7,0,0),\n           c(0,0.2,0),\n           c(0,0,0.1))\n\nfor(i in 1:n_runs){\n  x &lt;- rexp(n, rate = exp_rate)\n\n  # GMM\n  # Stichprobenmomente\n  m1 &lt;- 1/n * sum(x)\n  m2 &lt;- 1/n * sum((x - m1)^2)\n  m3 &lt;- 1/n * sum((x - m1)^3)\n  \n  gmm_criterion &lt;- function(param) {\n      lambda &lt;- param[1]\n  \n      # Polulationsmomente\n      t1 &lt;- 1/lambda\n      t2 &lt;- 1/lambda^2\n      t3 &lt;- 2\n  \n      # Momentengleichungen\n      diff &lt;- c(m1 - t1, \n                m2 - t2, \n                m3 - t3)\n  \n      # Zielfunktion mit Gewichtung A\n      gmm_obj &lt;- t(diff) %*% A %*% diff\n      return(gmm_obj)\n  }\n  \n  res_gmm &lt;- optim(init_param_gmm, gmm_criterion, method = \"BFGS\")\n  results[i,1] &lt;- res_gmm$par\n  \n  \n  # ML\n  log_likelihood &lt;- function(param) {\n    lambda &lt;- param[1]\n\n    logL &lt;- n * log(lambda) - lambda * sum(x)\n    return(-logL)  # Negative, weil optim() Minimum sucht\n  }\n  \n  res_ml &lt;- optim(init_param_ml, log_likelihood, method = \"BFGS\")\n  results[i,2] &lt;- res_ml$par\n}\n\nresults %&gt;%\n  data.frame() %&gt;%\n  rename(\"GMM\" = X1, \"ML\" = X2) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  ggplot(aes(x = value, color = name))+\n  geom_density()+\n  geom_vline(xintercept = exp_rate, linetype = \"dashed\")+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Methoden der EWF"
    ]
  },
  {
    "objectID": "ewf/ewf.html",
    "href": "ewf/ewf.html",
    "title": "EWF | Übersicht",
    "section": "",
    "text": "Lorem Ipsum :)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF | Übersicht"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html",
    "href": "statistical-machine-learning/sml.html",
    "title": "Statistical Machine Learning",
    "section": "",
    "text": "Required packages\nCode\nlibrary(jpeg)\nlibrary(mvtnorm)\nlibrary(mclust) # for EM and GMM\nlibrary(vroom)\nlibrary(plotly)\nlibrary(Matrix)\nlibrary(meanShiftR)\nlibrary(FNN)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(viridis)",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#central-limit-theorem",
    "href": "statistical-machine-learning/sml.html#central-limit-theorem",
    "title": "Statistical Machine Learning",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe central limit theorem says, that a sum of random variables will approximate the normal distribution. Here are two examples, one using a uniform distribution and one using the exponential distribution.\n\n\nCode\n# Define number of samples and sample size\nnum_samples &lt;- 20000\nsample_size_1 &lt;- 1\nsample_size_2 &lt;- 2\nsample_size_3 &lt;- 5\nsample_size_4 &lt;- 30\n\n# Generate samples, \nsamples_1 &lt;- replicate(n = num_samples, expr = runif(sample_size_1, min = 0, max = 1))\nsamples_2 &lt;- replicate(n = num_samples, expr = runif(sample_size_2, min = 0, max = 1))\nsamples_3 &lt;- replicate(n = num_samples, expr = runif(sample_size_3, min = 0, max = 1))\nsamples_4 &lt;- replicate(n = num_samples, expr = runif(sample_size_4, min = 0, max = 1))\n\n# Calculate sample sums\nsample_sums_1 &lt;- samples_1\nsample_sums_2 &lt;- colSums(samples_2)\nsample_sums_3 &lt;- colSums(samples_3)\nsample_sums_4 &lt;- colSums(samples_4)\n\n# Convert to data frame for plotting\n\nfacet_names &lt;- c(\n  sample_sums_1 = paste(sample_size_1, \"uniform samples\"),\n  sample_sums_2 = paste(sample_size_2, \"uniform samples\"),\n  sample_sums_3 = paste(sample_size_3, \"uniform samples\"),\n  sample_sums_4 = paste(sample_size_4, \"uniform samples\")\n)\n\ndata.frame(sample_sums_1, sample_sums_2, sample_sums_3, sample_sums_4) %&gt;%\n  pivot_longer(cols = 1:4) %&gt;%\n  ggplot(aes(x = value))+\n  geom_histogram(bins = 40)+\n  facet_wrap(~name, scales = \"free\", labeller = as_labeller(facet_names))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define number of samples and sample size\nnum_samples &lt;- 20000\nsample_size_1 &lt;- 1\nsample_size_2 &lt;- 2\nsample_size_3 &lt;- 5\nsample_size_4 &lt;- 30\n\n# Generate samples, \nsamples_1 &lt;- replicate(n = num_samples, expr = rexp(sample_size_1, rate = 1))\nsamples_2 &lt;- replicate(n = num_samples, expr = rexp(sample_size_2, rate = 1))\nsamples_3 &lt;- replicate(n = num_samples, expr = rexp(sample_size_3, rate = 1))\nsamples_4 &lt;- replicate(n = num_samples, expr = rexp(sample_size_4, rate = 1))\n\n# Calculate sample sums\nsample_sums_1 &lt;- samples_1\nsample_sums_2 &lt;- colSums(samples_2)\nsample_sums_3 &lt;- colSums(samples_3)\nsample_sums_4 &lt;- colSums(samples_4)\n\n# Convert to data frame for plotting\n\nfacet_names &lt;- c(\n  sample_sums_1 = paste(sample_size_1, \"exponential samples\"),\n  sample_sums_2 = paste(sample_size_2, \"exponential samples\"),\n  sample_sums_3 = paste(sample_size_3, \"exponential samples\"),\n  sample_sums_4 = paste(sample_size_4, \"exponential samples\")\n)\n\ndata.frame(sample_sums_1, sample_sums_2, sample_sums_3, sample_sums_4) %&gt;%\n  pivot_longer(cols = 1:4) %&gt;%\n  ggplot(aes(x = value))+\n  geom_histogram(bins = 40)+\n  facet_wrap(~name, scales = \"free\", labeller = as_labeller(facet_names))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNote how the mean and the variance get larger the larger the sum gets.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#priors-and-conditional-probabilities",
    "href": "statistical-machine-learning/sml.html#priors-and-conditional-probabilities",
    "title": "Statistical Machine Learning",
    "section": "Priors and conditional probabilities",
    "text": "Priors and conditional probabilities\nLet’s imagine we want to build an OCR (Optical Character Recognition ti) system. For that, we count the number of black pixels for a given letter and call it \\(x\\). Let’s also imagine that there are only two letters a and b.\nThe a priori probability of a data points belonging to a particular class is called the class prior. This is the probability we assume before we see the observation. We can find this probability by counting. In the English language for example, we could count the number of times a specific letter appears. Let’s say we have the following data set:   abaaa babaa aabba aaaaa\nNow we get \\[\\begin{align*}\nC_1 &= a \\qquad p(C_1) = \\frac{15}{20} = 0.75 \\\\\nC_2 &= b \\qquad p(C_2) = \\frac{5}{20} = 0.25\n\\end{align*}\\] All the probabilities have to sum up to 1, because we have to assign the data to one of the classes. \\[ \\sum_k p(C_k) = 1 \\]\nThe class conditional is the likelihood of making an observation \\(x\\) given that it comes from some class \\(C_k\\). Remember that \\(x\\) is the number of black pixels in our example.\n\n\n\nClass conditional probabilities or likelihood\n\n\nBut now, instead of just checking which likelihood is bigger (for example \\(p(x|b) &gt; p(x|a)\\) for \\(x = 15\\)), we also take into account the prior probability of observing a or b. To do so, we multiply the likelihood by the prior. For example, if the letter a makes up 99% of all letters, then we should be more inclined to classify a sample as a, even if the likelihood of observing a given \\(x\\) is (slightly) higher for b. We also add a normalizing term to the equation, so we get a probability distribution with a total area of 1. This is called Bayes’ Theorem: \\[p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{p(x)} = \\frac{p(x|C_k)p(C_k)}{\\sum_j p(x|C_j)p(C_j)}\\] which we can interpret as \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\cdot \\text{Prior}}{\\text{Normalization Factor}}\\]",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#bayesian-decision-theory",
    "href": "statistical-machine-learning/sml.html#bayesian-decision-theory",
    "title": "Statistical Machine Learning",
    "section": "Bayesian Decision Theory",
    "text": "Bayesian Decision Theory\nThe goal here is to minimize the probability of misclassification or the probability of making a wrong decision: \\[\\begin{align*}\np(\\text{error}) &= p(x \\in R_1, C_2) + p(x \\in R_2, C_1) \\\\\n&= \\int_{R_1} p(x, C_2) dx + \\int_{R_2} p(x, C_1) dx \\\\\n&= \\int_{R_1} p(x|C_2)p(C_2) dx + \\int_{R_2} p(x|C_1)p(C_1) dx\n\\end{align*}\\]\nHere, \\(p(x \\in R_1, C_2)\\) is the probability of observing a sample \\(x\\) that lies in the region \\(R_1\\) where we should choose class \\(C_1 = a\\), but we choose \\(C_2 = b\\) instead.\nWe now can craft a optimal decision rule, where we decide for \\(C_1\\) if \\[\\begin{align*}\np(C_1|x) &&gt; p(C_2|x) \\\\\n\\frac{p(x|C_1)p(C_1)}{p(x)} &&gt; \\frac{p(x|C_2)p(C_2)}{p(x)} \\\\\np(x|C_1)p(C_1) &&gt; p(x|C_2)p(C_2)\n\\end{align*}\\]\nwhich results in the Likelihood Ratio Test (LRT): \\[\\frac{p(x|C_1)}{p(x|C_2)} &gt; \\frac{p(C_2)}{p(C_1)}\\]",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#risk-minimization",
    "href": "statistical-machine-learning/sml.html#risk-minimization",
    "title": "Statistical Machine Learning",
    "section": "Risk Minimization",
    "text": "Risk Minimization\nWe can also use a loss function to minimize the risk of misclassification. The loss function \\(L(C_k, C_j)\\) is a measure of the loss incurred by deciding \\(C_j\\) when the true class is \\(C_k\\). This is especially relevant if the cost of misclassification is not the same for all classes. For example, it’s better to have a false alarm on a smoke detector than to miss a fire (asymmetric loss). We define a loss function \\[\\lambda(\\alpha_i | C_j) = \\lambda_{ij}\\] where \\(C_j\\) is the actual class and \\(\\alpha_i\\) is the decision. Now we can calculate the expected loss of making a decision \\(\\alpha_i\\) \\[\\begin{align*}\nR(\\alpha_i | x) &= \\mathbb{E}_{C_k \\sim p(C|x)}[\\lambda(\\alpha_i | C_k)] \\\\\n&= \\sum_j \\lambda(\\alpha_i | C_j) p(C_j | x)\n\\end{align*}\\]",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#parametric-models",
    "href": "statistical-machine-learning/sml.html#parametric-models",
    "title": "Statistical Machine Learning",
    "section": "Parametric Models",
    "text": "Parametric Models\nOne of the easiest parametric models is the Gaussian distribution. It is defined by the mean \\(\\mu\\) and the variance \\(\\sigma^2\\). The probability density function is given by \\[p(x|\\theta) = p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\] For data \\(x\\) that is generated by this model, we write \\(x \\sim p(x|\\theta)\\).\nMaximum Likelihood Estimation seeks the parameter \\(\\hat\\theta\\) which best explains the data \\(\\mathcal{D}\\). The likelihood of observing some data \\(\\mathcal{D}\\) given some parameters \\(\\theta\\) can be written as \\[\\begin{align*}\n\\mathcal{L}(\\theta) &= p(\\mathcal{D} | \\theta) \\\\\n&= p(x_1, … , x_N | \\theta) \\\\\n&= p(x_1 | \\theta) \\cdot … \\cdot p(x_n | \\theta) \\qquad \\text{assumption: data is i.i.d.} \\\\\n&= \\prod_{n=1}^{N} p(x_n | \\theta)\n\\end{align*}\\] which is transformed using the logarithm most of the time, as the logarithm does not change the position of \\(\\hat\\theta\\) and turns the product into a sum: \\[\\begin{align*}\n\\ln\\mathcal{L}(\\theta) &= \\ln \\prod_{n=1}^{N} p(x_n | \\theta) \\\\\n&= \\sum_{n=1}^{N} \\ln p(x_n | \\theta)\n\\end{align*}\\] This has mostly numerical advantages, as a product of small values can get very very small really really fast and cause issues for the computer.\nThis approach breaks down if the data set contains only one single data point, e.g. \\(\\mathcal{D} = \\{x_1\\}\\). Then, the mean equals the data point and the variance becomes zero.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#bayesian-estimation",
    "href": "statistical-machine-learning/sml.html#bayesian-estimation",
    "title": "Statistical Machine Learning",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\n…",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#non-parametric-models",
    "href": "statistical-machine-learning/sml.html#non-parametric-models",
    "title": "Statistical Machine Learning",
    "section": "Non-Parametric Models",
    "text": "Non-Parametric Models\nNon-parametric models are useful if the underlying probability density distribution family is unknown. They are directly estimated from data, without an explicit parametric model. Every data point is a parameter, so non-parametric models have an uncertain and possibly infinite number of parameters. The biggest problem with most estimation models is the “too smooth vs. not smooth enough” problem.\n\n\nCode\nmean1 &lt;- 2\nstd1 &lt;- 1\n\nmean2 &lt;- -3\nstd2 &lt;- 2\n\nset.seed(0)\nn_samples &lt;- 500\n\ndata1 &lt;- rnorm(n_samples/2, mean = mean1, sd = std1)\ndata2 &lt;- rnorm(n_samples/2, mean = mean2, sd = std2)\n\ndata &lt;- c(data1, data2)\n#head(data)\n\ndata.frame(data) %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density() +\n  labs(x = \"Value\", y = \"Density\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nHistograms\n\n\nCode\ndata.frame(data) %&gt;%\n  ggplot(aes(x = data)) +\n  geom_histogram(binwidth = .5) +\n  labs(x = \"Value\", y = \"Density\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe problem with histograms is, that for higher dimensions, the number of needed bins increases drastically. For \\(D\\) dimensions, we need \\(N^D\\) bins.\n\n\nKernel density estimation (KDE)\nThe idea behind kernel density estimation is to put a kernel upon each observation \\(\\mathbf{x}_n\\) and then to sum up those kernels: \\[ p(\\mathbf{x}) \\approx \\frac{K}{NV} = \\frac{1}{Nh^d} \\sum_{n=1}^N k(\\mathbf{x} - \\mathbf{x{_n}}) \\] where \\(k\\) is the kernel function, for example the Gaussian.\n\n\nCode\nexample_data &lt;- c(1, 5, 6)\n\ndensity_df &lt;- data.frame(\n  x = seq(from = -2, to = 8, by = .01),\n  sapply(example_data, function(m) dnorm(seq(from = -2, to = 8, by = .01), mean = m, sd = 1))\n) %&gt;%\n  pivot_longer(cols = X1:X3)\n\ndensity_df_sum &lt;- density_df %&gt;%\n  group_by(x) %&gt;%\n  summarise(value = sum(value))\n\nggplot() +\n  geom_point(data = data.frame(x = example_data), aes(x = x, y = 0)) +\n  geom_line(data = density_df, aes(x = x, y = value, color = name))+\n  geom_line(data = density_df_sum, aes(x = x, y = value))+\n  #geom_density(data = data.frame(x = example_data), aes(x = x), bw = 1)+ # zum Vergleich\n  coord_cartesian(xlim = c(-2, 8))+\n  theme_bw()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHere, three Gaussians have been put on top of the three data points (red, green, blue) and then summed up (black). Now, it only has to be scaled down so the area under the curve is equal to 1 and thus a valid probability density function.\n\n\nCode\n# Estimate the total area under the curve\ntotal_area &lt;- sum(density_df_sum$value) * .01\n\n# Scale down the summed density by the total area\ndensity_df_sum_scaled &lt;- density_df_sum %&gt;%\n  mutate(value = value / total_area)\n\nggplot() +\n  geom_point(data = data.frame(x = example_data), aes(x = x, y = 0)) +\n  #geom_line(data = density_df, aes(x = x, y = value, color = name))+\n  geom_line(data = density_df_sum_scaled, aes(x = x, y = value))+\n  #geom_density(data = data.frame(x = example_data), aes(x = x), bw = 1, linetype = \"dashed\", color = \"red\")+ #zum Vergleich\n  coord_cartesian(xlim = c(-2, 8))+\n  theme_bw()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nDifferent functions can be chosen as the kernel\n\n\nCode\nkernels &lt;- c(\"rectangular\", \"triangular\", \"gaussian\", \"biweight\")\n\ndata_sample &lt;- data.frame(data) %&gt;%\n  slice_head(n = 5)\n\np1 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"rectangular\", bw = .1) +\n  geom_point(aes(x = data, y = 0))+\n  labs(x = \"\", y = \"\", title = \"rectangular\")+\n  theme_bw()\n\np2 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"triangular\", bw = .1) +\n  geom_point(aes(x = data, y = 0))+\n  labs(x = \"\", y = \"\", title = \"triangular\")+\n  theme_bw()\n\np3 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"gaussian\", bw = .1) +\n  geom_point(aes(x = data, y = 0))+\n  labs(x = \"\", y = \"\", title = \"gaussian\")+\n  theme_bw()\n\np4 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"biweight\", bw = .1) +\n  geom_point(aes(x = data, y = 0))+\n  labs(x = \"\", y = \"\", title = \"biweight\")+\n  theme_bw()\n\nggarrange(p1, p2, p3, p4)\n\n\n\n\n\n\n\n\n\nBut the choice does not matter much for larger samples\n\n\nCode\nkernels &lt;- c(\"rectangular\", \"triangular\", \"gaussian\", \"biweight\")\n\ndata_sample &lt;- data.frame(data) %&gt;%\n  slice_head(n = 500)\n\np1 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"rectangular\", bw = .1) +\n  geom_point(aes(x = data, y = 0), alpha = .1)+\n  labs(x = \"\", y = \"\", title = \"rectangular\")+\n  theme_bw()\n\np2 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"triangular\", bw = .1) +\n  geom_point(aes(x = data, y = 0), alpha = .1)+\n  labs(x = \"\", y = \"\", title = \"triangular\")+\n  theme_bw()\n\np3 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"gaussian\", bw = .1) +\n  geom_point(aes(x = data, y = 0), alpha = .1)+\n  labs(x = \"\", y = \"\", title = \"gaussian\")+\n  theme_bw()\n\np4 &lt;- data_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(kernel = \"biweight\", bw = .1) +\n  geom_point(aes(x = data, y = 0), alpha = .1)+\n  labs(x = \"\", y = \"\", title = \"biweight\")+\n  theme_bw()\n\nggarrange(p1, p2, p3, p4)\n\n\n\n\n\n\n\n\n\nWhat does matter however, is the bandwidth! In the case of the Gaussian kernel, the bandwidth is the standard deviation.\n\n\nCode\nkernels &lt;- c(\"rectangular\", \"triangular\", \"gaussian\", \"biweight\")\n\ndata_sample &lt;- data.frame(data) %&gt;%\n  slice_head(n = 500)\n\ndata_sample %&gt;%\n  ggplot(aes(x = data)) +\n  geom_density(aes(color = \"0.1\"), kernel = \"gaussian\", bw = .1) +\n  geom_density(aes(color = \"0.5\"), kernel = \"gaussian\", bw = .5) +\n  geom_density(aes(color = \"3\"), kernel = \"gaussian\", bw = 3) +\n  geom_point(aes(x = data, y = 0), alpha = .2)+\n  labs(x = \"\", y = \"\")+\n  theme_bw() +\n  scale_color_manual(values = c(\"0.1\" = \"blue\", \"0.5\" = \"red\", \"3\" = \"darkgreen\"),\n                     labels = c(\"Bandwidth = 0.1\", \"Bandwidth = 0.5\", \"Bandwidth = 3\"),\n                     guide = guide_legend(title = \"\"))+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThe red line seems to be about right, while the blue line shows over fitting with a too high variance and the green line shows under fitting with a too high bias (bias-vs-variance trade off).\nAll the above examples were in the 1d case. Now let’s take a look at how KDE looks with two dimensions\n\n\nCode\nn &lt;- 1000\nmu1 &lt;- c(2, 4)\nSigma1 &lt;- rbind(\n  c(1, 0),\n  c(0, 2)\n)\n\nmu2 &lt;- c(5, 0)\nSigma2 &lt;- rbind(\n  c(3, 1),\n  c(.9, 2)\n)\n\ndata_2d &lt;- rbind(mvrnorm(n/2, mu1, Sigma1), mvrnorm(n/2, mu2, Sigma2))\n\np1 &lt;- data.frame(data_2d) %&gt;%\n  ggplot(aes(x = X1, y = X2))+\n  geom_point(alpha = .5)+\n  theme_bw()+\n  theme(legend.position = \"none\")\n\np2 &lt;- data.frame(data_2d) %&gt;%\n  ggplot(aes(x = X1, y = X2))+\n  geom_density_2d()+\n  theme_bw()+\n  theme(legend.position = \"none\")\n\nggarrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\nAgain, the bandwidth matters a lot!\n\n\nCode\np1 &lt;- data.frame(data_2d) %&gt;%\n  ggplot(aes(x = X1, y = X2))+\n  geom_density_2d(h = 1)+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(subtitle = \"h = 1\")\n\np2 &lt;- data.frame(data_2d) %&gt;%\n  ggplot(aes(x = X1, y = X2))+\n  geom_density_2d(h = 2)+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(subtitle = \"h = 2\")\n\nggarrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\nIn \\(d\\) Dimensions, the bandwidth is a vector of length \\(d\\).\n\n\nK-nearest neighbors (KNN)\nThe K-nearest neighbors approach calculates the distance from a point \\(x\\) to every point in the data set. It then sorts the neighbors by distance and checks the k’th distance. The higher the distance from point \\(x\\) to its k’th neighbor, the lower the density and vice versa.\n\n\nCode\n#head(data)\n\n# Define the number of neighbors\nk &lt;- 100\n\n# Define the points at which to estimate the density\nx_points &lt;- seq(min(data), max(data), by = 0.01)\n\n# For each point, calculate the distance to the kth nearest neighbor in the data\nkth_distances &lt;- sapply(x_points, function(x) sort(abs(data - x))[k])\n\n# The estimated density is k divided by the total number of data points times the kth distance\ndensity_estimates &lt;- k / (length(data) * kth_distances)\ndensity_estimates_normalized &lt;- density_estimates / (sum(density_estimates) * 0.01)\n\n# Plot the estimated density\ndata.frame(x = x_points, y = density_estimates_normalized) %&gt;%\n  ggplot()+\n  geom_line(aes(x = x, y = y))+\n  geom_density(data = data.frame(data), aes(x = data))+\n  geom_point(data = data.frame(data), aes(x = data, y = 0), alpha = .1)+\n  theme_bw()+\n  labs(y = \"\")\n\n\n\n\n\n\n\n\n\nInstead of the bandwidth in KDE, with KNN we need to specify how many neighbors we want to take into consideration. The above example used \\(k=100\\) neighbors.\nFor multi-dimensional data, we have to decide on how to measure distance. The most popular distance measure is the euclidean distance, calculated by \\[\nd(p, q) = \\|q-p\\|_2=\\sqrt{(q_1-p_1)^2+…+(q_n-p_n)^2}=\\sqrt{\\sum_{i=1}^n(q_i-p_i)^2}\n\\] which is equal to the Pythagoras theorem for \\(n=2\\) (2 dimensions). Other distance measures are the Manhattan distance (distance in a grid) or the Chebyshev distance (greatest distance along any coordinate dimension).\n\n\nCode\n#head(data_2d)\n\n# Define the number of neighbors\nk &lt;- 100\n\n# Define the points at which to estimate the density\nxy_points &lt;- expand.grid(seq(min(data_2d[,1]), max(data_2d[,1]), by = 0.1),\n                        seq(min(data_2d[,2]), max(data_2d[,2]), by = 0.1)) %&gt;%\n  as.matrix()\n\n# For each point, calculate the distance to the kth nearest neighbor in the data\nkth_distances &lt;- apply(xy_points, 1, function(x) {\n  distances &lt;- sqrt(rowSums((t(t(data_2d) - x))^2))\n  sort(distances)[k]\n})\n\n# The estimated density is k divided by the total number of data points times the kth distance\ndensity_estimates &lt;- k / (length(data_2d) * kth_distances)\ndensity_estimates_normalized &lt;- density_estimates / (sum(density_estimates) * 0.01)\n\n# Plotting\np1 &lt;- data.frame(data_2d) %&gt;%\n  ggplot(aes(x = X1, y = X2))+\n  geom_point(alpha = .5)+\n  theme_bw()+\n  theme(legend.position = \"none\")\n\np2 &lt;- data.frame(xy_points, density_estimates_normalized) %&gt;%\n  ggplot(aes(x = Var1, y = Var2, fill = density_estimates_normalized))+\n  geom_raster()+\n  scale_fill_viridis()+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(x = \"X1\", y = \"X2\")\n\nggarrange(p1, p2, nrow = 1)",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#mixture-models",
    "href": "statistical-machine-learning/sml.html#mixture-models",
    "title": "Statistical Machine Learning",
    "section": "Mixture Models",
    "text": "Mixture Models\nLet’s first take a step back and compare parametric and nonparametric models:\n\n\n\nParametric models\nNonparametric models\n\n\n\n\nGaussian, …\nKDE, kNN, …\n\n\nGood analytic properties\nGeneral\n\n\nSmall memory requirements\nLarge memory requirements\n\n\nFast\nSlow\n\n\n\nMixture models now can have the advantages of both model classes!\nAn example of an “easy” mixture model is the sum of multiple Gaussian distributions. \\[p(x) = \\sum_{j=1}^M p(x | z_j) \\cdot p(z_j)\\] where \\(m\\) is the number of clusters, \\(p(x|z_j)\\) the probability of \\(x\\) given it’s in cluster \\(j\\) and \\(p(z_j)\\) the probability of cluster \\(j\\) (prior).\n\n\nCode\nx &lt;- seq(from = -5, to = 15, by = 0.01)\n\nprior_g1 &lt;- 0.3\nprior_g2 &lt;- 0.2\nprior_g3 &lt;- 0.5\n\ng1 &lt;- dnorm(x, mean = 0, sd = 1) * prior_g1\ng2 &lt;- dnorm(x, mean = 4, sd = 3) * prior_g2\ng3 &lt;- dnorm(x, mean = 10, sd = 2) * prior_g3\n\ng &lt;- g1 + g2 + g3\n\ndata.frame(x, g1, g2, g3, g) %&gt;%\n  pivot_longer(cols = g1:g3) %&gt;%\n  ggplot()+\n  geom_line(aes(x = x, y = value, color = name))+\n  geom_line(aes(x = x, y = g))+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  labs(x = \"x\", y = \"p(x)\")\n\n\n\n\n\n\n\n\n\nNow the question is, how can one estimate a mixture model based on some data like the following, where we have drawn samples from the above mixture distribution.\n\n\nCode\nn &lt;- 1000\n\nd1 &lt;- rnorm(n/3, mean = 0, sd = 1)\nd2 &lt;- rnorm(n/3, mean = 3, sd = 3)\nd3 &lt;- rnorm(n/3, mean = 10, sd = 2)\n\nd &lt;- c(d1, d2, d3)\n\ndata.frame(x = d) %&gt;%\n  ggplot()+\n  geom_point(aes(x = x, y = 0), alpha = .1)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nOne could try to solve such find the parameters of such a model using Maximum Likelihood. But a better way is presented in the section on the EM algorithm.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#k-means-clustering",
    "href": "statistical-machine-learning/sml.html#k-means-clustering",
    "title": "Statistical Machine Learning",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nLet’s use the famous iris data set, which contains 150 data points with 4 measurements of the petal (deutsch: Blütenblatt) and sepal (deutsch: Kelchblatt) as well as a label for the species.\n\n\nCode\niris &lt;- read_csv(\"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\")\n\niris %&gt;%\n  ggplot(aes(x = petal.length, y = petal.width, color = variety))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nK-Means optimizes the following objective function \\[J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\|\\mathbf{x}_n - \\mathbf{\\mu}_k\\|^2\\] where \\(r_{nk}\\) is an indicator variable that checks whether \\(\\mathbf{\\mu}_k\\) is the nearest cluster center to point \\(\\mathbf{x}_n\\) \\[\nr_{nk} =\n\\begin{cases}\n1 \\qquad , \\text{if } k = \\text{arg min}_j \\|\\mathbf{x}_n - \\mathbf{\\mu}_k\\|^2 \\\\\n0 \\qquad , \\text{otherwise}\n\\end{cases}\n\\]\nOr a bit less formal:\n\nInitialization: Pick k arbitrary centroids (cluster means)\nAssign each sample to the closest centroid\nAdjust the centroids to be the means of the samples assigned to them\nGo to step 2 (until no further change)\n\n\n\nCode\nn_clusters &lt;- 3\nmax_iter &lt;- 20\n\n# Function to calculate euclidean distance\neuclidean_dist &lt;- function(a, b) sqrt(sum((a - b)^2))\n\niris_matrix &lt;- as.matrix(iris[,3:4])\n\n# Randomly select initial centroids\nset.seed(8)\n#centroids &lt;- array(dim = c(n_clusters,2, max_iter))\n#centroids[,,1] &lt;- iris_matrix[sample(nrow(iris_matrix), n_clusters), ]\ncentroids &lt;- iris_matrix[sample(nrow(iris_matrix), n_clusters), ]\n  \n# Create a variable to store cluster assignments\ncluster_assignment &lt;- rep(1, nrow(iris_matrix))\n\nold_cluster_assignment &lt;- matrix(nrow = nrow(iris), ncol = max_iter)\n\nfor(iter in 1:max_iter) {\n  #print(iter)\n  # Old cluster assignments\n  old_cluster_assignment[,iter] &lt;- cluster_assignment\n  \n  # Assign each point to the closest centroid\n  for (i in 1:nrow(iris_matrix)) {\n    distances &lt;- sapply(1:n_clusters, function(j) euclidean_dist(iris_matrix[i, ], centroids[j, ]))\n    cluster_assignment[i] &lt;- which.min(distances)\n  }\n  \n  # Update centroids\n  for (j in 1:n_clusters) {\n    points_in_cluster &lt;- iris_matrix[cluster_assignment == j, ]\n    if (nrow(points_in_cluster) &gt; 0)\n      centroids[j, ] &lt;- colMeans(points_in_cluster)\n  }\n  \n  # If no point's assignment has changed, we're done\n  if (all(cluster_assignment == old_cluster_assignment[,iter])) {\n    cluster_assignment &lt;- old_cluster_assignment[,1:iter]\n    break\n  }\n}\n\ncluster_assignment_df &lt;- as.data.frame(cluster_assignment) %&gt;%\n  mutate(i = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -i, names_to = \"iteration\", values_to = \"cluster\") %&gt;%\n  mutate(iteration = as.numeric(str_extract(iteration, \"\\\\d+\")))\n\niris %&gt;%\n  mutate(i = 1:nrow(.)) %&gt;%\n  left_join(cluster_assignment_df, by = join_by(i)) %&gt;%\n  ggplot(aes(x = petal.length, y = petal.width, color = as.character(cluster)))+\n  geom_point()+\n  facet_wrap(~iteration)+\n  theme_bw()+\n  theme(legend.position = \"top\")+\n  labs(color = \"assigned cluster\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrengths\nWeaknesses\n\n\n\n\nSimple and fast to compute\nProblem of finding suitable k\n\n\nGuaranteed to converge\nSensitive to initial centers and outliers\n\n\n\nOnly spherical clusters\n\n\n\nConverges only to local optimum",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#em-clustering",
    "href": "statistical-machine-learning/sml.html#em-clustering",
    "title": "Statistical Machine Learning",
    "section": "EM-Clustering",
    "text": "EM-Clustering\n For the EM algorithm, the assumption is that the data is generated by an underlying probability distribution which is a Gaussian mixture model. We now cluster using soft assignments, that means we assign each data point a probability of belonging to a cluster. But one after the other.\nLet’s start with a simple 1d data set that is generated by two Gaussian \\(N(\\mu_1 = 0,\\sigma_1 = 3)\\) and \\(N(\\mu_2 = 10,\\sigma_2 = 1)\\).\n\n\nCode\nset.seed(1)\nn &lt;- 100\n\nx1 &lt;- rnorm(n/2, mean = 0, sd = 3)\nx2 &lt;- rnorm(n/2, mean = 10, sd = 1)\n\nx &lt;- c(x1, x2)\n\ndata.frame(x) %&gt;%\n  ggplot(aes(x = x, y = 0))+\n  geom_point(alpha = .5)+\n  theme_bw()+\n  labs(y = \"\")\n\n\n\n\n\n\n\n\n\nAs with k-means, we need to specify the number of clusters or in this case the number of Gaussians. Let’s (correctly) assume \\(k = 2\\).\nWe first initialize the mean and the variance of the two Gaussians (red and blue) randomly. Here, we choose -1 and 1 as the means and 1 as the standard deviation.\n\n\nCode\nset.seed(1)\n\nmu_blue &lt;- -1\nsigma_blue &lt;- 1\n\nmu_red &lt;- 1\nsigma_red &lt;- 1\n\nx_seq &lt;- seq(from = min(x), to = max(x), by = 0.01)\ng_blue &lt;- dnorm(x_seq, mean = mu_blue, sd = sigma_blue)\ng_red &lt;- dnorm(x_seq, mean = mu_red, sd = sigma_red)\n\nggplot()+\n  geom_area(data = data.frame(x_seq, g_blue), aes(x = x_seq, y = g_blue), color = \"blue\", fill = \"blue\", alpha = .3)+\n  geom_area(data = data.frame(x_seq, g_red), aes(x = x_seq, y = g_red), color = \"red\", fill = \"red\", alpha = .3)+\n  geom_point(data = data.frame(x), aes(x = x, y = 0), alpha = .5)+\n  theme_bw()+\n  labs(x = \"x\", y = \"\")\n\n\n\n\n\n\n\n\n\nNow we calculate how likely each data point is to belong to the red Gaussian. Because we know the probability of \\(x_i\\) being generated by the red Gaussian \\[\np(x_i | \\text{red}) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\text{red}^2}}\\exp \\left( - \\frac{(x_i-\\mu_\\text{red})^2}{2\\sigma_\\text{red}^2} \\right)\n\\] we can calculate the probability of \\(x_i\\) belonging to the red Gaussian \\[\np(\\text{red}|x_i)=\n\\frac{p(x_i|\\text{red}) \\cdot p(\\text{red})}{p(x_i|\\text{red}) \\cdot p(\\text{red}) + p(x_i|\\text{blue}) \\cdot p(\\text{blue})}\n\\] This is the expectation step.\nIf we don’t know the priors, we can just divide them evenly, so \\(p(\\text{red}) = p(\\text{blue}) = \\frac{1}{k} = 0.5\\).\n\n\nCode\nprior_blue &lt;- .5\nprior_red &lt;- .5\n\nprob_red &lt;- (dnorm(x, mean = mu_red, sd = sigma_red) * prior_red) / ((dnorm(x, mean = mu_red, sd = sigma_red) * prior_red) + (dnorm(x, mean = mu_blue, sd = sigma_blue) * prior_blue))\n\nprob_blue &lt;- (dnorm(x, mean = mu_blue, sd = sigma_blue) * prior_blue) / ((dnorm(x, mean = mu_red, sd = sigma_red) * prior_red) + (dnorm(x, mean = mu_blue, sd = sigma_blue) * prior_blue))\n\n\nggplot()+\n  geom_area(data = data.frame(x_seq, g_blue), aes(x = x_seq, y = g_blue), color = \"blue\", fill = \"blue\", alpha = .3)+\n  geom_area(data = data.frame(x_seq, g_red), aes(x = x_seq, y = g_red), color = \"red\", fill = \"red\", alpha = .3)+\n  geom_point(data = data.frame(x, prob_blue, prob_red), aes(x = x, y = 0, color = prob_red))+\n  theme_bw()+\n  scale_color_gradient(low = \"blue\", high = \"red\")+\n  theme(legend.position = \"top\")+\n  labs(color = \"Prob. red\", x = \"x\", y = \"\")\n\n\n\n\n\n\n\n\n\nNow we can update the priors, means and variances in the maximization step. The updated prior for a class is the average probability of all data points belonging to that class. \\[\np(\\text{red})^\\text{new} = \\frac{1}{N} \\sum_{i=1}^N p(\\text{red}|x_i)\n\\] The updated mean is a simple average of all x-values weighted by the probability of belonging to the Gaussian. \\[\n\\mu_\\text{red}^\\text{new} = \\frac{\\sum_{i=1}^N p(\\text{red}|x_i) \\cdot x_i}{\\sum_{i=1}^N p(\\text{red}|x_i)}\n\\] The updated standard deviation is the square root of the average of the squared differences from the mean for all data points, where each difference is weighted by the probability of the data point belonging to the class. \\[\n\\sigma_\\text{red}^\\text{new} = \\frac{\\sqrt{\\sum_{i=1}^N p(\\text{red}|x_i) \\cdot (x_i - \\mu_\\text{red})^2}}{\\sum_{i=1}^N p(\\text{red}|x_i)}\n\\]\n\n\nCode\nprior_red &lt;- sum(prob_red) / length(x)\nprior_blue &lt;- sum(prob_blue) / length(x)\n\nmu_red &lt;- sum(prob_red * x) / sum(prob_red)\nmu_blue &lt;- sum(prob_blue * x) / sum(prob_blue)\n\nsigma_red &lt;- sqrt(sum(prob_red * (x - mu_red)^2) / sum(prob_red))\nsigma_blue &lt;- sqrt(sum(prob_blue * (x - mu_blue)^2) / sum(prob_blue))\n\n\n\nx_seq &lt;- seq(from = min(x), to = max(x), by = 0.01)\ng_blue &lt;- dnorm(x_seq, mean = mu_blue, sd = sigma_blue)\ng_red &lt;- dnorm(x_seq, mean = mu_red, sd = sigma_red)\n\nggplot()+\n  geom_area(data = data.frame(x_seq, g_blue), aes(x = x_seq, y = g_blue), color = \"blue\", fill = \"blue\", alpha = .3)+\n  geom_area(data = data.frame(x_seq, g_red), aes(x = x_seq, y = g_red), color = \"red\", fill = \"red\", alpha = .3)+\n  geom_point(data = data.frame(x), aes(x = x, y = 0), alpha = .5)+\n  theme_bw()+\n  labs(x = \"x\", y = \"\")\n\n\n\n\n\n\n\n\n\nGreat! Now let’s run the algorithm till convergence.\n\n\nCode\nmu_blue &lt;- -1\nsigma_blue &lt;- 1\n\nmu_red &lt;- 1\nsigma_red &lt;- 1\n\nprior_red &lt;- .5\nprior_red &lt;- .5\n\n\ntolerance &lt;- 1e-8\nchange &lt;- Inf\nmax_iter &lt;- 100\n\nfor (iter in 1:max_iter) {\n  old_mu_red &lt;- mu_red\n  old_mu_blue &lt;- mu_blue\n  old_sigma_red &lt;- sigma_red\n  old_sigma_blue &lt;- sigma_blue\n\n  # E-step\n  prob_red &lt;- (dnorm(x, mean = mu_red, sd = sigma_red) * prior_red) / ((dnorm(x, mean = mu_red, sd = sigma_red) * prior_red) + (dnorm(x, mean = mu_blue, sd = sigma_blue) * prior_blue))\n  prob_blue &lt;- (dnorm(x, mean = mu_blue, sd = sigma_blue) * prior_blue) / ((dnorm(x, mean = mu_red, sd = sigma_red) * prior_red) + (dnorm(x, mean = mu_blue, sd = sigma_blue) * prior_blue))\n\n  # M-step\n  prior_red &lt;- sum(prob_red) / length(x)\n  prior_blue &lt;- sum(prob_blue) / length(x)\n  mu_red &lt;- sum(prob_red * x) / sum(prob_red)\n  mu_blue &lt;- sum(prob_blue * x) / sum(prob_blue)\n  sigma_red &lt;- sqrt(sum(prob_red * (x - mu_red)^2) / sum(prob_red))\n  sigma_blue &lt;- sqrt(sum(prob_blue * (x - mu_blue)^2) / sum(prob_blue))\n\n  # Calculate change in parameters\n  change &lt;- max(abs(c(old_mu_red - mu_red, old_mu_blue - mu_blue, old_sigma_red - sigma_red, old_sigma_blue - sigma_blue)))\n  if(change &lt; tolerance) break\n}\n\nx_seq &lt;- seq(from = min(x), to = max(x), by = 0.01)\ng_blue &lt;- dnorm(x_seq, mean = mu_blue, sd = sigma_blue)\ng_red &lt;- dnorm(x_seq, mean = mu_red, sd = sigma_red)\n\nggplot()+\n  geom_area(data = data.frame(x_seq, g_blue), aes(x = x_seq, y = g_blue), color = \"blue\", fill = \"blue\", alpha = .3)+\n  geom_area(data = data.frame(x_seq, g_red), aes(x = x_seq, y = g_red), color = \"red\", fill = \"red\", alpha = .3)+\n  geom_point(data = data.frame(x, prob_red), aes(x = x, y = 0, color = prob_red), alpha = .5)+\n  scale_color_gradient(low = \"blue\", high = \"red\")+\n  theme_bw()+\n  labs(x = \"x\", y = \"\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThe algorithm converges after 18 iterations and correctly finds the two Gaussians.\n\n\n\nStrengths\nWeaknesses\n\n\n\n\nProbabilistic interpretation\nProblem of finding suitable k\n\n\nSoft assignments\nSensitive to initialization\n\n\nCan predict new data points\nComputationally expensive\n\n\n\nConverges only to local optimum\n\n\n\n\nMean Shift\nMean shift is a method for finding modes in a cloud of data points. To be more precise, it tries to find the modes of a kernel density estimate through local search\n\n\nCode\n#iris &lt;- read_csv(\"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\")\n\niris %&gt;%\n  ggplot(aes(x = petal.length, y = petal.width, color = variety))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nCode\niris_matrix &lt;- as.matrix(iris[, 1:4])\n\n\n\n\nCode\niris_cluster &lt;- meanShift(iris_matrix)\n\ndata.frame(iris, cluster = iris_cluster$assignment) %&gt;%\n  ggplot(aes(x = petal.length, y = petal.width, color = as.character(cluster)))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nTODO",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#discriminant-functions",
    "href": "statistical-machine-learning/sml.html#discriminant-functions",
    "title": "Statistical Machine Learning",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#fisher-discriminant-analysis",
    "href": "statistical-machine-learning/sml.html#fisher-discriminant-analysis",
    "title": "Statistical Machine Learning",
    "section": "Fisher Discriminant Analysis",
    "text": "Fisher Discriminant Analysis\nThe goal of Linear Discriminant Analysis (LDA) is to find a linear combination of features (vector \\(\\mathbf{w}\\)) that effectively separates or characterizes two or more classes of objects or events. It aims to reduce dimensionality while preserving as much of the class discriminatory information as possible.\n\\[\n\\text{max}_\\mathbf{w}J(\\mathbf{w}) = \\frac{(m_1 - m_2)^2}{s_1^2 + s_2^2} = \\frac{\\mathbf{w}^T\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^T\\mathbf{S}_W\\mathbf{w}}\n\\]\nIn this formula, \\(m_1 = \\mathbf{w}^T\\mathbf{m}_1\\) and \\(m_2 = \\mathbf{w}^T\\mathbf{m}_2\\) represent the means of the groups and \\(s_1^2\\) and \\(s_2^2\\) represent the in group variances. So we try to find a vector \\(\\mathbf{w}\\) that maximizes the distance between the means while minimizing the in group variances, when the data is projected onto it.\nThis formula can be solved analytically and results in \\[\n\\mathbf{w} \\propto \\mathbf{S}_W^{-1} (\\mathbf{m}_1 - \\mathbf{m}_2)\n\\] where \\(\\propto\\) means “is proportional to”. This is because only the direction of vector \\(\\mathbf{w}\\) is relevant and not its scale.\n\n\nCode\n# Data generation and base plot\nset.seed(2)\n\nn &lt;- 100\nmu_1 &lt;- c(2, 4)\nSigma_1 &lt;- rbind(\n  c(1, 1.6),\n  c(1.6, 4)\n)\n\nmu_2 &lt;- c(3, 0)\nSigma_2 &lt;- rbind(\n  c(1, 1.6),\n  c(1.6, 4)\n)\n\ndata_1 &lt;- mvrnorm(n/2, mu = mu_1, Sigma = Sigma_1)\ndata_2 &lt;- mvrnorm(n/2, mu = mu_2, Sigma = Sigma_2)\nlabels &lt;- c(rep(1, n/2), rep(2, n/2))\n\ndata &lt;- rbind(data_1, data_2)\n\n\n\n\nCode\np1 &lt;- data %&gt;%\n  data.frame() %&gt;%\n  cbind(labels) %&gt;%\n  ggplot(aes(x = X1, y = X2, color = as.character(labels)))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"none\")\n\n\n# First example\nw_1 &lt;- c(0, 1)\n\ndata_proj_1 &lt;- data %*% w_1\n\np2 &lt;- data.frame(x = data_proj_1) %&gt;%\n  cbind(labels) %&gt;%\n  ggplot()+\n  geom_density(aes(x = x, color = as.character(labels)), inherit.aes = FALSE)+\n  geom_point(aes(x = x, y = 0, color = as.character(labels)), alpha = 1)+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs(subtitle = \"w = [0, 1]\",\n       x = \"\", y = \"\")\n\n# Second example\nw_2 &lt;- c(1, 0)\n\ndata_proj_2 &lt;- data %*% w_2\n\np3 &lt;- data.frame(x = data_proj_2) %&gt;%\n  cbind(labels) %&gt;%\n  ggplot()+\n  geom_density(aes(x = x, color = as.character(labels)), inherit.aes = FALSE)+\n  geom_point(aes(x = x, y = 0, color = as.character(labels)), alpha = 1)+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs(subtitle = \"w = [1, 0]\",\n       x = \"\", y = \"\")\n\n\n\n\nCode\n# Calculate the means of each class\nclass1_data &lt;- data[labels == 1, ]\nclass2_data &lt;- data[labels == 2, ]\nm1 &lt;- colMeans(class1_data)\nm2 &lt;- colMeans(class2_data)\n\n# Calculate the overall mean\nm &lt;- colMeans(data)\n\n# Calculate the between-class scatter matrix\nS_B &lt;- (m1 - m2) %*% t(m1 - m2)\nS_B1 &lt;- nrow(class1_data) * (m1 - m) %*% t(m1 - m) + nrow(class1_data) * (m2 - m) %*% t(m - m)\n\n# Calculate the within-class scatter matrices\nS_W1 &lt;- cov(class1_data)\nS_W2 &lt;- cov(class2_data)\n\n# Compute the within-class scatter matrix\nS_W &lt;- S_W1 + S_W2\n\n# Compute the optimal w\nw_optimal &lt;- solve(S_W) %*% (m1 - m2)\n\ndata_proj_3 &lt;- data %*% w_optimal\n\np4 &lt;- data.frame(x = data_proj_3) %&gt;%\n  cbind(labels) %&gt;%\n  ggplot()+\n  geom_density(aes(x = x, color = as.character(labels)), inherit.aes = FALSE)+\n  geom_point(aes(x = x, y = 0, color = as.character(labels)), alpha = 1)+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs(subtitle = paste(\"w* = [\", round(w_optimal[1], 2), \", \", round(w_optimal[2], 2), \"]\", sep = \"\"),\n       x = \"\", y = \"\")\n\nggarrange(p1, ggarrange(p2, p3, p4, nrow = 3), nrow = 1)\n\n\n\n\n\n\n\n\n\nThis separation now allows use to classify the data in a much better way.\nLet’s take another look at the optimal \\(\\mathbf{w}^*\\)\n\n\nCode\nev &lt;- eigen(solve(S_W) %*% S_B1)$vectors\nslope &lt;- ev[1,2] / ev[1,1]\nslope_orth &lt;- ev[1,1] / -ev[1,2]\nintercept &lt;- ev[2,1]\n\ndata %&gt;%\n  data.frame() %&gt;%\n  cbind(labels) %&gt;%\n  ggplot(aes(x = X1, y = X2, color = as.character(labels)))+\n  geom_point()+\n  geom_abline(intercept = intercept, slope = w_optimal[2]/w_optimal[1], color = \"red\")+\n  geom_abline(intercept = -3, slope = w_optimal[1]/-w_optimal[2], color = \"red\")+\n  geom_abline(intercept = intercept, slope = slope, color = \"green\")+\n  geom_abline(intercept = intercept, slope = slope_orth, color = \"green\")+\n  theme_bw()+\n  coord_fixed()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\nw_optimal\n\n\n          [,1]\n[1,] -3.147675\n[2,]  1.638460\n\n\nCode\nw_optimal[2]/w_optimal[1]\n\n\n[1] -0.5205302\n\n\nThe red lines are calculated using the optimal \\(\\mathbf{w}^*\\). The green lines using the first eigenvector of \\(\\mathbf{W}^{-1}\\mathbf{B}\\) where \\(\\mathbf{W}\\) is the within class covariance matrix and \\(\\mathbf{B}\\) the between class covariance. Both red and green should be the same…\n\n\nCode\nset.seed(2014)\nlibrary(MASS)\n#library(DiscriMiner) # For scatter matrices\nlibrary(ggplot2)\nlibrary(grid)\n# Generate multivariate data\nmu1 &lt;- c(2, -3)\nmu2 &lt;- c(2, 5)\nrho &lt;- 0.6\ns1 &lt;- 1\ns2 &lt;- 3\nSigma &lt;- matrix(c(s1^2, rho * s1 * s2, rho * s1 * s2, s2^2), byrow = TRUE, nrow = 2)\nn &lt;- 50\n# Multivariate normal sampling\nX1 &lt;- mvrnorm(n, mu = mu1, Sigma = Sigma)\nX2 &lt;- mvrnorm(n, mu = mu2, Sigma = Sigma)\nX &lt;- rbind(X1, X2)\n# Center data\nZ &lt;- scale(X, scale = FALSE)\n# Class variable\ny &lt;- rep(c(0, 1), each = n)\n\n#####\n\n# Calculate the means of each class\nclass1_data &lt;- Z[y == 1, ]\nclass2_data &lt;- Z[y == 2, ]\nm1 &lt;- colMeans(class1_data)\nm2 &lt;- colMeans(class2_data)\n\n# Calculate the overall mean\nm &lt;- colMeans(Z)\n\n# Calculate the between-class scatter matrix\nB &lt;- (m1 - m2) %*% t(m1 - m2)\n#S_B1 &lt;- nrow(class1_data) * (m1 - m) %*% t(m1 - m) + nrow(class1_data) * (m2 - m) %*% t(m - m)\n\n# Calculate the within-class scatter matrices\nW1 &lt;- cov(class1_data)\nW2 &lt;- cov(class2_data)\n\n# Compute the within-class scatter matrix\nW &lt;- W1 + W2\n\n#####\n\n# Scatter matrices\n#B &lt;- betweenCov(variables = X, group = y)\n#W &lt;- withinCov(variables = X, group = y)\n\n# Eigenvectors\nev &lt;- eigen(solve(W) %*% B)$vectors\nslope &lt;- - ev[1,1] / ev[2,1]\nintercept &lt;- ev[2,1]\n\n# Create projections on 1st discriminant\nP &lt;- Z %*% ev[,1] %*% t(ev[,1])\n\n# ggplo2 requires data frame\nmy.df &lt;- data.frame(Z1 = Z[, 1], Z2 = Z[, 2], P1 = P[, 1], P2 = P[, 2])\n\nplt &lt;- ggplot(data = my.df, aes(Z1, Z2))\nplt &lt;- plt + geom_segment(aes(xend = P1, yend = P2), size = 0.2, color = \"gray\")\nplt &lt;- plt + geom_point(aes(color = factor(y)))\nplt &lt;- plt + geom_point(aes(x = P1, y = P2, colour = factor(y)))\nplt &lt;- plt + scale_colour_brewer(palette = \"Set1\")\nplt &lt;- plt + geom_abline(intercept = intercept, slope = slope, size = 0.2)\nplt &lt;- plt + coord_fixed()\nplt &lt;- plt + xlab(expression(X[1])) + ylab(expression(X[2]))\nplt &lt;- plt + theme_bw()\nplt &lt;- plt + theme(axis.title.x = element_text(size = 8),\n                   axis.text.x  = element_text(size = 8),\n                   axis.title.y = element_text(size = 8),\n                   axis.text.y  = element_text(size = 8),\n                   legend.position = \"none\")\nplt",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#perceptron-algorithm",
    "href": "statistical-machine-learning/sml.html#perceptron-algorithm",
    "title": "Statistical Machine Learning",
    "section": "Perceptron Algorithm",
    "text": "Perceptron Algorithm\nTODO",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#logistic-regression",
    "href": "statistical-machine-learning/sml.html#logistic-regression",
    "title": "Statistical Machine Learning",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a type of discriminative modelling, where we model the class posterior directly.\nWe assume that there are two discrete classes. Either the data belongs to class \\(C_1 \\Rightarrow y_i = 0\\) or to class \\(C_2 \\Rightarrow y_i = 1\\), so we can plot the data in a different way.\n\n\nCode\n# Generate some data\nn &lt;- 100\nx &lt;- runif(n, min = 1, max = 100)\nz &lt;- 1 * x + rnorm(n, mean = 0, sd = 20)\ny &lt;- if_else(z &lt; 50, 0, 1)\n\ndf &lt;- data.frame(y=y,x=x)\n\np1 &lt;- df %&gt;%\n  ggplot(aes(x = x, y = 0, color = as.factor(y)))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())+\n  labs(y = \"\")\n\np2 &lt;- df %&gt;%\n  ggplot(aes(x = x, y = y, color = as.factor(y)))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"none\")+\n  coord_cartesian(ylim = c(-1, 2))\n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\n\nLet’s take a look at the math:\n\\[\\begin{align*}\np(C_1 | \\mathbf{x}) &= \\frac{p(\\mathbf{x}|C_1) p(C_1)}{p(\\mathbf{x})} \\\\\n&= \\frac{p(\\mathbf{x}|C_1) p(C_1)}{\\sum_i p(\\mathbf{x}, C_i)} \\\\\n&= \\frac{p(\\mathbf{x}|C_1) p(C_1)}{\\sum_i p(\\mathbf{x}|C_i) p(C_i)} \\\\\n&= … \\\\\n&= \\frac{1}{1 + \\exp(-a)} \\qquad \\text{with } a = \\log \\frac{p(\\mathbf{x}|C_1) p(C_1)}{p(\\mathbf{x}|C_2) p(C_2)} \\\\\n&= \\sigma (a)\n\\end{align*}\\]\nWhere \\(\\sigma\\) is the cumulative distribution function (CDF) of the logistic distribution. We can then model \\(a = \\mathbf{w}^T\\mathbf{x} + w_0\\) and get the logistic regression function. \\[\np(C_1 | \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}^T\\mathbf{x} + w_0)}{1 + \\exp(\\mathbf{w}^T\\mathbf{x} + w_0)} = \\frac{1}{1 + \\exp(-\\mathbf{w}^T\\mathbf{x} - w_0)} = \\sigma (\\mathbf{w}^T\\mathbf{x} + w_0)\n\\] Now we model the outcome as a Bernoulli experiment. Either the data belongs to class \\(C_1 \\Rightarrow y_i = 0\\) or to class \\(C_2 \\Rightarrow y_i = 1\\). Then, we maximize the likelihood. \\[\\begin{align*}\nL(w) &= \\prod_{i=1}^N p(C_1|\\mathbf{x}_i; \\mathbf{w}; w_0)^{y_i} \\cdot p(C_2|\\mathbf{x}_i; \\mathbf{w}; w_0)^{1-y_i} \\\\\n&= \\prod_{i=1}^N \\sigma (\\mathbf{w}^T\\mathbf{x} + w_0)^{y_i} \\cdot (1 - \\sigma (\\mathbf{w}^T\\mathbf{x} + w_0))^{1-y_i}\n\\end{align*}\\] For maximization, we can use numerical methods, like the newton algorithm.\n\n\nCode\n# Define the likelihood function\nneg_log_likelihood &lt;- function(w, y, x) {\n  z &lt;- w[1] + w[2]*x\n  prob &lt;- 1/(1+exp(-z))\n  -sum(y*log(prob) + (1-y)*log(1-prob))\n}\n\n# Initial values for the parameters\nw_start &lt;- c(0,0)\n\n# Use optim() to estimate the parameters\nresult &lt;- optim(w_start, neg_log_likelihood, y=df$y, x=df$x, hessian=TRUE)\n\ndf %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  theme_bw()+\n  stat_function(fun = function(x) {1/(1+exp(-result$par[1]-result$par[2]*x))})\n\n\n\n\n\n\n\n\n\nIf we use different (here: quadratic) features, we can also model data that looks different\n\n\nCode\nset.seed(1)\n\nn &lt;- 100\nx &lt;- runif(n, min = 0, max = 10)\nz &lt;- 0.2*x - 0.02*x^2 + rnorm(n = n, mean = 0, sd = .15)\ny &lt;- if_else(z &lt; 0.3, 0, 1)\n\ndf &lt;- data.frame(x, y)\n\n# Define the likelihood function\nneg_log_likelihood &lt;- function(w, y, x) {\n  z &lt;- w[1] + w[2]*x + w[3]*x^2\n  prob &lt;- 1/(1+exp(-z))\n  -sum(y*log(prob) + (1-y)*log(1-prob))\n}\n\n# Initial values for the parameters\nw_start &lt;- c(0,0,0)\n\n# Use optim() to estimate the parameters\nresult &lt;- optim(w_start, neg_log_likelihood, y=df$y, x=df$x, hessian=TRUE)\n\ndf %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  theme_bw()+\n  stat_function(fun = function(x) {1/(1+exp(-result$par[1]-result$par[2]*x-result$par[3]*x^2))})",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#introduction-to-linear-regression",
    "href": "statistical-machine-learning/sml.html#introduction-to-linear-regression",
    "title": "Statistical Machine Learning",
    "section": "Introduction to Linear Regression",
    "text": "Introduction to Linear Regression\nFor linear regression, we are given data points \\((\\mathbf{x}_i, y_i)\\) which we can express as a linear combination \\(\\mathbf{x}_i^T \\mathbf{w} + w_0 = y_i\\). Sometimes, we also include \\(w_0\\) in \\(\\mathbf{w}\\) and \\(1\\) in \\(\\mathbf{x}_i\\). Now we can write \\[\n\\mathbf{X}^T\\mathbf{w} + \\mathbf{e}= \\mathbf{y}\n\\]\nwhere \\(\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 & … & \\mathbf{x}_n \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix}y_1 & … & y_n \\end{bmatrix}^T\\) and \\(\\mathbf{e}\\) is the error term.\n\\[\n\\underbrace{\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}^T\n}_{\\mathbf{X}^T}\n\\underbrace{\n\\begin{bmatrix}\n9.5 \\\\\n-2.3\n\\end{bmatrix}\n}_\\mathbf{w}\n+\n\\underbrace{\n\\begin{bmatrix}\n0.8 \\\\ -0.9 \\\\ -0.6 \\\\ 0.7\n\\end{bmatrix}\n}_\\mathbf{e}\n=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n9.5 \\\\\n-2.3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n0.8 \\\\ -0.9 \\\\ -0.6 \\\\ 0.7\n\\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n8 \\\\ 4 \\\\ 2 \\\\ 1\n\\end{bmatrix}\n}_\\mathbf{y}\n\\]\n\nNote on notation: Be careful, this is different to how economics people usually write the problem in a slightly different way, as they fill the \\(X\\) matrix differently so it does not need to be transposed. They also call the weights vector \\(\\mathbf{\\beta}\\). But both produce the same output.\n\nHere’s a simple example to show the difference in the notation between Econometrics and Machine Learning (or the slides).\n\nx &lt;- c(1,2,3,4)\ny &lt;- c(8,4,2,1)\n\nX_econ &lt;- cbind(1, x)\nX_econ\n\n       x\n[1,] 1 1\n[2,] 1 2\n[3,] 1 3\n[4,] 1 4\n\nX_ML &lt;- rbind(1, x)\nX_ML\n\n  [,1] [,2] [,3] [,4]\n     1    1    1    1\nx    1    2    3    4\n\nw &lt;- solve(X_ML %*% t(X_ML)) %*% X_ML %*% y\nw\n\n  [,1]\n   9.5\nx -2.3\n\ny_hat &lt;- t(X_ML) %*% w\ny_hat\n\n     [,1]\n[1,]  7.2\n[2,]  4.9\n[3,]  2.6\n[4,]  0.3\n\n\nThe first row consisting of only 1’s is there for the intercept. In the 2d case you can think of it as the y-axis intercept, if y is your output variable.\nAnd graphically, it looks like this.\n\n\nCode\ndata.frame(x, y) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  geom_abline(intercept = w[1], slope = w[2])+\n  coord_cartesian(xlim = c(0, 5), ylim = c(0, 10))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLinear regression has a analytical solution when you try to minimize the squared error \\[\\begin{align*}\n\\mathbf{w} &= \\underset{\\mathbf{w}}{\\arg\\min} \\ \\sum_{i=1}^N\\left(\\mathbf{X}^T\\mathbf{w} - \\mathbf{y} \\right)^2 \\\\\n&= \\underset{\\mathbf{w}}{\\arg\\min} \\ \\mathbf{e}^T\\mathbf{e} \\\\\n\\end{align*}\\] which, after derivation and setting zero leads to \\[\n\\mathbf{w} = \\left(\\mathbf{X} \\mathbf{X}^T \\right)^{-1} \\mathbf{X} \\mathbf{y}\n\\] as has been used in the code above.\n\nNote on notation:\nIn my notation here, \\(\\mathbf{X}\\) cannot only contain the “raw” data, but also features, for example a quadratic term, an interaction term or something else. On the slides, \\(\\mathbf{\\Phi}\\) was used to indicate that features are included.\n\nWhat are features? Let’s assume we are given a data set with \\(y\\) as the output variable and \\(x\\) and \\(z\\) as the input variables. We could then, for example, use a linear and quadratic features for \\(x\\) and \\(z\\) as well as a interaction term \\(x\\cdot z\\): \\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\nx_1 & x_2 & \\cdots & x_n \\\\\nx_1^2 & x_2^2 & \\cdots & x_n^2 \\\\\nz_1 & z_2 & \\cdots & z_n \\\\\nz_1^2 & z_2^2 & \\cdots & z_n^2 \\\\\nx_1z_1 & x_2z_2 & \\cdots & x_nz_n\n\\end{bmatrix}\n\\]\nHere is another example. We first generate some data points with \\(y = 2 + 3x - 0.5x^2 + 2\\sin(x)\\) and add some noise. Then we create the \\(\\mathbf{X}\\) matrix with the features \\(1,\\ x,\\ x^2,\\ \\sin(x)\\) and solve for the weights vector \\(w\\).\n\n\nCode\nset.seed(1)\nn &lt;- 100\n\nx &lt;- runif(n, min = 0, max = 10)\ne &lt;- rnorm(n, mean = 0, sd = 1)\ny &lt;- 2 + 3*x - 0.5*x^2 + 2*sin(x) + e\n\nX &lt;- rbind(1, x, x^2, sin(x))\nw &lt;- solve(X %*% t(X)) %*% X %*% y\nw\n\n\n        [,1]\n   2.0790292\nx  2.9033481\n  -0.4865858\n   1.7292521\n\n\nAs we can see, the estimates are pretty close. And here is the graph.\n\n\nCode\ndata.frame(x, y) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  geom_function(fun = function(x) w[1] + w[2]*x + w[3]*x^2 + w[4]*sin(x))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nJust be careful and don’t make your model too large with too many features (like polynomials of degree 20) as this will probably lead to overfitting.\nNote that this is still a linear regression problem, as it is linear in it’s parameters. Just the data is transformed. \\(y = a\\sin(bx)\\) would be an example of a non-linear regression problem, which cannot be solved this way. Some problems, like \\(y = \\exp(ax)\\) might look like they are not linear, but can be transformed into a linear problem: \\(\\ln(y) = ax\\).\nNow let’s take a look at what could go wrong in this approach. The largest numerical problem can arise with the inversion of \\(\\mathbf{X}\\mathbf{X}^T\\), in particular if the rows of \\(\\mathbf{X}\\) are strongly linearly dependent, as \\(\\mathbf{X}\\mathbf{X}^T\\) is not full rank anymore. \\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\nx_1 & x_2 & \\cdots & x_n \\\\\n2x_1 & 2x_2 & \\cdots & 2x_n\n\\end{bmatrix}\n\\] Let’s check that with our example from before.\n\nx &lt;- c(1,2,3,4)\ny &lt;- c(8,4,2,1)\n\nX &lt;- rbind(1, x, 2*x)\nsolve(X %*% t(X)) %*% X %*% y\n\nError in solve.default(X %*% t(X)): Lapack routine dgesv: system is exactly singular: U[3,3] = 0\n\nrankMatrix(X)[1]\n\n[1] 2\n\n\nAs expected, the inversion fails because the matrix is not full rank.\nSimilarly, the inversion might run into numerical issues if two features (rows of the \\(\\mathbf{X}\\) matrix) are very similar. This leads us straight to ridge regression, which will be discussed later.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#maximum-likelihood-approach",
    "href": "statistical-machine-learning/sml.html#maximum-likelihood-approach",
    "title": "Statistical Machine Learning",
    "section": "Maximum Likelihood approach",
    "text": "Maximum Likelihood approach\nLet’s take a deeper look at the error term \\(\\mathbf{e}\\) in \\(\\mathbf{X}^T\\mathbf{w} + \\mathbf{e}= \\mathbf{y}\\). We need to include this term, because our linear combination of \\(\\mathbf{X}\\) and \\(\\mathbf{w}\\) will not perfectly come out to \\(y\\) with real world data. But until now, we did not assume much about \\(\\mathbf{e}\\). Let’s now assume, that it is normally distributed following \\(e \\sim N(0, \\sigma^2)\\). Now all of a sudden, \\(y\\) is also randomly distributed with \\(y_i \\sim N(\\mathbf{x}_i^T\\mathbf{w},\\sigma^2)\\)\n\n\n\nDistribution of y in Linear Regression\n\n\nNow, we can not only get the solution to OLS by minimizing the squared error, but also by maximizing the likelihood of observing \\(y\\) in our data. \\[\np(\\mathbf{y} | \\mathbf{X}; \\mathbf{w}) = \\prod_{i=1}^n N(y_i | \\mathbf{x}_i^T\\mathbf{w},\\sigma^2)\n\\]\nAs before, we don’t maximize this likelihood function, but the log of this function. We then differentiate with respect to \\(\\mathbf{w}\\) and set it to zero. We then find, that this approach leads to the exact same solution as the least squares approach from before. That means, that even before, we implicitly assumed a distribution!\nWhen we differentiate the log-likelihood function with respect to \\(\\sigma^2\\), set it to zero and solve it we find a equation to estimate the variance \\(\\sigma^2 = \\frac{1}{n} \\mathbf{e}^T\\mathbf{e}\\)\nTODO: Loss function in regression",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#bias-and-variance-in-linear-regression",
    "href": "statistical-machine-learning/sml.html#bias-and-variance-in-linear-regression",
    "title": "Statistical Machine Learning",
    "section": "Bias and Variance in Linear Regression",
    "text": "Bias and Variance in Linear Regression\nTODO",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#bayesian-linear-regression",
    "href": "statistical-machine-learning/sml.html#bayesian-linear-regression",
    "title": "Statistical Machine Learning",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\nOverfitting is a big problem, especially in smaller datasets, as shown by the following example with polynomial degree 5.\n\n\nCode\nset.seed(1)\nn_1 &lt;- 10\nx_1 &lt;- runif(n_1, min = 0, max = 5)\ny_1 &lt;- 2 + 3*x_1 - 0.5*x_1^2 + rnorm(n_1, mean = 0, sd = .5)\n\nset.seed(2)\ny_2 &lt;- 2 + 3*x_1 - 0.5*x_1^2 + rnorm(n_1, mean = 0, sd = .5)\n\np1 &lt;- data.frame(x = x_1, y_1, y_2) %&gt;%\n  ggplot()+\n  geom_point(aes(x = x, y = y_1), shape = 3)+\n  geom_smooth(aes(x = x, y = y_1),method = \"lm\", se = FALSE,\n              formula = y ~ poly(x, 5),\n              alpha = .5, color = \"red\")+\n  geom_point(aes(x = x, y = y_2), shape = 4)+\n  geom_smooth(aes(x = x, y = y_2),method = \"lm\", se = FALSE,\n              formula = y ~ poly(x, 5),\n              alpha = .5, color = \"blue\")+\n  theme_bw()+\n  labs(subtitle = paste(\"N =\", n_1),\n       y = \"y\")+\n  coord_cartesian(ylim = c(2.5, 7.5))\n\nn_3 &lt;- 200\nx_3 &lt;- runif(n_3, min = 0, max = 5)\ny_3 &lt;- 2 + 3*x_3 - 0.5*x_3^2 + rnorm(n_3, mean = 0, sd = .5)\n\np2 &lt;- data.frame(x = x_3, y = y_3) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE,\n              formula = y ~ poly(x, 5))+\n  theme_bw()+\n  labs(subtitle = paste(\"N =\", n_3))+\n  coord_cartesian(ylim = c(2.5, 7.5))\n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\n\nIf we sampled 10 different points, our estimates would change drastically. This is shown in the left plot, where two different samples with their fitted line are overlayed. We therefore say that OLS has a high variance. To avoid that, we can use a prior distribution over the parameters \\(p(\\mathbf{w})\\). \\[\n\\underbrace{p(\\mathbf{w}|\\mathbf{X},\\mathbf{y})}_\\text{Posterior} \\propto \\underbrace{p(\\mathbf{y}|\\mathbf{X},\\mathbf{w})}_\\text{Likelihood} \\cdot \\underbrace{p(\\mathbf{w})}_\\text{Prior}\n\\]\nThis prior could be a Gaussian, so assume that each single weight \\(w_j\\) from the weights vector \\(\\mathbf{w}\\) is distributed normally so that we get \\(\\mathbf{w} \\sim N(\\mathbf{0}, \\sigma_0^2\\mathbf{I})\\). That means that, before having seen any data, we believe all the weights to be rather small. And that we need to see a lot of “evidence” to accept large weights. This will “smooth” out our fitted line, if we think in a graphical sense.\n\nA note on notation:\nFrom the slides, \\(N(\\mathbf{w}; 0, \\sigma_0^2)\\) means the exact same as \\(\\mathbf{w} \\sim N(0, \\sigma_0^2)\\). The \\(\\mathbf{w}\\) in the normal distribution is just there to indicate the variable that is drawn from the distribution.\n\\(p(\\mathbf{w}|\\mathbf{X},\\mathbf{y};\\sigma_0,\\sigma)\\) is the conditional distribution of \\(\\mathbf{w}\\) given the data \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) and the parameters \\(\\sigma_0\\) (from the prior) and \\(\\sigma\\) (from the error term). The \\(;\\) is there to differentiate types of parameters.\n\nNow we want to maximize this posterior probability, or rather the log-probability, because that’s easier and leads to the same result. Remember that the log splits a product into the sum of the logs and remember that we assumed a distribution for the error term with \\(e \\sim N(0,\\sigma^2)\\), which lead to \\(\\mathbf{y} \\sim N(\\mathbf{X}^T\\mathbf{w},\\sigma^2)\\)\n\\[\n\\begin{align*}\n\\mathbf{w}_\\text{MAP} &= \\underset{\\mathbf{w}}{\\arg\\max}\\ p(\\mathbf{w}|\\mathbf{X},\\mathbf{y}) \\\\\n&= \\underset{\\mathbf{w}}{\\arg\\max}\\ p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}) \\cdot p(\\mathbf{w})\\\\\n&= \\underset{\\mathbf{w}}{\\arg\\max}\\ \\log p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}) + \\log p(\\mathbf{w}) \\\\\n&= \\underset{\\mathbf{w}}{\\arg\\max}\\ \\log p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}) + \\log N(\\mathbf{0}, \\sigma_0^2\\mathbf{I}) \\\\\n& \\cdots \\\\\n&= (\\mathbf{X}\\mathbf{X}^T + \\frac{\\sigma^2}{\\sigma_0^2}\\mathbf{I})^{-1} \\mathbf{X} \\mathbf{y}\n\\end{align*}\n\\]\nFor reference, this video has (almost) all the math in it.\nThe term \\(\\frac{\\sigma^2}{\\sigma_0^2}\\mathbf{I}\\) is a regularization term. The larger it gets, the “smoother” the output. For easier readability, we write \\(\\frac{\\sigma^2}{\\sigma_0^2} = \\lambda\\).\n\n\nCode\n# Data generation\nset.seed(1)\nn &lt;- 20\nx &lt;- runif(n, min = 0, max = 5)\ny &lt;- 2 + 3*x - 0.5*x^2 + rnorm(n, mean = 0, sd = .5)\n\n# Plot 1\n# Solving\np &lt;- 5\nr &lt;- 0\nX &lt;- t(poly(x, p, raw = TRUE, simple = TRUE))\nw &lt;- solve(X %*% t(X)+ r*diag(p)) %*% X %*% y\n\n# Plotting\nx_plot &lt;- seq(from = min(x), to = max(x), by = 0.01)\nX_plot &lt;- t(poly(x_plot, p, raw = TRUE, simple = TRUE))\ny_plot &lt;- t(X_plot) %*% w\n\np1 &lt;- data.frame(x = x, y = y) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  geom_line(data = data.frame(x = x_plot, y = y_plot), aes(x = x, y = y))+\n  coord_cartesian(ylim = c(3, 7), xlim = c(0,5))+\n  theme_bw()+\n  labs(subtitle = paste(\"N = \", n, \"; λ = \", r, sep = \"\"))+\n  annotate(\"text\", x = 0, y = 6.5, hjust = 0,\n           label = paste(\"w1 = \", round(w[1], 2), \"\\n\", \"w2 = \", round(w[2], 2), \"\\n\", \"…\", sep = \"\"))\n\n# Plot 2\n# Solving\np &lt;- 5\nr &lt;- .5\nX &lt;- t(poly(x, p, raw = TRUE, simple = TRUE))\nw &lt;- solve(X %*% t(X)+ r*diag(p)) %*% X %*% y\n\n# Plotting\nx_plot &lt;- seq(from = min(x), to = max(x), by = 0.01)\nX_plot &lt;- t(poly(x_plot, p, raw = TRUE, simple = TRUE))\ny_plot &lt;- t(X_plot) %*% w\n\np2 &lt;- data.frame(x = x, y = y) %&gt;%\n  ggplot(aes(x = x, y = y))+\n  geom_point()+\n  geom_line(data = data.frame(x = x_plot, y = y_plot), aes(x = x, y = y))+\n  coord_cartesian(ylim = c(3, 7), xlim = c(0,5))+\n  theme_bw()+\n  labs(subtitle = paste(\"N = \", n, \"; λ = \", r, sep = \"\"))+\n  annotate(\"text\", x = 0, y = 6.5, hjust = 0,\n           label = paste(\"w1 = \", round(w[1], 2), \"\\n\", \"w2 = \", round(w[2], 2), \"\\n\", \"…\", sep = \"\"))\n\n\nggarrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\nLet’s remember what exactly \\(\\sigma_0^2\\) stands for: It is the variance from our prior distribution on \\(\\mathbf{w}\\). So the smaller this variance, the closer to 0 we assume the weights to be, before seeing any data. And now take a look at how our estimates for \\(\\mathbf{w}\\) behave for a larger \\(\\lambda\\) (which may be caused by a smaller \\(\\sigma_0^2\\)): The estimates get closer to 0. Exactly like we wanted! And this results in the smoother curve.\nThere is yet another way to arrive at the same outcome: Adding a regularization term to our objective function from OLS. \\[\n\\begin{align*}\n\\mathbf{w} &= \\underset{\\mathbf{w}}{\\arg\\min}\\ \\frac{1}{2}\\|\\mathbf{X}^T\\mathbf{w} - \\mathbf{y}\\|^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2 \\\\\n&= (\\mathbf{X}\\mathbf{X}^T + \\lambda\\mathbf{I})^{-1} \\mathbf{X} \\mathbf{y}\n\\end{align*}\n\\]\nAnother benefit of this regularization term is, that it helps us with the matrix inversion. Without this term and with very similar features, we could run into numerical problems. When we add this regularization term, we basically ensure that the matrix \\(\\mathbf{X}\\mathbf{X}^T + \\lambda\\mathbf{I}\\) is full rank before trying to invert it.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#kernel-methods-for-regression",
    "href": "statistical-machine-learning/sml.html#kernel-methods-for-regression",
    "title": "Statistical Machine Learning",
    "section": "Kernel Methods for Regression",
    "text": "Kernel Methods for Regression\nThe traditional approach of Linear Regression has a limitation — it requires “handcrafted” features. Ideally, we would want to let the number of features \\(d\\) grow to infinity (\\(d \\rightarrow \\infty\\)) and combine this with regularization to automate the feature selection process. However, this leads to computational challenges. For instance, we need to multiply a \\(n \\times d\\) matrix with a \\(d \\times n\\) matrix and then invert a \\(d \\times d\\) matrix to compute \\(\\mathbf{w}\\). When predicting new data points, we need to calculate \\(\\mathbf{X}^T\\mathbf{w}\\), where \\(\\mathbf{w}\\) is of size \\(d \\times 1\\). To overcome these issues, we introduce kernel methods.\nKernels are functions that measure the similarity between data points. They allow us to perform computations in a high-dimensional feature space without explicitly calculating the coordinates of the data points in that space. If \\(\\mathbf{x}\\) only enters our model as \\(\\phi(\\mathbf{x})^T \\phi(\\mathbf{x}^\\prime)\\), we can use a kernel function \\(k(\\mathbf{x}, \\mathbf{x}^\\prime)\\) instead of choosing \\(\\phi(\\mathbf{x})\\). The kernel function is represented as: \\[\nk(\\mathbf{x}, \\mathbf{x}^\\prime) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}^\\prime)\n\\]\nFor instance, consider the features in a regression setting: \\[\n\\phi(\\mathbf{x}) = (\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_1 \\mathbf{x}_2,\\mathbf{x}_1^2,\\mathbf{x}_2^2)\n\\] This transforms our 2-dimensional data \\((x_1,x_2)\\) into a 5-dimensional feature space \\(\\mathbf{\\Phi}(x)\\). While this transformation can be complex, we can simplify it using a kernel, such as a polynomial kernel: \\[\nk(\\mathbf{x}, \\mathbf{x}^\\prime) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}^\\prime) = (1 + \\mathbf{x}^T\\mathbf{x}^\\prime)^d\n\\]\nSome kernels, like the Gaussian kernel or the radial basis function (RBF) kernel, correspond to transformations, such as \\(\\phi(x)\\), which would be infinitely large and thus impossible to find.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#kernelized-ridge-regression",
    "href": "statistical-machine-learning/sml.html#kernelized-ridge-regression",
    "title": "Statistical Machine Learning",
    "section": "Kernelized Ridge Regression",
    "text": "Kernelized Ridge Regression\nIn contrast to much of econometrics, here in machine learning, we care mostly about predicting new data rather than understanding the connection between things. For example, economists might want to study the relation between drinking and deaths in motor vehicle accidents, see how big it is and what other influences there are. Machine learning people might only care about predicting the number of motor vehicle accidents for next year. Thus, they don’t actually need to know the weights \\(\\mathbf{w}\\).\n\nA note on notation:\nNow we will use \\(\\phi(\\mathbf{x})\\) to indicate a (non-linear) transformation to a single “raw” datapoint \\(\\mathbf{x}\\). And \\(\\mathbf{\\Phi}(\\mathbf{X})\\) denotes the transformed \\(\\mathbf{X}\\) matrix containing all the datapoints.\nAnd \\(\\mathbf{X}\\) as well as \\(\\mathbf{\\Phi}\\) are now again filled with one datapoint per row in contrast to how we did it in the regression chapter.\n\nLet’s say we want to predict the outcome \\(\\hat{y}\\) (the hat stands for a prediction) at the (test) input \\(\\mathbf{x}_\\star\\) We can write this as \\[\n\\begin{align*}\n\\hat{y}(\\mathbf{x}_\\star) &= \\mathbf{w}^T \\phi(\\mathbf{x}_\\star) \\\\\n&= \\left(\\left(\\mathbf{\\Phi}^T\\mathbf{\\Phi} + n\\lambda\\mathbf{I}\\right)^{-1} \\mathbf{\\Phi}^T\\mathbf{y} \\right)^T \\phi(\\mathbf{x_\\star}) \\\\\n&= \\mathbf{y}^T \\mathbf{\\Phi} \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi} + n\\lambda\\mathbf{I}\\right)^{-1} \\phi(\\mathbf{x_\\star}) \\\\\n&= \\mathbf{y}^T \\left(\\mathbf{\\Phi} \\mathbf{\\Phi}^T + n\\lambda\\mathbf{I}\\right)^{-1} \\mathbf{\\Phi} \\phi(\\mathbf{x_\\star}) \\\\\n&= \\mathbf{y}^T \\left(\\mathbf{K}(\\mathbf{X},\\mathbf{X}) + n\\lambda\\mathbf{I}\\right)^{-1} \\mathbf{K}(\\mathbf{X},\\mathbf{x_\\star}) \\\\\n&= \\mathbf{a}^T \\mathbf{K}(\\mathbf{X},\\mathbf{x_\\star})\n\\end{align*}\n\\]\n\nRemember:\n\\(y(\\mathbf{x}) = \\phi(\\mathbf{x})^T \\mathbf{w} = \\mathbf{w}^T \\phi(\\mathbf{x})\\) and\n\\(\\mathbf{K}(\\mathbf{x_\\star},\\mathbf{X}) = \\mathbf{K}(\\mathbf{X},\\mathbf{x}_\\star)\\) because it’s just a measure of distance where the order does not play a role.\n\nwith\n\\[\n\\begin{align*}\n\\mathbf{K}(\\mathbf{x_\\star},\\mathbf{X}) &=\n\\begin{bmatrix}\nk(\\mathbf{x}_\\star,\\mathbf{x}_1) \\\\\n\\cdots \\\\\nk(\\mathbf{x}_\\star,\\mathbf{x}_N)\n\\end{bmatrix} \\\\\n\\mathbf{K}(\\mathbf{X},\\mathbf{X}) &=\n\\begin{bmatrix}\nk(\\mathbf{x}_1,\\mathbf{x}_1) &  \\cdots & k(\\mathbf{x}_1,\\mathbf{x}_N) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nk(\\mathbf{x}_N,\\mathbf{x}_1) &  \\cdots & k(\\mathbf{x}_N,\\mathbf{x}_N) \\\\\n\\end{bmatrix} \\\\\n\\mathbf{a} &= \\left(\\mathbf{K}(\\mathbf{X},\\mathbf{X}) + \\lambda\\mathbf{I} \\right)^{-1} \\mathbf{y}\n\\end{align*}\n\\]\nThe important thing to note here is that we can represent every calculation with kernels, so we don’t need to touch \\(\\mathbf{w}\\) or \\(\\mathbf{\\Phi}\\) anymore.\nHere’s an example of kernel ridge regression with a 3d dataset\n\n\nCode\n# Set the seed for reproducibility\nset.seed(123)\n\n# Number of samples\nN &lt;- 1000\n\n# Generate inputs\nx1 &lt;- runif(N, -1, 1)\nx2 &lt;- runif(N, -1, 1)\n\n# Define function f\nf &lt;- function(x1, x2) {\n  sin(pi * x1) + cos(pi * x2)\n}\n\n# Generate outputs with some Gaussian noise\ne &lt;- rnorm(N, 0, 0.1)\ny &lt;- f(x1, x2) + e\n\ndata_train &lt;- data.frame(x1, x2, y)\n\n# Gaussian kernel function\ngaussian_kernel &lt;- function(x1, x2, sigma = 1) {\n  exp(-sum((x1 - x2)^2) / (2 * sigma^2))\n}\n\n# Create the kernel matrix\nK &lt;- matrix(0, N, N)\nfor (i in 1:N) {\n  for (j in 1:N) {\n    K[i, j] &lt;- gaussian_kernel(c(x1[i], x2[i]), c(x1[j], x2[j]))\n  }\n}\n\n# Ridge regression parameter\nlambda &lt;- 0.1\n\n# Solve for alpha\nalpha &lt;- solve(K + lambda * diag(N), y)\n\n# Prediction function\npredict_krr &lt;- function(x_star) {\n  y_hat &lt;- 0\n  for (i in 1:N) {\n    y_hat &lt;- y_hat + alpha[i] * gaussian_kernel(c(x1[i], x2[i]), x_star)\n  }\n  return(y_hat)\n}\n\n# Make a prediction\n#predict_krr(c(0.2, -0.5))\n#f(0.2, -0.5)\n\n\nAnd the corresponding plot, where the red points are from the training data and the surface is the prediction.\n\n\nCode\n# Create a grid of inputs\nx1_grid &lt;- seq(-1, 1, length.out = 30)\nx2_grid &lt;- seq(-1, 1, length.out = 30)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\n# Predict at each input in the grid\ny_grid &lt;- apply(grid, 1, predict_krr)\n\n# Create a 3D plot\nz &lt;- matrix(y_grid, length(x1_grid), length(x2_grid))\nres &lt;- persp(x1_grid, x2_grid, z, theta = 160, phi = 15, expand = .5, col = \"lightblue\", xlab = \"x1\", ylab = \"x2\", zlab = \"y\")\npoints(trans3d(x1, x2, y, pmat = res), col = 2, pch = 16)\n\n\n\n\n\n\n\n\n\nHere’s the same but as a crossection for \\(x_2 = 0\\).\n\n\nCode\n# Create a grid of inputs\nx1_grid &lt;- seq(-1, 1, by = 0.1)\nx2_grid &lt;- seq(-1, 1, by = 0.1)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\n# Predict at each input in the grid\ny_grid &lt;- apply(grid, 1, predict_krr)\n\ngrid_new &lt;- grid %&gt;% cbind(x3 = y_grid)\n\ngrid_new %&gt;%\n  filter(x2 == 0) %&gt;%\n  ggplot(aes(x = x1, y = x3))+\n  geom_line()+\n  geom_point(data = data_train %&gt;% filter(round(x2, 1) == 0), aes(x = x1, y = y))+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe following is another example but with a 2d dataset.\n\n\nCode\n# Set the seed for reproducibility\nset.seed(123)\n\n# Number of samples\nN &lt;- 100\n\n# Generate inputs\nx &lt;- runif(N, -1, 1)\n\n# Define function f\nf &lt;- function(x) {\n  sin(pi * x) + 7 - 0.1*x^2\n}\n\n# Generate outputs with some Gaussian noise\ne &lt;- rnorm(N, 0, .2)\ny &lt;- f(x) + e\n\n# Gaussian kernel function\ngaussian_kernel &lt;- function(x1, x2, sigma = 1) {\n  exp(-sum((x1 - x2)^2) / (2 * sigma^2))\n}\n\n# Create the kernel matrix\nK &lt;- matrix(0, N, N)\nfor (i in 1:N) {\n  for (j in 1:N) {\n    K[i, j] &lt;- gaussian_kernel(x[i], x[j])\n  }\n}\n\n# Ridge regression parameter\nlambda &lt;- 0.1\n\n# Solve for alpha in Ka = y\nalpha &lt;- solve(K + lambda * diag(N), y)\n\n# Prediction function\npredict_krr &lt;- function(x_star) {\n  y_hat &lt;- 0\n  for (i in 1:N) {\n    y_hat &lt;- y_hat + alpha[i] * gaussian_kernel(x[i], x_star)\n  }\n  return(y_hat)\n}\n\n# Test inputs\nx_star &lt;- seq(from = -1.5, to = 1.5, by = 0.01)\ny_hat &lt;- sapply(x_star, predict_krr)\n\nggplot()+\n  geom_point(data = data.frame(x, y), aes(x = x, y = y))+\n  geom_line(data = data.frame(x = x_star, y = y_hat), aes(x = x, y = y))+\n  theme_bw()",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#bayesian-linear-regression-1",
    "href": "statistical-machine-learning/sml.html#bayesian-linear-regression-1",
    "title": "Statistical Machine Learning",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\nNow we know from kernel ridge regression that we don’t actually need \\(\\mathbf{w}\\), if all we care about is predicting new data. So now the idea is to remove \\(\\mathbf{w}\\) by marginalizing over it or integrating it out. \\[\n\\begin{align*}\n\\underbrace{p(\\hat{y}|\\mathbf{x}_\\star, \\mathbf{X}, \\mathbf{y})}_\\text{predictive distribution} &= \\int p(\\hat{y},  \\mathbf{w}|\\mathbf{x}_\\star, \\mathbf{X}, \\mathbf{y}) \\ d\\mathbf{w} \\\\\n&= \\int \\underbrace{p(\\hat{y}|\\mathbf{x}_\\star, \\mathbf{w})}_\\text{regression model} \\ \\underbrace{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y})}_\\text{posterior} \\ d\\mathbf{w}\n\\end{align*}\n\\]\nHere, \\(\\hat{y}\\) is the predicted value based on a test input \\(\\mathbf{x}_\\star\\).\nBoth the regression model and the posterior are normally distributed. For the predictive distribution, we get a closed form solution: \\[\n\\hat{y} \\sim N(\\mu_\\star, \\sigma_\\star)\n\\]\nwith \\[\n\\begin{align*}\n\\mu_\\star &= \\mathbf{\\phi}(\\mathbf{x_\\star})^T \\underbrace{\\left(\\frac{\\sigma^2}{\\sigma_0^2}\\mathbf{I} + \\mathbf{\\Phi}^T\\mathbf{\\Phi} \\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{y}}_{\\mathbf{w}_\\text{MAP}} \\\\\n\\sigma_\\star &= \\frac{1}{\\sigma^2} + \\mathbf{\\phi}(\\mathbf{x_\\star})^T \\left(\\frac{1}{\\sigma_0^2}\\mathbf{I} + \\frac{1}{\\sigma^2} \\mathbf{\\Phi}^T \\mathbf{\\Phi} \\right)^{-1} \\mathbf{\\phi}(\\mathbf{x_\\star})\n\\end{align*}\n\\]\nNotice that \\(\\mu_\\star\\) is just a prediction using “normal” ridge regression with the prior \\(p(\\mathbf{w}) = N(0, \\sigma_0^2 \\mathbf{I})\\).\n\nNote on notation:\nBe careful. The slides use an inconsistent notation for \\(\\mathbf{\\Phi}\\) (or \\(\\mathbf{X}\\)). Now it seems like the datapoints are in the rows and features are in the columns, as it’s found in econometrics. Before, it was the other way around.\n\nLet’s look at an example. Here, we create some data points from the polynomial function \\(y = 3 + 5x - 2x^2 + 0.3x^3\\) with noise. We then use a polynomial of degree 8 (including the 1 for the intercept) as the feature function for the regression. We also need to choose the (expected?) data variance \\(\\sigma^2\\) and prior variance for our weights \\(\\sigma_0^2\\).\n\n\nCode\nset.seed(1)\n\n# Create some test data\nn &lt;- 15\nx &lt;- runif(n, min = 0, max = 5)\ny &lt;- 3 + 5*x - 2*x^2 + 0.3*x^3 + rnorm(n, mean = 0, sd = 1)\ny &lt;- y - mean(y)\n\ndata &lt;- data.frame(x, y)\n\n#data %&gt;%\n#  ggplot(aes(x = x, y = y))+\n#  geom_point()+\n#  theme_bw()\n\n\n\n\nCode\n# Define your hyperparameters\nsigma &lt;- 1\nsigma_0 &lt;- 2\np &lt;- 8\n\n# Define your basis functions\nphi_fun &lt;- function(x) {\n  c(1, poly(x, degree = p-1, raw = TRUE, simple = TRUE))\n}\n\n# Calculate Phi matrix for training data\nPhi &lt;- t(sapply(x, phi_fun))\n\n# Define a sequence of test inputs\nx_star &lt;- seq(min(x)-1.5, max(x)+1.5, by = 0.01)\n\n# Calculate mu_star and sigma_star for each x_star\nmu_star &lt;- sapply(x_star, function(x_star) {\n  phi_star &lt;- phi_fun(x_star)\n  t(phi_star) %*% solve((sigma^2/sigma_0^2) * diag(p) + t(Phi) %*% Phi) %*% t(Phi) %*% data$y\n})\n\nsigma_star &lt;- sapply(x_star, function(x_star) {\n  phi_star &lt;- phi_fun(x_star)\n  1/sigma^2 + t(phi_star) %*% solve(1/sigma_0^2 * diag(p) + 1/sigma^2 * t(Phi) %*% Phi) %*% phi_star\n})\n\ndata.frame(x_star, mu_star, sigma_star) %&gt;%\n  mutate(conf_interval_90 = qnorm(0.95, mean = 0, sd = sigma_star),\n         conf_interval_99 = qnorm(0.995, mean = 0, sd = sigma_star)) %&gt;%\n  ggplot()+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_99, ymax = mu_star+conf_interval_99, fill = \"99\"), alpha = 0.2)+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_90, ymax = mu_star+conf_interval_90, fill = \"90\"), alpha = 0.2)+\n  geom_line(aes(x = x_star, y = mu_star))+\n  geom_point(data = data, aes(x = x, y = y))+\n  scale_fill_manual(values = c(\"90\" = \"blue\", \"99\" = \"red\"),\n                    labels = c(\"90%\", \"99%\"))+\n  theme_bw()+\n  coord_cartesian(ylim = c(min(data$y)-5, max(data$y)+5))+\n  theme(legend.position = \"top\")+\n  labs(fill = \"Confidence Intervals\",\n       x = \"x\",\n       y = \"y\")\n\n\n\n\n\n\n\n\n\nFor every test input \\(x_\\star\\) we get a mean \\(\\mu_\\star\\) and a variance \\(\\sigma_\\star\\). These values fully describe a normal distribution around the mean. To visualize this, we calculated two confidence intervals and plotted them. The blue 90% confidence interval means, that 90% of the expected outcomes would lie inside that ribbon. As we hoped, the confidence intervals get larger for regions of \\(x\\) with little or no training data. Interestingly, the confidence intervals diverge drastically after the last training data point, while not doing so before the first. This is probably because the polynomial used as the model will diverge very fast for larger values of \\(x\\), while being relatively stable for values around 0.\nTweaking \\(\\sigma^2\\) changes the confidence intervals, while changing \\(\\sigma_0^2\\) mostly changes the means.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#gaussian-processes-regression",
    "href": "statistical-machine-learning/sml.html#gaussian-processes-regression",
    "title": "Statistical Machine Learning",
    "section": "Gaussian Processes Regression",
    "text": "Gaussian Processes Regression\nGaussian processes are now a combination of kernel ridge regression and the Bayesian approach. The idea here is to formulate the predictive distribution from Bayesian linear regression using kernels. The result is what we call Gaussian process regression.\nWe need training data \\(\\mathcal{T} = \\{\\mathbf{x}_i, y_i\\}_{i=1}^n\\), a kernel \\(k(\\mathbf{x},\\mathbf{x}^\\prime)\\), a noise variance \\(\\sigma^2\\) and a test input \\(\\mathbf{x}_\\star\\).\nWe then can compute \\(m_\\star\\) and \\(s_\\star\\) \\[\n\\begin{align*}\n\\mu_\\star &= \\mathbf{K}(\\mathbf{X}, \\mathbf{x}_\\star)^T \\left(\\sigma^2\\mathbf{I} + \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) \\right)^{-1} \\mathbf{y} \\\\\n\\sigma^2_\\star &= k(\\mathbf{x}_\\star, \\mathbf{x}_\\star) - \\mathbf{K}(\\mathbf{X}, \\mathbf{x}_\\star)^T \\left(\\sigma^2\\mathbf{I} + \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) \\right)^{-1} \\mathbf{K}(\\mathbf{X}, \\mathbf{x}_\\star)\n\\end{align*}\n\\] to find the posterior predictive function \\(p(f(\\mathbf{x}_\\star)|\\mathbf{y}) = N\\left((f(\\mathbf{x}_\\star);\\mu_\\star, \\sigma^2_\\star \\right)\\).\n\n\nCode\nsigma &lt;- .7\n\ngaussian_kernel &lt;- function(x1, x2) {\n  exp(-sum((x1 - x2)^2) / (2 * sigma^2))\n}\n\nK_X_X &lt;- matrix(0, nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    K_X_X[i, j] &lt;- gaussian_kernel(data$x[i], data$x[j])\n  }\n}\n\nK_X_X_inv &lt;- solve(sigma^2 * diag(n) + K_X_X)\n\nx_seq &lt;- seq(min(data$x)-1.5, max(data$x)+1.5, by = 0.01)\nMu_star &lt;- vector(length = length(x_seq))\nSigma_star &lt;- vector(length = length(x_seq))\nfor(k in 1:length(x_seq)){\n  x_star &lt;- x_seq[k]\n  \n  K_X_xstar &lt;- matrix(0, nrow = n, ncol = 1)\n  for(i in 1:n){\n    K_X_xstar[i] &lt;- gaussian_kernel(data$x[i], x_star)\n  }\n  \n  K_xstar_xstar &lt;- gaussian_kernel(x_star, x_star)\n  \n  mu_star &lt;- t(K_X_xstar) %*% K_X_X_inv %*% data$y\n  sigma_star &lt;- K_xstar_xstar - t(K_X_xstar) %*% K_X_X_inv %*% K_X_xstar\n  \n  Mu_star[k] &lt;- mu_star\n  Sigma_star[k] &lt;- sigma_star\n}\n\ndata.frame(\"x_star\" = x_seq, \"mu_star\" = Mu_star, \"sigma_star\" = Sigma_star) %&gt;%\n  mutate(conf_interval_90 = qnorm(0.95, mean = 0, sd = sigma_star),\n         conf_interval_99 = qnorm(0.995, mean = 0, sd = sigma_star)) %&gt;%\n  ggplot()+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_99, ymax = mu_star+conf_interval_99, fill = \"99\"), alpha = 0.2)+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_90, ymax = mu_star+conf_interval_90, fill = \"90\"), alpha = 0.2)+\n  geom_line(aes(x = x_star, y = mu_star))+\n  geom_point(data = data, aes(x = x, y = y))+\n  scale_fill_manual(values = c(\"90\" = \"blue\", \"99\" = \"red\"),\n                    labels = c(\"90%\", \"99%\"))+\n  theme_bw()+\n  #coord_cartesian(ylim = c(min(data$y)-5, max(data$y)+5))+\n  theme(legend.position = \"top\")+\n  labs(fill = \"Confidence Intervals\",\n       x = \"x\",\n       y = \"y\")\n\n\n\n\n\n\n\n\n\nThe benefits of the Gaussian Process Regression are clear: We don’t need to specify features and in contrast to Bayesian Linear Regression, the predictions for \\(x_\\star\\) that are outside of the region where we have training data, don’t diverge but go towards 0.\nBut it seems like this approach needs centered data. Otherwise and with a too large \\(\\sigma\\), the fitted line does not even reach the data but stays near zero.\nThe hyperparameter \\(\\sigma\\) is very sensitive here and wrong values can easily lead to nonsensical results:\n\n\nCode\nsigma &lt;- .1\n\ngaussian_kernel &lt;- function(x1, x2) {\n  exp(-sum((x1 - x2)^2) / (2 * sigma^2))\n}\n\nK_X_X &lt;- matrix(0, nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    K_X_X[i, j] &lt;- gaussian_kernel(data$x[i], data$x[j])\n  }\n}\n\nK_X_X_inv &lt;- solve(sigma^2 * diag(n) + K_X_X)\n\nx_seq &lt;- seq(min(data$x)-1.5, max(data$x)+1.5, by = 0.01)\nMu_star &lt;- vector(length = length(x_seq))\nSigma_star &lt;- vector(length = length(x_seq))\nfor(k in 1:length(x_seq)){\n  x_star &lt;- x_seq[k]\n  \n  K_X_xstar &lt;- matrix(0, nrow = n, ncol = 1)\n  for(i in 1:n){\n    K_X_xstar[i] &lt;- gaussian_kernel(data$x[i], x_star)\n  }\n  \n  K_xstar_xstar &lt;- gaussian_kernel(x_star, x_star)\n  \n  mu_star &lt;- t(K_X_xstar) %*% K_X_X_inv %*% data$y\n  sigma_star &lt;- K_xstar_xstar - t(K_X_xstar) %*% K_X_X_inv %*% K_X_xstar\n  \n  Mu_star[k] &lt;- mu_star\n  Sigma_star[k] &lt;- sigma_star\n}\n\np1 &lt;- data.frame(\"x_star\" = x_seq, \"mu_star\" = Mu_star, \"sigma_star\" = Sigma_star) %&gt;%\n  mutate(conf_interval_90 = qnorm(0.95, mean = 0, sd = sigma_star),\n         conf_interval_99 = qnorm(0.995, mean = 0, sd = sigma_star)) %&gt;%\n  ggplot()+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_99, ymax = mu_star+conf_interval_99), alpha = 0.2, fill = \"red\")+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_90, ymax = mu_star+conf_interval_90), alpha = 0.2, fill = \"blue\")+\n  geom_line(aes(x = x_star, y = mu_star))+\n  geom_point(data = data, aes(x = x, y = y))+\n  scale_fill_manual(values = c(\"90\" = \"blue\", \"99\" = \"red\"),\n                    labels = c(\"90%\", \"99%\"))+\n  theme_bw()+\n  #coord_cartesian(ylim = c(min(data$y)-5, max(data$y)+5))+\n  theme(legend.position = \"top\")+\n  labs(fill = \"Confidence Intervals\",\n       x = \"x\",\n       y = \"y\",\n       subtitle = paste(\"σ = \", sigma, sep = \"\"))\n\n\n###############\n\nsigma &lt;- 10\n\ngaussian_kernel &lt;- function(x1, x2) {\n  exp(-sum((x1 - x2)^2) / (2 * sigma^2))\n}\n\nK_X_X &lt;- matrix(0, nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    K_X_X[i, j] &lt;- gaussian_kernel(data$x[i], data$x[j])\n  }\n}\n\nK_X_X_inv &lt;- solve(sigma^2 * diag(n) + K_X_X)\n\nx_seq &lt;- seq(min(data$x)-1.5, max(data$x)+1.5, by = 0.01)\nMu_star &lt;- vector(length = length(x_seq))\nSigma_star &lt;- vector(length = length(x_seq))\nfor(k in 1:length(x_seq)){\n  x_star &lt;- x_seq[k]\n  \n  K_X_xstar &lt;- matrix(0, nrow = n, ncol = 1)\n  for(i in 1:n){\n    K_X_xstar[i] &lt;- gaussian_kernel(data$x[i], x_star)\n  }\n  \n  K_xstar_xstar &lt;- gaussian_kernel(x_star, x_star)\n  \n  mu_star &lt;- t(K_X_xstar) %*% K_X_X_inv %*% data$y\n  sigma_star &lt;- K_xstar_xstar - t(K_X_xstar) %*% K_X_X_inv %*% K_X_xstar\n  \n  Mu_star[k] &lt;- mu_star\n  Sigma_star[k] &lt;- sigma_star\n}\n\np2 &lt;- data.frame(\"x_star\" = x_seq, \"mu_star\" = Mu_star, \"sigma_star\" = Sigma_star) %&gt;%\n  mutate(conf_interval_90 = qnorm(0.95, mean = 0, sd = sigma_star),\n         conf_interval_99 = qnorm(0.995, mean = 0, sd = sigma_star)) %&gt;%\n  ggplot()+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_99, ymax = mu_star+conf_interval_99), alpha = 0.2, fill = \"red\")+\n  geom_ribbon(aes(x = x_star, ymin = mu_star-conf_interval_90, ymax = mu_star+conf_interval_90), alpha = 0.2, fill = \"blue\")+\n  geom_line(aes(x = x_star, y = mu_star))+\n  geom_point(data = data, aes(x = x, y = y))+\n  scale_fill_manual(values = c(\"90\" = \"blue\", \"99\" = \"red\"),\n                    labels = c(\"90%\", \"99%\"))+\n  theme_bw()+\n  #coord_cartesian(ylim = c(min(data$y)-5, max(data$y)+5))+\n  theme(legend.position = \"top\")+\n  labs(fill = \"Confidence Intervals\",\n       x = \"x\",\n       y = \"y\",\n       subtitle = paste(\"σ = \", sigma, sep = \"\"))\n\n##############\n\nggarrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\nFor \\(\\sigma\\) values too small, the model shows massive overfitting and for large \\(\\sigma\\)s, we can see extreme underfitting.\nAnother problem of this other kernel methods is scaling with larger datasets, because we need to invert the \\(\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) \\in \\mathbb{R}^{N \\times N}\\) matrix. This works only up to around 50 thousand datapoints at the extreme. Normal computer will already struggle with a few thousend datapoints. Bayesian linear regression does not have this problem, because there the largest inversion is of \\(\\mathbf{\\Phi}^T \\mathbf{\\Phi} \\in \\mathbb{R}^{D \\times D}\\) where \\(D\\) is the number of features. This runs much, much faster. This problem of (not) scaling is one reason for the rise of neural networks, which can deal with much larger dataset sizes.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#motivation",
    "href": "statistical-machine-learning/sml.html#motivation",
    "title": "Statistical Machine Learning",
    "section": "Motivation",
    "text": "Motivation\nLatent variables are ones that we don’t observe. This could be the class an object truly belongs to (clustering) or the desire to buy something, when we only observe the final decision (logistic regression).",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#linear-dimensionality-reduction",
    "href": "statistical-machine-learning/sml.html#linear-dimensionality-reduction",
    "title": "Statistical Machine Learning",
    "section": "Linear Dimensionality Reduction",
    "text": "Linear Dimensionality Reduction\nOur goal for linear dimensionality reduction is to find a mapping \\(\\mathbf{x}_n \\rightarrow \\mathbf{z}_n\\), where \\(\\mathbf{x}_n\\) is the original data point with a high dimension \\(D\\) and \\(\\mathbf{z}_n\\) is a low dimensional representation of \\(\\mathbf{x}_n\\) with dimension \\(M \\ll D\\). This mapping should be a linear function \\(\\mathbf{z}_n = \\mathbf{B}\\mathbf{x}_n\\) with \\(\\mathbf{B} \\in \\mathbb{R}^{M \\times D}\\).\n\nNote on notation:\nHere, we write \\(\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 & \\cdots & \\mathbf{x}_N \\end{bmatrix} \\in \\mathbb{R}^{D \\times N}\\) again. So one column for each datapoint.\n\nIn the projection or mapping, we want to keep as much of the variation in the data as possible. We then find the variance and mean of the projection as \\[\n\\begin{align*}\n\\bar{z} &= \\mathbf{u}^T\\mathbf{\\mu} \\\\\n\\sigma^2 &= \\mathbf{u}^T\\mathbf{C}\\mathbf{u} \\qquad \\text{with}\\ \\mathbf{C} = \\frac{1}{N}\\mathbf{X}\\mathbf{X}^T\n\\end{align*}\n\\] where \\(\\mathbf{C}\\) is the covariance matrix of the data. The first principal direction \\(\\mathbf{u}_1\\) is the direction along which the data variance is maximal. The second principal direction then maximizes the variance of the data in the orthogonal complement of the first principal direction.\nThis maximization problem can be written as \\[\n\\mathbf{u}_1 = \\underset{\\mathbf{u}}{\\arg\\max}\\ \\mathbf{u}^T\\mathbf{C}\\mathbf{u} \\qquad \\text{s.t.}\\ \\mathbf{u}^T\\mathbf{u}=1\n\\] \\(\\mathbf{u}^T\\mathbf{u}=1\\) is the restriction that \\(\\mathbf{u}\\) has to be a unit vector.\nWe then find that we just need to compute the Eigenvectors and Eigenvalues of the data’s covariance matrix \\(\\mathbf{C}\\). Then we choose the first \\(M\\) (for example 2 for a 2d plot) Eigenvectors based on the size of the Eigenvalues.\nIn the following example, we again use the iris dataset with it’s 4 dimensions (excluding the labels).\n\n\nCode\niris &lt;- read_csv(\"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\")\n\n# Standarize data by mean centering and setting variances to 1\niris_prepped &lt;- iris %&gt;%\n  select(1:4) %&gt;%\n  scale() \n\n# Calculating covariance matrix and eigenvectors/values\niris_cov &lt;- 1/nrow(iris_prepped) * t(iris_prepped) %*% iris_prepped\n\niris_eigenvalues &lt;- eigen(iris_cov)$values\niris_eigenvectors &lt;- eigen(iris_cov)$vectors # already ordered\n\n# Projecting data\niris_PC &lt;- iris_prepped %*% iris_eigenvectors[, 1:2]\n\niris_PC %&gt;%\n  data.frame() %&gt;%\n  rename(\"PC1\" = 1, \"PC2\" = 2) %&gt;%\n  cbind(variety = iris$variety) %&gt;%\n  ggplot(aes(x = PC1, y = PC2, color = variety))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nOne more example using data from Spotify. I created a playlist with songs from 3 different genres: Hip Hop, Rock and Classical. Spotify gives us many different features for each song, such as danceability, speechiness or duration. We use 12 of these features, so our data is 12-dimensional. Let’s see if we can see a difference between the three genres.\n\n\nCode\nmusic_complete &lt;- read_csv(\"data/music_export.csv\")\nmusic_labels &lt;- music_complete %&gt;%\n  select(genre)\nmusic &lt;- music_complete %&gt;%\n  select(danceability:tempo, track.duration_ms) %&gt;%\n  scale()\n\nmusic_cov &lt;- cov(music)\nmusic_eigenvalues &lt;- eigen(music_cov)$values\nmusic_eigenvectors &lt;- eigen(music_cov)$vectors\n\nmusic_PC &lt;- music %*% music_eigenvectors[, 1:2]\n\ndata.frame(music_PC, music_labels) %&gt;%\n  rename(\"PC1\" = 1, \"PC2\" = 2) %&gt;%\n  ggplot(aes(x = PC1, y = PC2, color = genre))+\n  geom_point()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nWe can definetly see how classical music is different from hip hop and rock. But they on the other hand seem rather similar. Taking a closer look at the eigenvectors, we can see that energy (scaled by -0.42) makes up the largest part of the first principal component. So songs that have a low PC1 value are higher in energy. This makes sense when comparing classical music to hip hop.\nWe can also check what percentage of the variance is explained by two principal components using a scree plot. For that, we divide the eigenvalues by the sum of the eigenvalues, to get the percentage of variacne explained.\n\n\nCode\nvariance_explained &lt;- music_eigenvalues/sum(music_eigenvalues)\ncumulative_variance &lt;- cumsum(variance_explained)\n\nscree &lt;- data.frame(Principal_Component = 1:length(music_eigenvalues), \n                 Variance_Explained = variance_explained,\n                 Cumulative_Variance = cumulative_variance)\n\nggplot(scree, aes(x = Principal_Component)) +\n  geom_line(aes(y = Variance_Explained, color = \"variance\")) +\n  geom_line(aes(y = Cumulative_Variance, color = \"cum_variance\"))+\n  scale_x_continuous(breaks = 1:nrow(scree))+\n  labs(y = \"Variance Explained\", x = \"Principal Component\")+\n  theme_bw()+\n  scale_color_manual(values = c(\"variance\" = \"blue\", \"cum_variance\" = \"red\"),\n                     labels = c(\"Cumulative Variance Explained\", \"Variance Explained\"),\n                     guide = guide_legend(title = \"\"))+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nAs we can see, using two principal components only keeps around 50% of the data’s variance or information. So it’s not a big surprise anymore that we cannot sepearate the genres very well.",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#expectation-maximization",
    "href": "statistical-machine-learning/sml.html#expectation-maximization",
    "title": "Statistical Machine Learning",
    "section": "Expectation Maximization",
    "text": "Expectation Maximization\nThis chapter is very similar to this one on EM-Clustering.\nThe EM (Expectation-Maximization) algorithm is a two-step iterative process used for finding maximum likelihood estimates of parameters in statistical models when the model depends on unobserved latent variables.\n\nThe E-step, or Expectation step, calculates the expected value of the log likelihood function, with respect to the conditional distribution of the latent variables given the observed data under the current estimates of the parameters. In the context of Gaussian mixture models, this step computes the “responsibilities” that each Gaussian component takes for each data point. The responsibility of a component for a particular data point is the probability that the data point was generated by that component.\nThe M-step, or Maximization step, updates the parameters to maximize the expected log-likelihood found on the E step. The parameters are updated in a way that they maximize the “completeness” of the data. In the context of Gaussian mixture models, this step re-estimates the parameters (mean, covariance matrix and mixing weights) of the Gaussian components based on the responsibilities computed in the E-step. This is done so as to make the Gaussian components align better with the data points for which they have taken responsibility.\n\nThe E-step and M-step are repeated iteratively until the algorithm converges, meaning that the parameters do not change significantly from one iteration to the next.\nThe EM algorithm can be used to perfom image segmentation. Here’s an example using a 128x128 picture.\n\n\nCode\n# Loading and plotting image\nimg &lt;- readJPEG(\"./pictures/cropped.jpg\")\n\n# Convert the image to a data frame and normalize the values\nimg_data &lt;- data.frame(\n  r = as.vector(img[,,1]),\n  g = as.vector(img[,,2]),\n  b = as.vector(img[,,3])\n)\n\nimg_data %&gt;%\n  mutate(y = rep(1:128, times = 128), \n         x = rep(1:128, each = 128)) %&gt;%\n  ggplot(aes(x = x, y = -y, fill = rgb(r, g, b)))+\n  geom_raster()+\n  scale_fill_identity()+\n  theme_minimal()+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(1)\n\n# Convert img_data to a matrix\nimg_data_mat &lt;- as.matrix(img_data)\n\n# Define number of clusters\nn_cluster &lt;- 3\n\n# Initialize parameters\nmu &lt;- array(runif(n_cluster * 3), dim = c(n_cluster, 3))\nsigma &lt;- array(diag(runif(3), 3, 3), dim = c(3, 3, n_cluster))\npi &lt;- rep(1/n_cluster, n_cluster)\n\n# EM algorithm\nfor(i in 1:30) {\n  # E-step: calculate responsibilities\n  resp &lt;- sapply(1:n_cluster, function(k) pi[k] * dmvnorm(img_data_mat, mean = mu[k, ], sigma = sigma[,,k]))\n  resp &lt;- sweep(resp, 1, rowSums(resp), \"/\")\n  \n  # M-step: re-estimate parameters\n  for(k in 1:n_cluster) {\n    mu[k, ] &lt;- colSums(resp[,k] * img_data_mat) / sum(resp[,k])\n    sigma[,,k] &lt;- t(sqrt(resp[,k]) * sweep(img_data_mat, 2, mu[k, ])) %*% (sqrt(resp[,k]) * sweep(img_data_mat, 2, mu[k, ])) / sum(resp[,k])\n    pi[k] &lt;- mean(resp[,k])\n  }\n}\n\n# Get segmentation by assigning each pixel to the component with the highest responsibility\nsegmentation &lt;- apply(resp, 1, which.max)\n\nimg_data %&gt;%\n  mutate(y = rep(1:128, times = 128), \n         x = rep(1:128, each = 128)) %&gt;%\n  cbind(segmentation) %&gt;%\n  ggplot(aes(x = x, y = -y, fill = segmentation))+\n  geom_raster()+\n  #scale_fill_identity()+\n  theme_minimal()+\n  coord_fixed()+\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "statistical-machine-learning/sml.html#variational-inference",
    "href": "statistical-machine-learning/sml.html#variational-inference",
    "title": "Statistical Machine Learning",
    "section": "Variational Inference",
    "text": "Variational Inference",
    "crumbs": [
      "Statistical Machine Learning"
    ]
  },
  {
    "objectID": "fofi/fundamentals_of_finance.html",
    "href": "fofi/fundamentals_of_finance.html",
    "title": "Fundamentals of Finance | Übersicht",
    "section": "",
    "text": "Lorem Ipsum :)",
    "crumbs": [
      "Fundamentals of Finance",
      "Fundamentals of Finance | Übersicht"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html",
    "href": "fofi/cofi_I.html",
    "title": "Corporate Finance 1",
    "section": "",
    "text": "Name five requirements for a BaFin license.\nFor selling shares in a stock-exchange in Germany, a company has to acquire a license from the regulator BaFin. Some requirements for that are: - Initial capital of €50,000 - Business plan and organizational structure - Names of the involved owners, information on reliability and amount of participation - Disclosure on compensation policy and practices - Information about connection between other natural or legal persons\n\nWhich involved party suggests the starting price of the share?\nThe investment bank managing the IPO along with the company going public suggest the starting/offer price.\n\nWhat correlates with IPO intensity?\n\nA countries’ education level\nEconomic climate/freedom\nDegree of urbanization\nWhether a state has a major financial center\n\n\nDescribe the rend of listings in Germany since 1950. Can a similar trend be observed in the USA?\nThe number of new IPOs as well as the number of public companies have been broadly declining in Germany since the 1950s. Simultaneously, the number of delisting has increased. The same holds true for the US.\n\nName and explain two types of special IPOs.\n\nCarve-out: A parent company sells shares in its subsidiary to the public. One example is Porsche and VW.\nSpin-off: Separate listing of part of an existing listed company. An example is E-On and its spin-off Uniper.\nDirect listing: Company shares become publicly traded without a formal IPO. No new shares are sold. An example is Spotify.\nSPAC: A company-shell is created and raises money in an IPO. It then tries to acquire a private company. An example of a company that went public this way is the producer of electric vehicles Lucid.\n\n\nExplain the difference between a carve-out and a spin-off. Provide one example for each type.\nWith both a carve-out and a spin-off, a new independent publicly listed company is created from an existing parent company. However, in a carve-out, new shares of a subsidiary are sold to the public. With a spin-off on the other hand, the shares are distributed to the existing shareholders of the parent company.\n\nWhat are the different parties involved in an IPO?\nThe main parties in an IPO include: - Issuing company - Investment banks - Regulators - Investors",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-1",
    "href": "fofi/cofi_I.html#chapter-1",
    "title": "Corporate Finance 1",
    "section": "",
    "text": "Name five requirements for a BaFin license.\nFor selling shares in a stock-exchange in Germany, a company has to acquire a license from the regulator BaFin. Some requirements for that are: - Initial capital of €50,000 - Business plan and organizational structure - Names of the involved owners, information on reliability and amount of participation - Disclosure on compensation policy and practices - Information about connection between other natural or legal persons\n\nWhich involved party suggests the starting price of the share?\nThe investment bank managing the IPO along with the company going public suggest the starting/offer price.\n\nWhat correlates with IPO intensity?\n\nA countries’ education level\nEconomic climate/freedom\nDegree of urbanization\nWhether a state has a major financial center\n\n\nDescribe the rend of listings in Germany since 1950. Can a similar trend be observed in the USA?\nThe number of new IPOs as well as the number of public companies have been broadly declining in Germany since the 1950s. Simultaneously, the number of delisting has increased. The same holds true for the US.\n\nName and explain two types of special IPOs.\n\nCarve-out: A parent company sells shares in its subsidiary to the public. One example is Porsche and VW.\nSpin-off: Separate listing of part of an existing listed company. An example is E-On and its spin-off Uniper.\nDirect listing: Company shares become publicly traded without a formal IPO. No new shares are sold. An example is Spotify.\nSPAC: A company-shell is created and raises money in an IPO. It then tries to acquire a private company. An example of a company that went public this way is the producer of electric vehicles Lucid.\n\n\nExplain the difference between a carve-out and a spin-off. Provide one example for each type.\nWith both a carve-out and a spin-off, a new independent publicly listed company is created from an existing parent company. However, in a carve-out, new shares of a subsidiary are sold to the public. With a spin-off on the other hand, the shares are distributed to the existing shareholders of the parent company.\n\nWhat are the different parties involved in an IPO?\nThe main parties in an IPO include: - Issuing company - Investment banks - Regulators - Investors",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-2",
    "href": "fofi/cofi_I.html#chapter-2",
    "title": "Corporate Finance 1",
    "section": "Chapter 2",
    "text": "Chapter 2\n\nExplain possible advantages and disadvantages of a listing.\nAdvantages: - Access to new equity - Easier access to future capital through seasoned equity offerings - Cashing out opportunity for current owners - Better image, more press - Employee compensation through stock\nDisadvantages: - Profit-sharing - Loss of control - Loss of confidentiality - Strict reporting and publicity requirements - IPO expenses\n\nCosts of an IPO can be divided into “costs of going public” and “costs of being public”. How can these costs be further classified? Give an example of each.\n\n\n\nIPO costs\n\n\n\nName one possible advantage for employees coming with an IPO.\nThere is now the possibility of getting compensation in company stock (which can actually be sold – in contrast to private companies).\n\nSet the formula for the cost of going and being public. Which factors need to be put into consideration?\nWe pretend as if costs of going and being public occur (stretched out) every year. \\[\nk_e^c = \\underbrace{\\frac{1}{1 - \\alpha f^{IPO}}}_{\\text{Cost of IPO itself}} \\underbrace{(k_e + l + \\gamma f^{SEO})}_{\\text{Costs each year}}\n\\] with \\[\n\\begin{aligned}\nk_e^c &= \\text{Equity cost for firm} \\\\\nk_e &= \\text{Investor equity return} \\\\\n\\alpha &= \\text{IPO-proceeds to market-cap ratio} \\\\\nl &= \\text{Annual listing and disclosure cost ratio} \\\\\n\\gamma &= \\text{Annual SEO-proceeds to market-cap ratio} \\\\\nf^{IPO} &= \\text{IPO cost ratio} \\\\\nf^{SEO} &= \\text{SEO cost ratio} \\\\\n\\end{aligned}\n\\] If we assume some values like 20% IPO proceeds at an 8% IPO fee, 5% investor return, 0.1% yearly listing fee and a yearly SEO with a volume of 10% of the market cap with a 2% fee, we get the following: \\[\n\\begin{aligned}\nk_e^c &= \\frac{1}{1-20\\%\\cdot8\\%} \\cdot (5\\%+0.1\\%+10\\%\\cdot2\\%) \\\\\n&= 1.016 \\cdot 0.053\\\\\n&= 5.39\\%\n\\end{aligned}\n\\] So the largest factor (with these kinds of values) is the expected return of the investor.\n\nWhy should the timing of a financial decision not matter?\nAccording to the Modigliani/Miller theorem and the Efficient Market Hypothesis, issuing new securities via an IPO should not, on average, provide a positive net present value (NPV) opportunity for the company. This is because, in an efficient market, the new shares will be priced fairly based on all available information.\n\nCompany management intends an IPO in the medium term. What problems should be solved by an IPO? What effects (advantages/improvements) does the management expect from this step? Explain three possible advantages of a listing for this company from the context. Additionally, when would it make sense to proceed with the IPO, when not? Describe two points in time and give reasons.\n\nProblems solved: Raise capital for growth or expansion, provide liquidity for early investors to cash out, and increase the public profile of the company.\nEffects: Better access to public capital markets for future financing, use stock as acquisition currency, pay employees with stock.\nTiming: It makes sense to proceed with an IPO when the company has a strong financial track record and growth prospects. It doesn’t make sense when financials are poor or the market is in a downturn.\n\n\nThe management intends a delisting in the medium term. What problems should be solved by a delisting? What effects (advantages/improvements) does the management expect from this step?\n\nProblems solved: Reduce regulatory burden and costs, protect proprietary information, concentrate ownership and control.\nEffects: Avoid public reporting requirements, maintain confidentiality of proprietary information, managers can focus on long-term goals rather than quarterly results.",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-3",
    "href": "fofi/cofi_I.html#chapter-3",
    "title": "Corporate Finance 1",
    "section": "Chapter 3",
    "text": "Chapter 3\n\nExplain the consequences when going public.\n\nDiversification & Bargaining Power: Public firm shareholders are better diversified, whereas large private investors have more bargaining power.\nInvestor Conviction & Cost: Public companies need to convince a broader investor base, incurring costs reflected in lower share prices.\nPublic Share Pricing: IPOs allow observable share pricing, reducing outsider evaluation costs through free-riding opportunities.\nVenture Capital Financing: Reduces information costs but demands higher returns due to less diversification and greater bargaining power, unlike public companies which face duplicated information production costs.\n\n\nWhy do companies experience an extension of the maturity of their debt after going public?\nCompanies tend to take on debt with longer repayment periods due to improved public information and stronger bargaining positions.\n\nWhat should unlisted companies consider when deciding to go public?\nIssuing new shares will dilute current ownership percentages.\n\nWhy do companies in the same industry go public in clusters?\n\nReasons external to the firm and external to the stock market:\n\nMore projects need to be funded, based on the broader economy and the business cycle.\nReduction in IPO costs by changes in the compliance requirements, listing rules or commission fees.\n\nReasons external to the firm but internal to the stock market:\n\nFavorable stock markets (i.e. high valuations) will lead to more IPOs.\n\nReasons internal to the firm:\n\nEntrepreneur needs to obtain external financing to undertake a positive net present value project.\n\n\n\nWhat empirical evidence for IPOs did you learn? Explain three study results.\n\nMain factor affecting IPO probability is the market-to-book ratio of firms in the same industry.\nSecond most important factor is firm size, as larger firms are more likely to go public.\nThere is clustering in IPO activity over time.\nCompanies experience a reduction in the cost of bank loans after an IPO.\nFounders do not bring their company public mainly for diversifying.",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-4",
    "href": "fofi/cofi_I.html#chapter-4",
    "title": "Corporate Finance 1",
    "section": "Chapter 4",
    "text": "Chapter 4\n\nWhat is meant by “underpricing”?\nThere are large returns on the first day of trading of equity IPOs. The underpricing can thus be calculated as \\[\nR = \\frac{P(t_1)}{P(t_0)}-1\n\\] where \\(R\\) is the first day return, \\(P(t_1)\\) is the closing price of the first trading day and \\(P(t_0)\\) is the offering price. One could also include a market correction by subtracting the market return of the day. This however is seldom used, because market returns tend to be small compared to the IPO return.\n\nExplain the theoretical models that try to explain the phenomenon of underpricing. When is underpricing high, and when is it low?\nThere is asymmetric information between all parties of an IPO (issuer, investor, underwriter). This results in the following hypothesis: - Winner’s curse: Uninformed/average investors who win bids might overpay, because informed investors will only bid on underpriced shares, leading uninformed investors to win the “bad” IPO bids. That’s why uninformed investors bid lower from the beginning. - Market feedback hypothesis: The underwriter needs market feedback for pricing an IPO. However, investors would not give this information “for free”. That’s why underwriters compensate with underpricing. - Signalling hypothesis: Issuers want to “leave a good taste” with investors for futures SEOs. Thats why they accept underpricing.\n\nExplain two advantages and disadvantages that underpricing and overpricing can have.\n\n\n\n\n\n\n\n\n\nUnderpricing\nOverpricing\n\n\n\n\n(+)\n- Increased initial demand- Positive market momentum\n- Maximize capital raised- Buffer against market volatility\n\n\n(-)\n- Leaving money on the table- Perceived lack of confidence\n- Poor post-IPO performance- Risk of IPO failure\n\n\n\n\nList three difficulties that are present in connection with pricing in an IPO.\n\nThere is no observable market price prior to offering.\nThe issuing firm might be young and have little operating history.\nIf price is set too low, issuer leaves money on the table.\nIf price is set too high, not all shares get sold and IPO fails.\nInvestors would not buy IPOs from underwriter with history over overpricing.\n\n\nGiven the first day return \\(R\\) and the closing price of the first day \\(P(t_1)\\), calculate the offer price \\(P(t_0)\\). Which simplification can mostly be used?\nWe can mostly ignore the market return of the day, because it tends to be small in comparison to the first day return. \\[\n\\begin{aligned}\nR &= \\frac{P(t_1)}{P(t_0)}-1 \\\\\n\\iff P(t_0) &= \\frac{P(t_1)}{R-1}\n\\end{aligned}\n\\]\n\nCalculate how much money is left on the table considering: number of shares sold, offer price and closing price.\nWith the number of shares \\(N\\), offer price \\(P(t_0)\\) and closing price \\(P(t_1)\\), we can find the money left on the table \\(M\\). \\[\n\\begin{aligned}\nM &= \\text{actual proceeds} - \\text{could-have proceeds}\\\\\n&= N \\cdot P(t_1) - N \\cdot P(t_0) \\\\\n&= N \\cdot \\left( P(t_1) - P(t_0) \\right)\n\\end{aligned}\n\\]\n\nExplain the winner’s curse hypothesis and give an example on how it occurs.\nWe assume there are two types of investors: Informed ones who know a stock’s true value and uninformed ones who don’t know that. Informed investors only buy an underpriced IPO. So, uninformed investors only win their full bid in overpriced IPOs.\nExample: Good IPOG (+20% initial return)and bad IPOB (-5% initial return), both offer 1500 shares. The informed investor bids for 500 shares in IPOG and 0 shares in IPOB. The uninformed investor bids for 1500 shares in both IPOs. IPOG is over-subscribed and both investors receive \\(\\frac{1500}{2000}=75\\%\\) of their bid. In IPOB, the uninformed investor receives their full bid. Now the expected return for the informed investor is 20% while for the uninformed investor it is \\[\nE(r) = \\frac{75\\% \\cdot 20\\% + 100\\% \\cdot -5\\%}{2}=5\\%\n\\] That’s why underwriters create underpricing: To keep all investors happy and the IPO market alive.\n\nWhat is the Market Feedback Hypothesis? Example: true price \\(x\\), investor bids \\(y\\); What is the underpricing and expected profit?\nThe idea behind the market feedback hypothesis is, that investors are better informed than the investment bank and needed for pricing the IPO. However, investors are not willing to provide valuable information without compensation. This compensation then comes in the form of underpricing and larger allotments (with the bookbuilding procedure). With the true price \\(x\\) and the investor bid \\(y\\), we can calculate the resulting underpricing and expected profit as \\(x-y\\).",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-5",
    "href": "fofi/cofi_I.html#chapter-5",
    "title": "Corporate Finance 1",
    "section": "Chapter 5",
    "text": "Chapter 5\n\nPresent the main steps to the IPO in correct order and briefly explain their importance.\n\nDecision to go public\nBeauty contest and IPO pitches\nSelection of underwriter(s)\nDefinition of the IPO structure\nPreparation of documents & filings, company valuation and analysis\nRoadshow and bookbuilding\nOrder taking, price setting\nAllotment\nPrice support\n\n\nWhere are the shares drawn from when a young or old company goes public?\nYoung companies typically sell primary shares. These are shares that are newly created. The proceeds go to the company and may be used for expansion, for example. Older companies usually sell mostly secondary shares, which come from existing shareholders. In this case, no new cash comes to the company. Usually, there is a mixture of primary and secondary shares sold at an IPO.\n\nWhat empirical evidence for IPOs did you learn in the lecture?\n\nHigher underwriter reputation leads to higher underpricing. This effect reduces when there has been a relationship between underwriter and issuer before the IPO.\nSocial ties between underwriter and issuer (for example management is friends with bankers) impacts the outcome of an IPO.\nVenture capital backing reduces underpricing.\n\n\nHow does venture capital influence underpricing and why?\nIPOs with venture capital backing tend to have a lower underpricing. This may be due to smaller information asymmetries and a positive of the venture capital backing.\n\nDoes the underwriter play a role in underpricing? Explain.\nEvidence suggests, that higher underwriter reputation leads to higher underpricing. This might be a kind of additional fee for better analyst coverage.",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-6",
    "href": "fofi/cofi_I.html#chapter-6",
    "title": "Corporate Finance 1",
    "section": "Chapter 6",
    "text": "Chapter 6\n\nExplain the steps of the (decoupled) bookbuilding procedure.\nClassical Bookbuilding procedure: 1. Analyst meeeting 2. Pre-marketing phase including publishing of the price range 3. Marketing and order-taking phase 4. Publishing of the final offer price and allotting shares Decoupled Bookbuilding procedure: 1. Analyst meeting 2. Pre-marketing phase 3. Marketing phase 4. Publishing the price range 5. Order taking phase 6. Publishing final offer price and allotting shares\n\nName the three price-building procedures. Give two advantages and disadvantages per procedure.\n\n\n\n\n\n\n\n\nPrice-Building Procedure\nAdvantages\nDisadvantages\n\n\n\n\nFixed Price Offering\n- Simplicity: Easy for investors to understand.- Accessibility: Small investors can participate.\n- Mispricing Risks: Might not reflect market demand.- Limited Capital Maximization: Might not obtain maximal possible capital if demand is higher.\n\n\nBook Building\n- Market-Driven Pricing: Adjusts price based on investor demand.- Information Gathering: Provides market feedback on price and demand.\n- Complexity: More complicated and harder for smaller investors to understand.- Bias Towards Institutional Investors: Larger investors might have greater influence.\n\n\nDutch Auction\n- Fair Market Price Discovery: All bids determine the final price.- Democratization: Equal playing field for all investor types.\n- Uncertainty: Final price not known until completion.- Complex Understanding: Some investors find the process challenging.\n\n\n\n\nName and explain the most important allocation methods and describe their advantages and disadvantages.\n\n\n\n\n\n\n\n\n\nAllocation Method\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nFixed Price\nAn offer price is set prior to requests for shares being submitted.\n- Simple and transparent process.\n- Cannot adjust price based on demand. - Often leads to high underpricing if demand is strong. - Difficult to allocate shares efficiently.\n\n\nAuctions\nInvestors bid for shares, and a clearing price is determined.\n- May result in less underpricing. - Allows price discovery.\n- Complicated for investors. Difficult for issuers to control allocation.- Provides little incentive for information production.\n\n\nBookbuilding\nInvestors provide non-binding indications of demand. Offer price set based on demand. Underwriters allocate shares.\n- Allows price discovery,- incentivizes information production,- discretion in allocation.\n- Lack of transparency, potential conflicts of interest.\n\n\n\n\nWhat factors drive the allocation decision in the bookbuilding process\nWith the bookbuilding process, the investment bank can allocate shares at their own discretion. To really make it possible to choose which investors to allocate shares, the demand must be larger than the supply, usually by a factor of three.\nThe investment bank then chooses the allocation based on the importance of the institution/the investor, participation in the roadshow, transparency of purchase intentions, deal and price feedback, price aggressiveness. Also, the investment bank looks to limit stock flowback, by preferring long-term investors like pension funds. At the same time, a certain trading volume after the IPO is wanted, so some allocation also goes to active trading investors. Retail investors typically get an allocation as well.\n\nWhat method was mostly used before the bookbuilding and what problems occurred?\nBefore the bookbuilding process become popular, fixed price offerings were the defacto standard. There were some problems however: - Limited Market Feedback: With the fixed price IPO, the price is set by the issuer and investment bank without any price discovery by the market. - Mispricing Risk: Because the price is set by the issuer, it may be significantly over- or underpriced. - No check on investor sentiment: Without price bids, it’s hard to gauge investor sentiment or market conditions\n\nExplain the allocation variations in the fixed price process.\nIf demand is larger than supply, shares are allocated pro rata or randomly. However, requests for large numbers of shares are typically cut back more.\n\nThere are two types of bookbuilding in allocating shares. What is their name and elaborate the difference by graphing each process.\nThere are the classical and the decoupled bookbuilding process.",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-8",
    "href": "fofi/cofi_I.html#chapter-8",
    "title": "Corporate Finance 1",
    "section": "Chapter 8",
    "text": "Chapter 8\n\nName and explain the methods of price stabilization.\n\nDirect intervention: The underwriter directly buys shares in the open market when the price drops below the offer price, thus pushing up the share price.\nGreenshoe option: The underwriter initially sells more shares than it should, effectively creating a short position. When the share price drops, it can then buy back the shares it over-allocated to push up the price.\n\n\nExplain how the greenshoe option can contribute to price stabilization after the IPO, depending on the price development.\nFirst, the underwriter sells excess shares, creating a short position. The following then depends on the development of the share price: - Share price drops below offer price: The underwriter can buy back the excess shares and close its short position. This creates demand and pushes up the share price. - Share price stays above offer price: The underwriter does not buy back the excess shares. For this scenario, the issuer provided secondary shares to the underwriter beforehand.\n\nDisplay the “Pay off structure” of a greenshoe option.\n The most important takeaway: The bank always makes extra profit when the shareprice diverges from the offer price.\n\nShare price rises: The underwriter does not buy back the excess shares. It effectively created a higher IPO volume and thus makes more in commission fees.\nShare price falls: The underwriter closes its short position and makes money doing so.\n\n\nWhat can be seen in the graphic? Explain the effect and provide reasons. \nThe graphic shows a drop in the share price after the IPO. This might be due to overpricing or volatility and illiquidity. In this case, the underwriter can choose to stabilize the price either by buying shares directly “onto their book” or by utilizing its greenshoe option.",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-9",
    "href": "fofi/cofi_I.html#chapter-9",
    "title": "Corporate Finance 1",
    "section": "Chapter 9",
    "text": "Chapter 9\n\nExplain three possible disadvantages of a listing\n\nThe cost of going public is very high: Investment banks charge high fees, and the underpricing is a cost as well.\nThe cost of being public is high: Publicly traded companies have stricter regulatory and reporting requirements, which can incur additional costs for compliance and financial reporting.\nLoss of control: Going public often requires selling a significant portion of the company’s equity to outside investors.\n\n\nWhat is a lock-up period? How long can it last?\nLock-ups are standard arrangements in an IPO. They restrict the sale of shares by pre-issue shareholders for a certain period, often times 6 months. This is done to align the interests of current owners to the owners and a quick “cashing out” on the IPO day.\n\nHow are fees in an IPO structured?\n\n20% management fee to the lead manager for due diligence, prospectus, pricing, organization of syndicate, road show\n20% underwriting fee for placement risk and price stabilization\n60% selling concession for each share sold by a syndicate member\n\n\nWhat is the cost of an IPO in relation to its volume in the US and other countries?\nIn the US, the cost is 7% of the IPO volume on average. In other countries, this is around 3.5%.\n\nWhat are some findings concerning the lock-up period and its ending? How can it be explained?\n\nThe volume traded on the day the lock-up ends is much higher than on average (+60%) because of early investors selling their shares.\nThe trading volume remains elevated after this day (+40%) because more shares are now available for trading.\nOn the around the lock-up expiration, there is a significant and permanent drop in price (-2.5%).\nThe price drop is anticipated yet still happens, mostly because of trading costs, difficulty of entering short positions because of low float and tax reasons.\n\n\nCompany X is significantly overpriced after three years. How does this fit into the empirical evidence?\nIPOs do not have a great long run performance on average when compared to similar companies. So, Company X would be an outlier.\n\nWhat does empirical evidence indicate for the long-run performance when companies choose an underwriter with less reputation?\nCompanies that choose less reputable underwriters for the IPO perform poorer in the long run. This might be because these companies tend to be smaller and are less likely to be backed by venture capitalists, thus having worse corporate governance.\n\nThe following characteristics of companies increase the likelihood for what? High-reputation of underwriter, larger firm size, higher ROA, high-tech industry company.\nThese characteristics increase the likelihood of a company being included in an index like the S&P 500.",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-10",
    "href": "fofi/cofi_I.html#chapter-10",
    "title": "Corporate Finance 1",
    "section": "Chapter 10",
    "text": "Chapter 10\n\nIs there clustering, underpricing and underperformance in SEOs? How does it compare to IPOs?\n\nYes, clustering / cycles also exist for SEOs. However, they are not as pronounced and shorter as for IPOs.\nYes, there is underpricing with SEOs, around 2,5%. This is way less then the underpricing found for IPOs. The underpricing tends to higher for the first SEO a company does.\nThere is also underperformance after an SEO. In contrast to IPOs though, this underperformance does not vanish when controlling for size. This might be due to the different capital structure of firms that did an SEO and one that did not.\n\n\nWith the help of a diagram, show how to make money with seasoned equity offerings.\n\n\n\nSEO short positions\n\n\n\nSell the stock short when an SEO is announced.\nWait for the stock to drop.\nCover the short position. It will typically reach its lowest level at the offer day.\n\n\nWhat are the consequences of a seasoned equity offering?\nThere are multiple consequences of an SEO: - Lower leverage: With an SEO, a company raises new equity thus reducing the equity-to-equity ratio or leverage. This makes the company a less risky investment. - Higher liquidity in the stock market: The additional shares lead to more shares circulating in the market and therefore a higher liquidity. - New cash: A company’s management needs a good story what they will do with the new cash they have on hand. Ideally, the cash is used for profitable projects.\n\nHow can capital increases be distinguished? What are the differences?\nCapital increases can be distinguished by the means of the increase: - Capital increase in return for cash - Capital increase in the return for stock - Capital increase from the company’s reserves",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/cofi_I.html#chapter-11",
    "href": "fofi/cofi_I.html#chapter-11",
    "title": "Corporate Finance 1",
    "section": "Chapter 11",
    "text": "Chapter 11\n\nWhat is “underperforming” and how do you determine that in IPOs/SEOs?\nUnderperformance means that a firm which did an IPO/SEO perforce worse than the broad market or a specific subset over a long time period, typically measured between 1 to 5 years. It is typically calculated using the Buy-And-Hold-Abnormal-Return of stock \\(i\\) up to month \\(M\\) after an IPO/SEO: \\[\nBHAR_{i,M} = \\left( \\prod_{m=1}^{M} (1+r_{i,m}) -1 \\right) - \\left( \\prod_{m=1}^{M} (1+ E[r_{i,m}]) -1 \\right)\n\\] The expected return \\(E[r_{i,m}]\\) of stock \\(i\\) in month \\(m\\) can be approximated by the return of the broader market or a subset of matching firms in that month.\n\nWhat happens when an equity issuer lowers its leverage?\nWhen an issuer raises equity in using a SEO it automatically reduces the leverage. This in turn reduces the risk of default. This reduction in risk will result in a lower risk premium in the stock market.\n\nWhy does the risk have to decrease when investments are financed through equity?\nIf a capital investment is financed by equity, then risk must decrease because investment in effect extinguishes the risky growth options.\nBy allocating capital to a specific investment, the company is effectively closing the door on some of the other growth options it may have had. Those other options, being uncertain and yet-to-be-realized, carry risk. Committing to a known investment eliminates the uncertainty associated with those unrealized growth options.\n\nWhat does management do if the share price appears to be too high?\nWhen the stock is overvalued (from the managements perspective), it can choose to sell new shares to profit from this. The opposite would be buying back shares when the stock is undervalued.\n\nOutline investors’ reaction to negative announcements.\nAfter a SEO announcement, the stock price drop around 2.5% in the following two days. This can either be attributed directly to the SEO or it could just mean that new information has entered the market and impacted the stock price.\n\nName three securities involved in the OTC transactions.\nSecurities involved in a PIPE deal can be - common stock - preferred stock - convertible preferred stock - convertible debt - other types of equity\n\nWhy can’t investors cover their short positions with shares from the PIPE transaction?\nUsually, investors hedge their PIPE investment by short-selling. However, the investor cannot cover its short positions with the actual shares received in the PIPE transaction. Shares to cover short-sales must have been sold when the short-sale was made.\n\nPresent the main benefits for an issuer when using PIPEs\nPIPE (Private Investment in Public Equity) are a means of selling shares to an investor over the counter. The main benefits to a “normal” SEO are - It’s quicker than a SEO - Small issues/transactions are possible, that would not be worth it with a SEO - Short-term stock market reactions are positive - Flexibility: Various contract terms can be agreed on - The shares can be sold either at a discount or a markup",
    "crumbs": [
      "Fundamentals of Finance",
      "Corporate Finance 1"
    ]
  },
  {
    "objectID": "fofi/household_finance.html",
    "href": "fofi/household_finance.html",
    "title": "Household Finance",
    "section": "",
    "text": "Wie kann man den Preisverlauf einer Aktie oder eines anderen Wertpapiers modellieren?\nDer Wert einer Aktie im Zeitpunkt \\(t\\) sei \\(S\\). Dann ist der Wert im Zeitpunkt \\(t+dt\\) gleich \\(S+dS\\). \\(dS\\) kann als stochastischer Prozess modelliert werden: \\[\n\\begin{aligned}\ndS &= \\mu S \\cdot dt + \\sigma S \\cdot dW \\\\\n\\text{mit }W&\\sim N(0, t) \\\\\ndW &\\sim N(0, \\sqrt{t})\n\\end{aligned}\n\\] \\(\\mu\\) ist der Drift bzw. die erwartete Rendite der Aktie und \\(\\sigma\\) die Standardabweichung. Führt man damit eine Monte-Carlo Simulation durch, d.h. man zieht verschiedene Werte für \\(dW\\), erhält man beispielsweise folgende Pfade. Ein Pfad wurde beispielhaft rot eingefärbt. \n\nWie wird der Payoff einer europäischen Call-Option bestimmt?\nDer Payoff ergibt sich als \\((S(T)-K)^+\\). Dabei ist \\(S(T)\\) der Wert des Underlyings bzw. der Aktie am Tag der Fälligkeit \\(T\\) und \\(K\\) ist der vorher vereinbarte Strike Preis. Wenn \\(S(T)\\) kleiner ist als \\(K\\), so gibt es keinen Payoff und die Option ist wertlos. Liegt \\(S(T)\\) über \\(K\\), so ist die Differenz der Payoff.\n\nWie kann man ein Portfolio bilden, das das Verhalten einer europäischen Option über einen bestimmten Zeitraum (hier eine Periode des Binomialbaums) perfekt nachbildet, wenn man nur die Aktie und ein (risikoloses) Geldkonto verwendet?\nMan befindet sich am Anfang einer Periode und der Aktienkurs im Wert von \\(S\\) kann sich in einem der beiden Zustände entwickeln: \\(S_u = u S\\) oder \\(S_d = d S\\).\nBezeichnen wir \\(\\Pi\\) als ein solches Portfolio. \\(\\Pi\\) bestehe aus \\(\\alpha\\) Aktien der Aktie und \\(\\beta\\) in bar. Sowohl \\(\\alpha\\) als auch \\(\\beta\\) können positiv oder negativ sein, je nachdem, ob man Aktien besitzt/verkauft oder Bargeld leiht/verleiht. Der Wert von \\(\\Pi\\) ist also \\[\\Pi = \\alpha S + \\beta\\] Um \\(\\alpha\\) und \\(\\beta\\) zu bestimmen, nehmen wir an, dass die Option am Ende einer Periode \\(V_u\\) im Aufwärtszustand und \\(V_d\\) im Abwärtszustand wert ist. Da wir wollen, dass sich unser Portfolio repliziert, wollen wir, dass sich sein Wert \\(\\Pi\\) zu genau denselben Zuständen entwickelt, d.h. \\(\\Pi_u = V_u\\) und \\(\\Pi_d = V_d\\). Daraus ergeben sich zwei Gleichungen: \\[\n\\begin{aligned}\n\\Pi_u &= \\alpha S_u + \\beta (1 + R) = V_u \\\\\n\\Pi_d &= \\alpha S_d + \\beta (1 + R) = V_d\n\\end{aligned}\n\\] - Die Aktienkomponente des Portfolios \\(\\Pi\\) hat sich von \\(\\alpha S\\) zu \\(\\alpha S_u\\) oder \\(\\alpha S_d\\) entwickelt, weil sich der Aktienkurs entwickelt hat. - Der Bargeldbetrag \\(\\beta\\), der zu Beginn der Periode geliehen/verliehen wurde, hat uns Zinsen gekostet/verdient, daher der Faktor \\(1+R\\).\nLöst man diese zwei Gleichungen für die zwei Unbekannten \\(\\alpha\\) und \\(\\beta\\) erhält man: \\[\n\\begin{aligned}\n\\alpha &= \\frac{V_u - V_d}{S_u-S_d} \\\\\n\\beta &= \\frac{1}{1+R} \\frac{u V_d - d V_u}{(u-d)}\n\\end{aligned}\n\\] Daraus ergibt sich die Anzahl der Aktien (\\(\\alpha\\)), die man kaufen/verkaufen müsste, und die Anzahl der Barmittel (\\(\\beta\\)), die man leihen/verleihen müsste, um die Option perfekt zu replizieren.\nErklärung von Stack Exchange – Quantitative Finance",
    "crumbs": [
      "Fundamentals of Finance",
      "Household Finance"
    ]
  },
  {
    "objectID": "fofi/household_finance.html#derivatebewertung",
    "href": "fofi/household_finance.html#derivatebewertung",
    "title": "Household Finance",
    "section": "",
    "text": "Wie kann man den Preisverlauf einer Aktie oder eines anderen Wertpapiers modellieren?\nDer Wert einer Aktie im Zeitpunkt \\(t\\) sei \\(S\\). Dann ist der Wert im Zeitpunkt \\(t+dt\\) gleich \\(S+dS\\). \\(dS\\) kann als stochastischer Prozess modelliert werden: \\[\n\\begin{aligned}\ndS &= \\mu S \\cdot dt + \\sigma S \\cdot dW \\\\\n\\text{mit }W&\\sim N(0, t) \\\\\ndW &\\sim N(0, \\sqrt{t})\n\\end{aligned}\n\\] \\(\\mu\\) ist der Drift bzw. die erwartete Rendite der Aktie und \\(\\sigma\\) die Standardabweichung. Führt man damit eine Monte-Carlo Simulation durch, d.h. man zieht verschiedene Werte für \\(dW\\), erhält man beispielsweise folgende Pfade. Ein Pfad wurde beispielhaft rot eingefärbt. \n\nWie wird der Payoff einer europäischen Call-Option bestimmt?\nDer Payoff ergibt sich als \\((S(T)-K)^+\\). Dabei ist \\(S(T)\\) der Wert des Underlyings bzw. der Aktie am Tag der Fälligkeit \\(T\\) und \\(K\\) ist der vorher vereinbarte Strike Preis. Wenn \\(S(T)\\) kleiner ist als \\(K\\), so gibt es keinen Payoff und die Option ist wertlos. Liegt \\(S(T)\\) über \\(K\\), so ist die Differenz der Payoff.\n\nWie kann man ein Portfolio bilden, das das Verhalten einer europäischen Option über einen bestimmten Zeitraum (hier eine Periode des Binomialbaums) perfekt nachbildet, wenn man nur die Aktie und ein (risikoloses) Geldkonto verwendet?\nMan befindet sich am Anfang einer Periode und der Aktienkurs im Wert von \\(S\\) kann sich in einem der beiden Zustände entwickeln: \\(S_u = u S\\) oder \\(S_d = d S\\).\nBezeichnen wir \\(\\Pi\\) als ein solches Portfolio. \\(\\Pi\\) bestehe aus \\(\\alpha\\) Aktien der Aktie und \\(\\beta\\) in bar. Sowohl \\(\\alpha\\) als auch \\(\\beta\\) können positiv oder negativ sein, je nachdem, ob man Aktien besitzt/verkauft oder Bargeld leiht/verleiht. Der Wert von \\(\\Pi\\) ist also \\[\\Pi = \\alpha S + \\beta\\] Um \\(\\alpha\\) und \\(\\beta\\) zu bestimmen, nehmen wir an, dass die Option am Ende einer Periode \\(V_u\\) im Aufwärtszustand und \\(V_d\\) im Abwärtszustand wert ist. Da wir wollen, dass sich unser Portfolio repliziert, wollen wir, dass sich sein Wert \\(\\Pi\\) zu genau denselben Zuständen entwickelt, d.h. \\(\\Pi_u = V_u\\) und \\(\\Pi_d = V_d\\). Daraus ergeben sich zwei Gleichungen: \\[\n\\begin{aligned}\n\\Pi_u &= \\alpha S_u + \\beta (1 + R) = V_u \\\\\n\\Pi_d &= \\alpha S_d + \\beta (1 + R) = V_d\n\\end{aligned}\n\\] - Die Aktienkomponente des Portfolios \\(\\Pi\\) hat sich von \\(\\alpha S\\) zu \\(\\alpha S_u\\) oder \\(\\alpha S_d\\) entwickelt, weil sich der Aktienkurs entwickelt hat. - Der Bargeldbetrag \\(\\beta\\), der zu Beginn der Periode geliehen/verliehen wurde, hat uns Zinsen gekostet/verdient, daher der Faktor \\(1+R\\).\nLöst man diese zwei Gleichungen für die zwei Unbekannten \\(\\alpha\\) und \\(\\beta\\) erhält man: \\[\n\\begin{aligned}\n\\alpha &= \\frac{V_u - V_d}{S_u-S_d} \\\\\n\\beta &= \\frac{1}{1+R} \\frac{u V_d - d V_u}{(u-d)}\n\\end{aligned}\n\\] Daraus ergibt sich die Anzahl der Aktien (\\(\\alpha\\)), die man kaufen/verkaufen müsste, und die Anzahl der Barmittel (\\(\\beta\\)), die man leihen/verleihen müsste, um die Option perfekt zu replizieren.\nErklärung von Stack Exchange – Quantitative Finance",
    "crumbs": [
      "Fundamentals of Finance",
      "Household Finance"
    ]
  },
  {
    "objectID": "fofi/household_finance.html#kapitel-1",
    "href": "fofi/household_finance.html#kapitel-1",
    "title": "Household Finance",
    "section": "Kapitel 1",
    "text": "Kapitel 1\n\nWie sieht das CAPM aus?\nDas CAPM Modell sieht wie folgt aus: \\[\n\\begin{aligned}\nE(R_i) &= R_f + \\left(E[R_M] - R_f \\right)\\cdot \\beta_i \\\\\n\\text{mit }\\beta_i &= \\frac{cov(R_i, R_M)}{var(R_M)}\n\\end{aligned}\n\\] \\(R_f\\) steht dabei für den risikofreien Zinssatz/Rendite\\(, R_i\\) steht für die Rendite eines einzelnen Wertpapiers \\(i\\) und \\(R_M\\) für die Marktrendite. Die Differenz aus der Marktrendite und dem risikofreien Zins wird Marktrisikoprämie genannt. Das \\(\\beta_i\\) beschreibt die Volatilität bzw. das Risiko von \\(i\\) im Vergleich zum Markt. Die erwartete Rendite von Wertpapier \\(i\\) also die risikofreie Rendite plus einer Risikoprämie, welche \\(\\beta_i\\) mal Differenz aus \\(E[R_M]\\) und \\(R_f\\) ist.\nIst die Volatilität von \\(i\\) also gleich der des Marktes, ergibt sich als erwartete Rendite die (erwartete) Marktrendite: \\[\n\\begin{aligned}\nE(R_i) &= R_f + \\left(E[R_M] - R_f \\right)\\cdot 1 \\\\\n&= R_f + E[R_M] - R_f \\\\\n&= E[R_M]\n\\end{aligned}\n\\] Ist das Risiko von \\(i\\) geringer (\\(\\beta_i&lt;1\\)), dann liegt die erwartete Rendite unter der des Marktes.\n\nWelche Implikationen des CAPM lassen sich testen?\n\nBeziehung zwischen erwarteter Rendite und Risiko ist linear.\n\\(\\beta_i\\) ist vollständiges Maß für Risiko.\nHöheres Risiko ist mit höherer erwarteter Rendite verbunden.\n\n\nWie haben Fama/MacBeth das CAPM getestet? Erklären Sie mögliche Probleme der Testweise.\nFolie 36-39 Fama/MacBeth haben das CAPM mit monatlichen Aktienkursdaten aller an der NYSE notierten Aktien von 1926-1968 getestet. Sie haben die Betas der Aktien über 4 Jahre geschätzt, dann in aufsteigender Reihenfolge 20 Portfolios gebildet, die Betas innerhalb der Portfolios gemittelt und schließlich das CAPM über 4 Jahre mit den durchschnittlichen Betas getestet. Probleme dieser Testweise sind möglicherweise Heteroskedastizität, Intervalling und fehlende Stationarität der Renditen.\nHier ist ein anschaulicher Blogeintrag dazu. Diese Vorgehen bringt jedoch empirische Schwierigkeiten mit sich, insbesondere Heteroskedastizität, Intervalling und fehlende Stationaritä der Rendite-Zeitreihen.\n\nWas ist die akademische Sicht auf spekulative Blasen?\nFolie 50-66 Die akademische Sicht ist, dass es keine eindeutigen Nachweise für das Vorliegen spekulativer Blasen gibt. Starke Preisrückgänge werden oft als rationale Reaktion auf Änderungen bei Fundamentaldaten interpretiert. Ökonometrische Studien zeigen zwar kurzfristige Abweichungen von Fundamentalwerten, die als platzende Blasen gedeutet werden könnten, aber der Nachweis bleibt schwierig.\n\nDie experimentelle Studie von Caginalp, Porter und Smith (2000) untersucht, ob an Kapitalmärkten spekulative Blasen entstehen können. Erläutern Sie kurz den Experimentaufbau und skizzieren Sie dann die zentralen Ergebnisse für unterschiedliche Teilnehmergruppen.\nFolie 67-68 Caginalp et al. haben in einem Experiment mit 15 Handelsperioden für ein Dividendenpapier ohne Restwert spekulative Blasen in Abhängigkeit von der Zusammensetzung der Teilnehmergruppen untersucht. Bei rein rationalen Teilnehmern entstand keine Blase. Bei teilnehmenden CEOs und Undergraduates jedoch entstanden im Laufe des Experiments Blasen, die zum Ende hin platzten.\n\nWelche Kursreaktionen wurden auf die Ausstrahlung von CEO Interviews im Amerikanischen Nachrichtenkanal CNBC beobachtet? Wie lassen sich diese Kursreaktionen erklären?\nFolie 71-80 Bei CEO-Interviews auf CNBC wurden signifikant positive Überrenditen am Tag der Ausstrahlung sowie deutlich erhöhte Handelsvolumina beobachtet. Dies lässt darauf schließen, dass in den Interviews wohl neue Informationen vermittelt werden, die noch nicht im Aktienkurs enthalten waren.",
    "crumbs": [
      "Fundamentals of Finance",
      "Household Finance"
    ]
  },
  {
    "objectID": "fofi/household_finance.html#kapitel-2",
    "href": "fofi/household_finance.html#kapitel-2",
    "title": "Household Finance",
    "section": "Kapitel 2",
    "text": "Kapitel 2\n\nWas ist die Entscheidungsgrundlage der Erwartungsnutzentheorie?\nFolie 7-21 Die Entscheidungsgrundlage der Erwartungsnutzentheorie ist ein Axiomensystem zur Ableitung einer Präferenz. Wenn die Präferenz eines Entscheiders die Axiome der vollständigen Ordnung, der Stetigkeit und bei riskanten Alternativen der Unabhängigkeit erfüllt, dann existiert eine Nutzenfunktion, deren Erwartungsnutzen die Präferenz abbildet.\n\nNennen Sie die drei der Erwartungsnutzentheorie zu Grunde liegenden Axiome. Erklären Sie diese anhand eines Beispiels.\nFolie 10-13 Die drei der Erwartungsnutzentheorie zugrunde liegenden Axiome sind:\n\nDas Axiom der vollständigen Ordnung besagt, dass für jedes Lotteriepaar eine eindeutige Präferenzrelation gilt (entweder A &lt; B oder B &lt; A) und diese Präferenz transitiv ist (aus A &lt; B und B &lt; C folgt A &lt; C).\nDas Axiom der Stetigkeit besagt, dass sich für zwei Lotterien durch Mischen eine dritte Lotterie konstruieren lässt, die genau zwischen den beiden anderen liegt.\nDas Axiom der Unabhängigkeit (Substitution) besagt, dass die Präferenz zwischen zwei Lotterien unabhängig von einer dritten Lotterie ist.\n\n\nSkizzieren Sie die Nutzenfunktionen für risikofreudige, risikoneutrale und risikoscheue Anleger und begründen Sie die Lage der Kurven mit Bezug auf das Risikopotential.\nFolie 15-21 Die Nutzenfunktion für einen risikofreudigen Anleger ist konvex, da er das Risiko einer Lotterie positiv bewertet. Die Nutzenfunktion eines risikoneutralen Anlegers ist linear, da er dem Risiko einer Lotterie gleichgültig gegenübersteht. Die Nutzenfunktion eines risikoscheuen Anlegers ist konkav, da er das Risiko einer Lotterie negativ bewertet.\nFür die Lotterie gilt \\(RP(a) = E(a) - SÄ(a)\\), wobei \\(RP\\) die Risikoprämie, \\(E\\) der Erwartungswert und \\(SÄ\\) das Sicherheitsäquivalent ist.\n\n\n\nNutzenfunktion\n\\(RP\\)\nRisikoeinstellung\n\n\n\n\nlinear\n\\(=0\\)\nrisikoneutral\n\n\nkonkav\n\\(&gt;0\\)\nrisikoscheu\n\n\nkonvex\n\\(&lt;0\\)\nrisikofreudig\n\n\n\n\nDie theoretische Fundierung der Behavioral Finance basiert auf der Prospect Theory, die in drei wesentlichen Elementen von der klassischen Entscheidungstheorie abweicht. Beschreiben Sie diese drei Elemente\nFolie 57-58 Die Prospect Theory weicht von der klassischen Entscheidungstheorie in drei wesentlichen Punkten ab:\n\nEditing Phase: Der Entscheider editiert die Alternativen vor der Bewertung durch Coding, Segregation, Combination etc.\nWertfunktion: Die Wertfunktion ist konkav im Gewinnbereich und konvex im Verlustbereich.\nWahrscheinlichkeitsgewichtung: Entscheider ordnen Wahrscheinlichkeiten ein subjektives Gewicht zu.\n\n\nWie sieht die Nutzenfunktion in der Prospect Theory aus?\n\n\n\nutility-function\n\n\n\nDas Unternehmen XY fürchtet eine feindliche Übernahme, welche mit einer Wahrscheinlichkeit von 2% eintritt. Daher entwickelt es einen HTO-Indikator, der die Börsendaten analysiert und warnt, wenn ein feindlicher Übernahmeversuch bevorsteht. Allerdings erkennt der Indikator 8% der geplanten Übernahmen nicht und warnt in 5% der Fälle, in denen gar keine Übernahme geplant ist. Das Unternehmen möchte sich sicher sein, dass mit einer Wahrscheinlichkeit von mindestens 80% tatsächlich eine Übernahme bevorsteht, wenn der Indikator ein Warnsignal gibt. Erfüllt der entwickelte Indikator diese Bedingung?\nFolie 22-36 Zuerst schreiben wir auf, was aus der Aufgabenstellung bekannt ist. \\(Ü\\) steht für eine Übernahme und \\(W\\) für eine Warnung. \\[\n\\begin{aligned}\np(Ü)&=0.02 \\Rightarrow p(\\overline{Ü})=0.98 \\\\\np(\\overline{W}|Ü)&=0.08 \\Rightarrow p(W|Ü)=0.92\\\\\np(W|\\overline{Ü})&=0.05 \\Rightarrow p(\\overline{W}|\\overline{Ü})=0.95\\\\\n\\end{aligned}\n\\] Gesucht ist \\(p(Ü|W)\\).\nZum Beantworten dieser Frage benutzen wir den Satz von Bayes: \\[\n\\begin{aligned}\np(a|b) &= \\frac{p(b|a) \\cdot p(a)}{p(b)} \\\\\n\\text{mit } p(b)&=p(b|a)\\cdot p(a) + p(b|\\overline{a})\\cdot p(\\overline{a})\n\\end{aligned}\n\\] Zuerst müssen wir noch die totale Wahrscheinlichkeit \\(p(W)\\) berechnen: \\[\n\\begin{aligned}\np(W)&=p(W|Ü)\\cdot p(Ü) + p(W|\\overline{Ü})\\cdot p(\\overline{Ü}) \\\\\n&= 0.92\\cdot 0.02 + 0.05\\cdot 0.98 \\\\\n&= 0.0674\n\\end{aligned}\n\\] Nun lässt sich die gesuchte Wahrscheinlichkeit bestimmen: \\[\n\\begin{aligned}\np(Ü|W) &= \\frac{p(W|Ü)\\cdot p(Ü)}{p(W)} \\\\\n&= \\frac{0.92 \\cdot 0.02}{0.0674} \\\\\n&= 0.273\n\\end{aligned}\n\\] Die Wahrscheinlichkeit, dass eine Übernahme bevorsteht, wenn das System eine Warnung ausgibt, liegt demnach bei knapp 27% und somit unter den gewünschten 80%.",
    "crumbs": [
      "Fundamentals of Finance",
      "Household Finance"
    ]
  },
  {
    "objectID": "fofi/household_finance.html#kapitel-3",
    "href": "fofi/household_finance.html#kapitel-3",
    "title": "Household Finance",
    "section": "Kapitel 3",
    "text": "Kapitel 3\n\nErläutern Sie die Idee einer Regression zum Durchschnitt. Was bedeutet das für die zu erwartende Rendite am deutschen Aktienmarkt im Jahr 2007? Erklären Sie in diesem Zusammenhang den Monte-Carlo-Effekt als heuristisch bedingte Verzerrung.\nFolie 7 Die Idee einer Regression zum Durchschnitt besagt, dass auf Jahre mit überdurchschnittlichen Renditen am Aktienmarkt Jahre mit unterdurchschnittlichen Renditen folgen werden. Für 2007 wäre demnach nach Jahren hoher Renditen am deutschen Aktienmarkt mit einer Rendite unter dem langfristigen Durchschnitt zu rechnen. Der Monte-Carlo-Effekt ist eine heuristisch bedingte Verzerrung, bei der Investoren irrtümlich annehmen, dass eine zufällige Sequenz (z.B. Kopf-Zahl beim Münzwurf) einem repräsentativen Muster folgen müsse. Dies führt zu Fehleinschätzungen von Wahrscheinlichkeiten.\n\nWie erklärt Terrance Odean (1998) die schwachen Lerneffekte bei Overconfidence im Zeitablauf?\nFolie 28 Laut Terrance Odean lernt Overconfidence nur langsam ab, da das Feedback an den Finanzmärkten oft langsam und verrauscht ist. Schnelles und klares Feedback ist aber nötig für schnelles Lernen. Daher kalibriert sich das Vertrauen in die eigenen Fähigkeiten an den Finanzmärkten nur langsam.\n\nWas ist der self-serving Bias? Welche Theorien erklären ihn und welche Faktoren treiben ihn? Welche Auswirkungen hat er?\nFolie 30-34 Der self-serving Bias ist die Tendenz, Erfolge eher internalen Faktoren wie eigenen Fähigkeiten zuzuschreiben und Misserfolge eher externalen Faktoren. Er wird durch Motivationstheorien (Schutz des Selbstwertgefühls) und Informationsverarbeitungstheorien (verzerrte Informationsbewertung) erklärt. Getrieben wird er u.a. durch Alter, Kultur und Erfahrung. Er kann zu überhöhtem Selbstbewusstsein und suboptimalen Entscheidungen führen.\n\nFührende amerikanische Finanzprofessoren sind der Meinung, dass Overconfidence das bedeutendste psychologisch bedingte Problem an den internationalen Kapitalmärkten ist. 1. Erläutern Sie, was Overconfidence ist und wie es zum Ausdruck kommt. 2. Warum verschwindet Overconfidence an Kapitalmärkten nicht schnell durch konsequentes Lernen? 3. Overconfidence ist nicht immer gleich stark ausgeprägt. Welche Unterschiede zwischen verschiednen Gruppen von Kapitalmarktteilnehmern wurden festgestellt?\nFolie 16-29 1. Overconfidence bedeutet, dass Investoren ihre Fähigkeiten und Kenntnisse bei Prognosen überschätzen. Dies kommt z.B. zum Ausdruck in zu engen Prognoserisikobändern und der Annahme, den Markt übertreffen zu können. 2. Overconfidence verschwindet laut Terrance Odean nicht schnell, da das Feedback an Kapitalmärkten oft verzögert und verrauscht ist. Für schnelles Lernen wäre aber klares, zeitnahes Feedback nötig. 3. Jüngere, unerfahrene Investoren sowie Männer zeigen tendenziell mehr Overconfidence. Kulturell ist sie in Asien stärker verbreitet als in den USA.\n\nWas besagen die Phänomene “Verlustaversion” und “mentale Konten” und weshalb können sie sich negativ auf Portfolio-Renditen auswirken?\nFolie 66-75 Verlustaversion bedeutet, dass Verluste stärker gewichtet werden als Gewinne. Dies führt dazu, dass Verluste nicht realisiert werden (Dispositionseffekt). Mentale Konten bedeuten, dass Anleger separate Konten für einzelne Investments führen und das Gesamtportfolio nicht optimieren. Beide Phänomene können zu einer suboptimalen Portfoliozusammensetzung und Depotumschichtungen führen, die die Rendite schmälern.\n\nMilton Friedman stellte 1953 fest, dass jede Preisabweichung eine attraktive Investmentmöglichkeit generiert und dass rationale Investoren sofort attraktive Investments erkennen und entsprechend handeln. Diese Aussage ist in ihrer Gesamtheit fragwürdig. Erläutern Sie, warum sie fragwürdig ist und geben Sie die Risiken an, die beim Ausnutzen von Fehlbewertungen am Aktienmarkt im Kalkül berücksichtigt werden müssen.\nFolie 6 Die Aussage ist fragwürdig, da Investoren aufgrund von heuristischen Verzerrungen und begrenzter Rationalität Preisabweichungen nicht immer korrekt als Investmentchancen erkennen und nicht immer entsprechend handeln. Risiken beim Ausnutzen von Fehlbewertungen sind z.B. unbekannte Risiken der Aktie, Behavioral Risks wenn sich die Fehlbewertung verstärkt statt abschwächt und Liquiditätsrisiken, wenn die Aktie nicht handelbar ist.\n\nBeschreiben Sie die Annahmen und Hauptaussagen der neoklassischen Spartheorie.\nFolie 80 Annahmen: - Rationalität - rationale Erwartungen - intertemporale Nutzenmaximierung - Selbstkontrolle\nHauptaussagen: - Dreieckiger Verlauf des Vermögenspfads - Unabhängigkeit des Konsums von Einkommen - Gleichlauf von Konsum- und Einkommensverlauf\n\nWie funktionieren gesetzliche und private Krankenversicherungen und in welchen Punkten unterscheiden sie sich?\nFolie 87 Die gesetzliche Krankenversicherung ist pflichtversichert mit einkommensabhängigen Beiträgen. Die private Krankenversicherung ist freiwillig mit risikoabhängigen Beiträgen. Die gesetzliche Krankenversicherung übernimmt alle gesetzlich festgeschriebenen Leistungen, die private leistet nur vertraglich festgelegte Leistungen.",
    "crumbs": [
      "Fundamentals of Finance",
      "Household Finance"
    ]
  },
  {
    "objectID": "fofi/household_finance.html#kapitel-4",
    "href": "fofi/household_finance.html#kapitel-4",
    "title": "Household Finance",
    "section": "Kapitel 4",
    "text": "Kapitel 4\n\nMomentumstrategien basieren auf einem Mittelfristigen Anlagehorizont und unterstellen, dass Aktienrenditen zumindest in Teilen vorhersagbar sind. 1. Erläutern Sie die allgemeine Vorgehensweise dieser mittelfristigen Anlagestrategien. 2. Welche Erklärungsansätze für den Erfolg dieser Strategien wurden überprüft, um die Resultate im Einklang mit der Theorie effizienter Kapitalmärkte zu erklären? Welche Ergebnisse wurden für diese Erklärungsansätze gefunden? 3. Welche Erklärungsmöglichkeit bietet die Behavioral Finance an?\n\nFolie 11\n\nEs wird eine Formationsperiode definiert (typischerweise 3-12 Monate).\nIn dieser Periode werden die Aktien nach ihrer Rendite sortiert.\nNach der Formationsperiode werden die Aktien mit der besten Rendite gekauft und die mit der schlechtesten Leerverkauft. Die Positionen werden dann für einen bestimmten Zeitraum (z.B. 3 oder 6 Monate) gehalten.\n\nFolie 19\n\nZufall ist es nicht, die Signifikanz der Ergebnisse wurde statistisch überprüft.\nMarktfriktionen wie Transaktionskosten, Illiquidität und Leerverkaufsbeschränkungen können die Rentabilität dieser Strategie einschränken.\nRisiko: Es wurde untersucht, ob die Gewinner-Aktien risikoreicher sind. Die Ergebnisse zeigten aber, dass die Risikoadjustierung die Überrenditen der Gewinner-Aktien nicht erklären kann.\n\nFolie 21 Die Behavioral Finance erklärt den Erfolg durch systematische Fehler in der Informationsverarbeitung. Es kommt zu einer zu langsamen Anpassung der Erwartungen an neue Informationen.",
    "crumbs": [
      "Fundamentals of Finance",
      "Household Finance"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Diese Seite habe ich erstellt, um meine Mitschriften aus der Uni zu teilen. Falls Euch Fehler auffallen, erstellt gerne ein Issue auf Github.\n\nDas GitHub Repository dieser Seite."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Info",
    "section": "",
    "text": "Diese Seite habe ich erstellt, um meine Mitschriften aus der Uni zu teilen. Falls Euch Fehler auffallen, erstellt gerne ein Issue auf Github.\n\nHier sind ein paar nützliche Links:\n\nDas GitHub Repository dieser Seite\n…"
  },
  {
    "objectID": "ewf/ewf-uebungen.html",
    "href": "ewf/ewf-uebungen.html",
    "title": "EWF Übungen",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gridExtra)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Übungen"
    ]
  },
  {
    "objectID": "ewf/ewf-uebungen.html#übung-2",
    "href": "ewf/ewf-uebungen.html#übung-2",
    "title": "EWF Übungen",
    "section": "Übung 2",
    "text": "Übung 2\nInstallieren Sie in R das Package Ecdat und laden Sie den Datensatz Caschool. Dieser Datensatz enthält Informationen über 420 amerikanische Schulen. Es soll der Zusammenhang zwischen der durchschnittlich erreichten Punktzahl im Test (testscr) und der Anzahl der Schüler pro Lehrer (str) sowie der Anzahl der zur Verfügung stehenden Computer je Schüler (compstu) mittels Regressionsanalyse untersucht werden. a) Berechnen und interpretieren Sie mit Hilfe der Funktionen lm() und coef() den OLS-Schätzer für das Regressionsmodell \\(\\text{testscr}_i = \\beta_1 + \\beta_2 \\cdot \\text{str}_i + \\beta_3 \\cdot \\text{compstu}_i\\) b) Wie verändern sich die Schätzergebnisse, wenn alle Beobachtungen von testscr bzw. str mit dem Faktor 10 multipliziert werden? c) Geben Sie die Effekte der Regressoren aus a) in Standardabweichungen an. Interpretieren Sie die Ergebnisse. d) Verifizieren Sie für \\(\\beta_2\\) das Ergebnis aus Aufgabe 2d).\n\na)\n\n# Wir laden das Package und den Datensatz in `D`\nlibrary(Ecdat)\n\nLoading required package: Ecfun\n\n\n\nAttaching package: 'Ecfun'\n\n\nThe following object is masked from 'package:base':\n\n    sign\n\n\n\nAttaching package: 'Ecdat'\n\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\ndata(Caschool)\nD &lt;- Caschool\n\n# Wir berechnen das Regressionsmodell\nR4 &lt;- lm(testscr ~ str + I(100*compstu), data=D) \ncoef(R4)\n\n     (Intercept)              str I(100 * compstu) \n     676.5829650       -1.5928058        0.6515992 \n\n\nWir können das Modell auch teilweise visualisieren und mit den tatsächlichen Daten vergleichen.\n\nnewdata &lt;- expand_grid(str = seq(from = 14, to = 27, by = 0.1),\n                      compstu = seq(from = 0, to = 0.4, by = 0.01)) %&gt;%\n  mutate(testscr = predict(R4, newdata = .))\n\np1 &lt;- ggplot()+\n  geom_tile(data = newdata, aes(x = str, y = compstu, fill = testscr))+\n  scale_fill_viridis_c()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\np2 &lt;- ggplot()+\n  geom_point(data = D, aes(x = str, y = compstu, color = testscr))+\n  scale_color_viridis_c()+\n  theme_bw()+\n  theme(legend.position = \"top\")\n\ngrid.arrange(p2, p1, nrow = 1)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Übungen"
    ]
  },
  {
    "objectID": "ewf/ewf-uebungen.html#b",
    "href": "ewf/ewf-uebungen.html#b",
    "title": "EWF Übungen",
    "section": "b)",
    "text": "b)\n\nR5 &lt;- lm( 10*testscr ~ str + I(100*compstu), data=D)\ncoef(R5)\n\n     (Intercept)              str I(100 * compstu) \n     6765.829650       -15.928058         6.515992",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "EWF Übungen"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html",
    "href": "ewf/zeitreihenanalyse.html",
    "title": "Zeitreihenanalyse",
    "section": "",
    "text": "Die folgenden Packages wurden für das Erstellen dieses Dokuments verwendet:\nCode\nlibrary(vars)\nlibrary(car)\nlibrary(gridExtra)\nlibrary(tsDyn)\nlibrary(tidyverse)\nlibrary(WDI)\nlibrary(urca)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#ar1-prozess-selbst-gebaut",
    "href": "ewf/zeitreihenanalyse.html#ar1-prozess-selbst-gebaut",
    "title": "Zeitreihenanalyse",
    "section": "AR(1) Prozess selbst gebaut",
    "text": "AR(1) Prozess selbst gebaut\nBei einem Autoregressiven Prozess erster Ordnung AR(1) hängt der Wert der aktuellen Periode \\(y_t\\) von dem Wert der Vorperiode \\(y_{t-1}\\), einer Konstanten \\(\\alpha\\) und einem Störterm \\(u_t\\) ab. Der Einfluss der Vorperiode wird über den Faktor \\(\\rho\\) gesteuert. Liegt dieser zwischen \\(-1\\) und \\(1\\) spricht man von einem Stationären Prozess. Das bedeutet, dass der Prozess immer wieder zu seinem Mittelwert, welcher bei \\(\\frac{\\alpha}{1 - \\rho}\\) liegt, zurückkehrt.\nDie Funktionsvorschrit lautet also \\(y_t = \\alpha + \\rho y_{t-1} + u_t\\) mit \\(|\\rho| &lt; 1\\) und wird in diesem Beispiel simuliert\n\n\nCode\nn &lt;- 150\nalpha &lt;- 0\nrho &lt;- .8\n\nu &lt;- rnorm(n, mean = 0, sd = 5)\ny0 &lt;- numeric(n)\ny0[1] &lt;- 10\n\nfor(t in 2:n){\n  y0[t] &lt;- alpha + rho * y0[t-1] + u[t]\n}\ny &lt;- y0[31:130]\n\n#plot(y0, type = \"l\"); abline(h = alpha / (1 - rho), lty = 2)\nplot(y, type = \"l\"); abline(h = alpha / (1 - rho), lty = 2)\n\n\n\n\n\n\n\n\n\nEin AR(1) Prozess kann auch als MA(\\(\\infty\\)) Prozess dargestellt werden: \\(y_t = \\frac{\\alpha}{1-\\rho}+\\sum_{j=0}^{\\infty}\\rho^j u_{t-j}\\).\nACF & PACF Anhand der Autokorrelationsfunktion (ACF) kann festgestellt werden, welche Lags signifikante Korrelationen aufweisen. Dadruch können Muster und Eigenschaften der Zeitrehe aufgezeigt werden.\nNachfolgend wird zuerst eine selbstberechnete ACF gezeigt und dann der Output der acf() Funktion.\n\n\nCode\nh_max &lt;- 20\n\nacf_list &lt;- numeric(length = h_max)\nfor(h in 1:h_max){\n  acf_list[h] &lt;- cor(y[-c(1:h)], lag(y, n = h)[-c(1:h)])\n}\n\npar(mfrow = c(1, 2))\nplot(acf_list, type = \"h\"); abline(h = 0, lty = 1)\nacf(y) # zum Vergleich\n\n\n\n\n\n\n\n\n\nDie partielle Autokorrelationsfunktion (PACF) zeigt den direkten Einfluss von Periode \\(t-h\\) auf Periode \\(t\\). Die Effekte der dazwischenliegenden Perioden werden herausgerechnet. Bsp.: Eine Korrelation von \\(y_t\\) und \\(y_{t-2}\\) kann zustande kommen, weil \\(y_{t-2}\\) einen direkten Einfluss auf \\(y_t\\) hat, oder weil \\(y_{t-2}\\) einen Einfluss auf \\(y_{t-1}\\) hat welches dann wiederum einen Einfluss auf \\(y_t\\) hat.\nNachfolgend wird zuerst eine selbstberechnete PACF gezeigt und dann der Output der pacf() Funktion.\n\n\nCode\nh_max &lt;- 20\n\npacf_list &lt;- numeric(h_max)\nfor(i in 1:h_max){\n  eq &lt;- as.formula(paste(\"y ~\", paste(\"lag(y,\", 1:i, \")\", collapse = \"+\")))\n  coef &lt;- coef(lm(eq))[i+1]\n  pacf_list[i] &lt;- coef\n}\n\npar(mfrow = c(1, 2))\nplot(pacf_list, type = \"h\"); abline(h = 0, lty = 1)\npacf(y) # Zum Vergleich\n\n\n\n\n\n\n\n\n\nDie PACF wird mit steigendem Lag \\(h\\) immer ungenauer, da ein immer größeres Regressionsmodell berechnet werden muss.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#ma1-prozess-selbst-gebaut",
    "href": "ewf/zeitreihenanalyse.html#ma1-prozess-selbst-gebaut",
    "title": "Zeitreihenanalyse",
    "section": "MA(1) Prozess selbst gebaut",
    "text": "MA(1) Prozess selbst gebaut\nBei einem Moving Average Prozess erster Ordnung MA(1) hängt der Wert der aktuellen Periode \\(y_t\\) von dem Störterm der Vorperiode \\(y_{t-1}\\), einer Kostanten \\(\\mu\\) sowie einem Störterm \\(u_t\\) ab. Der Einfluss der Vorperiode wird über ein \\(\\theta\\) gesteuert. Dieses wird typischerweise zwischen \\(-1\\) und \\(1\\) gewählt. Im Gegensatz zum AR(1) Prozess ergibt sich für \\(|\\theta| &gt; 1\\) jedoch auch ein stationärer Prozess.\n\\(y_t = \\mu + u_t + \\theta u_{t-1}\\) mit \\(|\\theta|&lt;1\\) und \\(\\{u_t\\}\\) als weißem Rauschen\n\n\nCode\nn &lt;- 150\nmu &lt;- 5\ntheta &lt;- 1.8\n\nu &lt;- rnorm(n, mean = 0, sd = 5)\ny0 &lt;- numeric(n)\ny0[1] &lt;- 10\n\nfor(t in 2:n){\n  y0[t] &lt;- mu + u[t] + theta * u[t-1]\n}\ny &lt;- y0[31:130]\n\nplot(y, type = \"l\")\n\n\n\n\n\n\n\n\n\nEin MA(1) Prozess kann auch als AR(\\(\\infty\\)) Prozess dargestellt werden: \\[y_t = \\mu(1 - \\theta + \\theta^2 - \\theta^3 + …) + \\theta y_{t-1} - \\theta^2 y_{t-2} + \\theta^3 y_{t-3} - … + u_t = \\mu\\sum_{j=0}^{\\infty}(-1)^{j}\\theta^j+\\sum_{j=1}^{\\infty}(-1)^{j-1}\\theta^j + u_t\\]\nACF & PACF ACF bricht nach einem Lag ab, PACF klingt exponentiell ab wenn \\(|\\theta|&lt;1\\).\n\n\nCode\npar(mfrow = c(1, 2))\nacf(y)\npacf(y)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#ar2",
    "href": "ewf/zeitreihenanalyse.html#ar2",
    "title": "Zeitreihenanalyse",
    "section": "AR(2)",
    "text": "AR(2)\nBei einem autoregressiven Prozess zweiter Ordnung AR(2) hängt der Wert der aktuellen Periode \\(y_t\\) von den Werten der beiden davorliegenden Perioden \\(y_{t-1}\\) und \\(y_{t-2}\\) ab. Hierbei gilt die Stationaritätsbedingung \\(|\\rho_1 + \\rho_2| &lt; 1\\).\n\\(y_t = \\alpha + \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + u_t\\)\n\n\nCode\nn &lt;- 150\n\nalpha &lt;- 5\nrho1 &lt;- 1.2\nrho2 &lt;- -0.5\n\nu &lt;- rnorm(n, mean = 0, sd = 5)\ny0 &lt;- numeric(n)\ny0[1] &lt;- 0\ny0[2] &lt;- 0\n\nfor(t in 3:n){\n  y0[t] &lt;- alpha + rho1 * y0[t-1] + rho2 * y0[t-2] + u[t]\n}\ny &lt;- y0[31:130]\n\nplot(y, type = \"l\")",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#gdp-deutschland",
    "href": "ewf/zeitreihenanalyse.html#gdp-deutschland",
    "title": "Zeitreihenanalyse",
    "section": "GDP Deutschland",
    "text": "GDP Deutschland\nHier ein Beispiel mit jährlichen Daten des Bruttoinlandsprodukts Deutschlands zwischen 1970 und 2020.\n\n\nCode\n#WDIsearch(\"gdp\")\n#WDIsearch('gdp.*capita.*constant')\n\ngdp_de &lt;- WDI(indicator = \"NY.GDP.PCAP.KD\", country = \"DE\", start = \"1970\", end = \"2020\") %&gt;%\n  select(year, gdp_per_capita = NY.GDP.PCAP.KD) %&gt;%\n  mutate(gdp_per_capita_log = log(gdp_per_capita),\n         diff = gdp_per_capita_log - lag(gdp_per_capita_log, n = 1, order_by = year))\n\ngdp_de %&gt;%\n  select(year, gdp_per_capita, diff) %&gt;%\n  drop_na() %&gt;%\n  pivot_longer(cols = -year) %&gt;%\n  ggplot(aes(x = year, y = value))+\n  geom_line()+\n  facet_wrap(~name, scales = \"free_y\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 2))\nacf(drop_na(gdp_de)$diff)\npacf(drop_na(gdp_de)$diff)\n\n\n\n\n\n\n\n\n\nIn der ACF und PACF lassen sich zwar Korrelationen erkennen, allerdings sind diese alle Werte nicht signifikant. Daher ist keine konkrete Aussage zu etwaigen Konjunkturzyklen zu treffen.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#saisonalität-in-co2-emissions",
    "href": "ewf/zeitreihenanalyse.html#saisonalität-in-co2-emissions",
    "title": "Zeitreihenanalyse",
    "section": "Saisonalität in CO2 emissions",
    "text": "Saisonalität in CO2 emissions\nIm folgenden wird der Datensatz co2 verwendet. Es werden für die Auswertung aus Gründen der Übersichtlichkeit nur die letzten 10 Jahre (= 120 Beobachtungen) betrachtet. Es soll gezeigt werden, welchen Unterschied verschiedene Lagordnungen haben können.\n\n\nCode\nc02_small &lt;- window(co2, start = 1987)\n\n# Set the layout to have 2 rows and 2 columns\npar(mfrow = c(3, 3),mar=c(3,3,3,2), las=.5)\n\n# 1. Zeile - data\nplot(c02_small, main = \"Rohdaten (monatlich)\")\nplot(diff(c02_small, lag = 1), type = \"l\", main = \"Diff (Lag 1 Monat)\", ylab = \"Growthrate\")\nplot(diff(c02_small, lag = 12), main = \"Diff (Lag 12 Monate)\", ylab = \"Growthrate\")\n\n\n# 2. Zeile - ACF\nacf(c02_small, main = NA)\nacf(diff(c02_small, lag = 1), main = NA)\nacf(diff(c02_small, lag = 12), main = NA)\n\n# 3. Zeile - PACF\npacf(c02_small, main = NA)\npacf(diff(c02_small, lag = 1), main = NA)\npacf(diff(c02_small, lag = 12), main = NA)\n\n\n\n\n\n\n\n\n\nDie erste Zeile zeigt jeweils den Plot der Daten, die zweite Zeile die ACF und die dritte Zeile die PACF.\nBei den Rohdaten handelt es sich um eine nichtstationäre Zeitreihe. Dementsprechend ist die ACF auch überall positiv. Bildet man die Differenzen zum Vormonat, so wird die Zeitreihe stationär, zeigt aber immernoch eine ausgeprägte Saisonfigur. Diese spiegeln sich auch in der ACF und PACF wieder. Erst wenn die Differenzen über 12 Monate gebildet werden, erhält man eine stationäre Zeitreihe ohne Saisonfiguren. Die ACF klingt nun regelmäßig ab.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#stationaritätsbedingung",
    "href": "ewf/zeitreihenanalyse.html#stationaritätsbedingung",
    "title": "Zeitreihenanalyse",
    "section": "Stationaritätsbedingung",
    "text": "Stationaritätsbedingung\nDamit eine Zeitreihe stationär ist, müssen die Lösungen der z-Transformation außerhalb des Einheitskreises liegen. Man spricht in diesem Zusammenhang auch von dem Stationaritätsdreieck.\nFür ein AR(1) Modell heißt das, dass für einen stationören Prozess \\(|\\rho| &lt; 1\\) sein muss. Bei einem AR(2) Modell müssen \\(|\\rho_1 + \\rho_2| &lt; 1\\) sein. Es soll hier illustriert werden, was bereits ein kleiner Unterschied in einem der Parameter bewirken kann. Für \\(|\\rho_1 + \\rho_2| = 1\\) tritt der Unit-Root Fall ein, welcher im Plot rechts zu sehen ist. \\(|\\rho_1 + \\rho_2| &gt; 1\\) würden eine explodierende Zeitreihe erzeugen.\n\n\nCode\nn &lt;- 150\n\nalpha &lt;- 5\nrho1a &lt;- 1.2\nrho2a &lt;- -0.3\n\nrho1b &lt;- 1.2\nrho2b &lt;- -0.2\n\nu &lt;- rnorm(n, mean = 0, sd = 5)\ny0a &lt;- numeric(n)\ny0a[1] &lt;- 0\ny0a[2] &lt;- 0\n\ny0b &lt;- numeric(n)\ny0b[1] &lt;- 0\ny0b[2] &lt;- 0\n\nfor(t in 3:n){\n  y0a[t] &lt;- alpha + rho1a * y0a[t-1] + rho2a * y0a[t-2] + u[t]\n  y0b[t] &lt;- alpha + rho1b * y0b[t-1] + rho2b * y0b[t-2] + u[t]\n}\nya &lt;- y0a[31:100]\nyb &lt;- y0b[31:100]\n\n\npar(mfrow = c(1, 2))\nplot(ya, type = \"l\", main = paste(\"Rho1=\", rho1a, \", Rho2=\", rho2a))\nplot(yb, type = \"l\", main = paste(\"Rho1=\", rho1b, \", Rho2=\", rho2b))",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#vorbereitende-datentransformation",
    "href": "ewf/zeitreihenanalyse.html#vorbereitende-datentransformation",
    "title": "Zeitreihenanalyse",
    "section": "Vorbereitende Datentransformation",
    "text": "Vorbereitende Datentransformation\nIm vorherigen Plot ist sehr leicht erkennbar, dass die Zeitreihe nicht stationär ist und eine ausgepägte Saisonalität aufweist. Wie bei fast allen ökonomischen Zeitrehen wird daher zuerst logarithmiert und im Anschluss die 12. Differenzen gebildet. Das Ergebnis ist die logarithmierte Wachstumsrate zum Vorjahresmonat.\n\n\nCode\nAirPass_trans &lt;- AirPassengers %&gt;%\n  log() %&gt;%\n  diff(lag = 12)\n\nAirPass_trans %&gt;%\n  plot()",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#identifikation-p-und-q",
    "href": "ewf/zeitreihenanalyse.html#identifikation-p-und-q",
    "title": "Zeitreihenanalyse",
    "section": "Identifikation p und q",
    "text": "Identifikation p und q\nUm die Ordnungen des AR() und MA() Teils zu finden, können die Autokorrelationsfunktion und Partielle Autokorrelationsfunktion hilfreich sein.\n\n\nCode\npar(mfrow = c(1, 2))\nacf(AirPass_trans)\npacf(AirPass_trans)\n\n\n\n\n\n\n\n\n\nAllerdings kann eine Identifikation über diese Plots subjektiv und generell schwierig sein. Eine bessere Alternative stellt die Identifikation über Informationskriterien dar. Hierbei werden insbesondere die Akaike, Bayesian und Hannah-Quinn Informationskriterien unterschieden. Für diese Auswertung wird das Bayesian Kriterium BIC verwendet.\nDie Informationskriterien verwenden die Residualvarianz eines ARMA(p,q) Modells und korrigieren diese anhand der Modellordnung. Das ist notwendig, da größere Modelle sich besser an die Daten anpassen und somit automatisch zu einer kleineren Residualvarianz führen. Die Modellordnung (p,q), welche das minimale Informationskriterium aufweist, wird ausgewählt.\n\n\nCode\npq_values &lt;- expand.grid(p = 1:10, q = 1:10)\n\nbest_bic &lt;- Inf\nbest_model &lt;- NULL\n\nfor(i in 1:nrow(pq_values)){\n  p &lt;- pq_values[i,1]\n  q &lt;- pq_values[i,2]\n  \n  #print(paste(\"p: \", p, \" q: \", q, sep = \"\"))\n  \n  arma_model &lt;- arima(AirPass_trans, order = c(p, 0, q))\n  bic &lt;- BIC(arma_model)\n  \n  if (bic &lt; best_bic) {\n    best_p &lt;- p\n    best_q &lt;- q\n    best_bic &lt;- bic\n    best_model &lt;- arma_model\n  }\n}\n\npaste(\"Best p: \", best_p, \" | Best q: \", best_q, sep = \"\")\n\n\n[1] \"Best p: 4 | Best q: 2\"\n\n\nCode\n#best_model\n#best_bic\n\n\nFür manche Kombinationen (z.B. p = 7, q = 4) gibt es Warnhinweise. Wieso? Was bedeuten die?\nNach BIC bestes Modell: p = 4, q = 2\nZum Vergleich: Das AIC Kriterium gibt als bestes Modell: p=10, q=7 (Achtung, es wurde auch nur bis p,q=10 untersucht!)",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#parameterschätzung",
    "href": "ewf/zeitreihenanalyse.html#parameterschätzung",
    "title": "Zeitreihenanalyse",
    "section": "Parameterschätzung",
    "text": "Parameterschätzung\nNun werden anhand der vorher bestimmten Modellordnung die Parameter geschätzt. Die Schätzung eines AR() Modells ist dabei ziemlich einfach, da alle Daten vorhanden sind. Die Schätzung eines MA() Modells hingegen ist schwieriger, da die \\(u_t\\)s nicht beobachtet werden, sondern ebenfalls erst geschätzt werden müssen. Dafür wird zuerst ein AR(r = max(p,q)) geschätzt und die Residuen extrahiert. Daraufhin kann das ARMA() Modell geschätzt werden. Eventuell können nun erneut die Residuen extrahiert und für eine neue genauerere Schätzung verwendet werden.\n\n\nCode\nbest_model &lt;- arima(AirPass_trans, order = c(best_p, 0, best_q))\nbest_model\n\n\n\nCall:\narima(x = AirPass_trans, order = c(best_p, 0, best_q))\n\nCoefficients:\n         ar1      ar2     ar3     ar4      ma1     ma2  intercept\n      0.6634  -0.6166  0.2620  0.3338  -0.0859  1.0000     0.1152\ns.e.  0.0843   0.1018  0.0994  0.0834   0.0292  0.0317     0.0169\n\nsigma^2 estimated as 0.001409:  log likelihood = 242.77,  aic = -469.55\n\n\n\nOLS\nAR() Modelle (hier AR(2)) können direkt mit OLS geschätzt werden.\n\n\nCode\ny &lt;- as.numeric(AirPass_trans)[-c(1, 2)]\nX &lt;- cbind(rep(1, length(AirPass_trans)), \n           lag(as.numeric(AirPass_trans), n = 1),\n           lag(as.numeric(AirPass_trans), n = 2))[-c(1, 2),]\n\nsolve(t(X) %*% X) %*% t(X) %*% y\n\n\n           [,1]\n[1,] 0.02604782\n[2,] 0.54876013\n[3,] 0.23656239\n\n\nCode\n# zum Vergleich\ncoef(arima(AirPass_trans, order = c(2, 0, 0)))\n\n\n      ar1       ar2 intercept \n0.5540192 0.2378039 0.1150271 \n\n\nWieso unterscheiden sich die Schätzwerte, insbesondere der Intercpet?\n\n\nML Schätzung\nNeben OLS kann auch eine Maximum-Likelihood Schätzung angewandt werden.\n\\(\\text{loglik} = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum \\left( y - \\text{intercept} - \\text{ar1} \\cdot \\left(0, y_{t-1}, \\ldots, y_{1}\\right) - \\text{ar2} \\cdot \\left(0, 0, y_{t-2}, \\ldots, y_{1}\\right) \\right)^2\\)\n\n\nCode\nar2_loglik &lt;- function(params, y) {\n  intercept &lt;- params[1]\n  ar1 &lt;- params[2]\n  ar2 &lt;- params[3]\n  sigma &lt;- params[4]\n  \n  n &lt;- length(y)\n  loglik &lt;- -n/2 * log(2 * pi * sigma^2) - 1/(2 * sigma^2) * sum((y - intercept - ar1 * c(0, head(y, -1)) - ar2 * c(0, 0, head(y, -2)))^2)\n  \n  return(-loglik)\n}\n\ninitial_values &lt;- c(mean(y), 0, 0, sd(y))\nmle_result &lt;- optim(par = initial_values, fn = ar2_loglik, y = as.numeric(AirPass_trans))\n\nintercept_hat &lt;- mle_result$par[1]\nar1_hat &lt;- mle_result$par[2]\nar2_hat &lt;- mle_result$par[3]\nsigma_hat &lt;- mle_result$par[4]\n\npaste(\"intercept_hat =\", round(intercept_hat, 2), \"| ar1_hat =\", round(ar1_hat, 2), \"| ar2_hat =\", round(ar2_hat, 2), \"| sigma_hat =\", round(sigma_hat, 2))\n\n\n[1] \"intercept_hat = 0.03 | ar1_hat = 0.55 | ar2_hat = 0.23 | sigma_hat = 0.04\"",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#diagnose",
    "href": "ewf/zeitreihenanalyse.html#diagnose",
    "title": "Zeitreihenanalyse",
    "section": "Diagnose",
    "text": "Diagnose\nNun da das Modell geschätzt ist, muss noch überprüft werden, ob es überhaupt sinnvoll erscheint. Dazu können in einem ersten Schritt insbeonsdere die Plots der Residuen und deren Dichte herangezogen werden. Die Residuen sollten sich wie normalverteiltes weißes Rauschen ohne Autokorrelation verhalten. Sollte doch noch Autokorrelation in den Residuen vorhanden sein, so ist wahrscheinlich das Modell falsch spezifiert, denn das Modell sollte ja die gesamte Autokorrelation erklären.\n\n\nCode\npar(mfrow = c(1, 2))\nplot(resid(best_model), main = \"Residuen\", ylab = \"\")\nplot(density(resid(best_model)), main = \"Dichte\", ylab = \"\")\n\n\n\n\n\n\n\n\n\nNeben den teilweise subjektiv einzuschätzenden Plots gibt es auch den Box-Pierce und Ljung-Box Test, welche auf Autokorrelation testen. Dabei gilt die Nullhypothese \\(H_0\\): keine Autokorrelation.\nEin hoher p-Wert in der Ausgabe des Tests bedeutet, dass \\(H_0\\) nicht verworfen werden kann.\n\n\nCode\nBox.test(resid(best_model), lag = 1, type = \"Box-Pierce\")\n\n\n\n    Box-Pierce test\n\ndata:  resid(best_model)\nX-squared = 0.0607, df = 1, p-value = 0.8054\n\n\nCode\nBox.test(resid(best_model), lag = 1, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  resid(best_model)\nX-squared = 0.062091, df = 1, p-value = 0.8032\n\n\nWie zu sehen ist wird bei beiden Tests die Nullhypothese (keine Autokorrelation) nicht verworfen. Es kann also davon ausgegangen werden, dass das Modell korrekt spezifiziert ist und die Autokorrelation in den Daten angemessen erklärt.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#prognose",
    "href": "ewf/zeitreihenanalyse.html#prognose",
    "title": "Zeitreihenanalyse",
    "section": "Prognose",
    "text": "Prognose\nDie Prognose muss nun Schritt für Schritt vorgenommen werden. Da die Daten nur bis \\(y_T\\) vorliegen, muss zuerst \\(y_{T+1}\\) geschätzt werden. Dieser Wert wird dann als bekannt angenommen und nun liegen Daten bis \\(y_{T+1}\\) vor, wodurch \\(y_{T+2}\\) geschätzt werden kann usw.\nDadurch dass für die prognostizierten Werte keine Residuen bestimmt werden können, wird der Einfluss des MA() Teils mit fortschreitender Prognose in die Zukunft immer kleiner. Für \\(h &gt; \\text{max}\\{p,q\\}\\) verschwindet der MA() Teil aus der Prognose.\n\n\nCode\nforecast_values &lt;- predict(best_model, n.ahead = 30)\n\ncombined_data &lt;- ts(c(AirPass_trans, forecast_values$pred), start = start(AirPass_trans), frequency = frequency(AirPass_trans))\ncombined_df &lt;- data.frame(date = time(combined_data), values = as.vector(combined_data))\n\nggplot(combined_df, aes(x = date, y = values)) +\n  geom_line() +\n  geom_line(data = filter(combined_df, date &gt; max(time(AirPass_trans))), color = \"blue\") +\n  labs(title = \"Historische Daten und Prognose\", x = \"Time\", y = \"Value\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nEs ist wichtig zu beachten, dass hier die logaritmierten Wachstumsraten prognostiziert werden. Die in Schritt 1 vorgenommenen Transformationen müssten also rückgängig gemacht werden.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#empirisches-beispiel",
    "href": "ewf/zeitreihenanalyse.html#empirisches-beispiel",
    "title": "Zeitreihenanalyse",
    "section": "Empirisches Beispiel",
    "text": "Empirisches Beispiel\nBei der Vektorautoregression werden mehrere Zeitrehen gleichzeitig betrachtet. Als Beispiel wird hier der Raotbl6 Datensatz verwendet, welcher verschiedene Zeitrehen der US Ökonomie enthalt, darunter das Gross National Product, Unit Labor Costs und den GNP Deflator.\n\nrgnp : Real GNP.\npgnp : Potential real GNP.\nulc : Unit labor cost.\ngdfco : Fixed weight deflator for personal consumption expenditure excluding food and energy.\ngdf : Fixed weight GNP deflator.\ngdfim : Fixed weight import deflator.\ngdfcf : Fixed weight deflator for food in personal consumption expenditure.\ngdfce : Fixed weight deflator for energy in personal consumption expenditure.\n\n\n\nCode\nRaotbl6 &lt;- read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/Raotbl6.csv\", show_col_types = FALSE)\nRaotbl6\n\n\n# A tibble: 123 × 9\n   date        rgnp  pgnp   ulc gdfco   gdf gdfim gdfcf gdfce\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1959-01-01 1606. 1608.  47.5  36.9  37.4  26.9  32.3  23.1\n 2 1959-04-01 1637  1622.  47.5  37.4  37.5  27    32.2  23.4\n 3 1959-07-01 1630. 1636.  48.7  37.6  37.6  27.1  32.4  23.4\n 4 1959-10-01 1643. 1650.  48.8  37.7  37.8  27.1  32.5  23.8\n 5 1960-01-01 1672. 1665.  49.1  37.8  37.8  27.2  32.4  23.8\n 6 1960-04-01 1667. 1679   49.6  38    38    27.4  32.8  23.9\n 7 1960-07-01 1668. 1694.  50    38.1  38.1  27.4  32.9  24.1\n 8 1960-10-01 1654. 1708.  50.2  38.2  38.2  27.2  33.2  24.2\n 9 1961-01-01 1671. 1723.  50.1  38.2  38.2  27.2  33.2  24.2\n10 1961-04-01 1692. 1738.  49.8  38.3  38.2  27.2  33.2  24.2\n# ℹ 113 more rows\n\n\nCode\nRaotbl6 %&gt;%\n  pivot_longer(-date, names_to = \"indicator\") %&gt;%\n  ggplot(aes(x = date, y = value))+\n  geom_line()+\n  facet_wrap(~indicator, scales = \"free_y\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNun soll ein Vektorautoregressionsmodell mit den Variablen rgnp, ulc und gdf berechnet werden.\n\n\nCode\ndaten_var &lt;- Raotbl6 %&gt;%\n  select(date, rgnp, ulc, gdf)\n\ndaten_var_trans &lt;- daten_var %&gt;%\n  mutate(rgnp = c(0, 0, 0, 0, diff(log(rgnp), 4)),\n         ulc = c(0, 0, 0, 0, diff(log(ulc), 4)),\n         gdf = c(0, 0, 0, 0, diff(log(gdf), 4))) %&gt;%\n  filter(date &gt; \"1959-10-01\") %&gt;%\n  select(-date)\n\n# Auswahl des besten Modells\nVARselect(daten_var_trans, lag.max = 10, type = \"const\")$selection\n\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     7      5      1      7 \n\n\nCode\nR &lt;- VAR(daten_var_trans, p = 7)\nsummary(R)\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: rgnp, ulc, gdf \nDeterministic variables: const \nSample size: 112 \nLog Likelihood: 1299.381 \nRoots of the characteristic polynomial:\n0.9528 0.9255 0.9255 0.9025 0.9025 0.8832 0.8832 0.8817 0.8817 0.8738 0.8738 0.8233 0.8233 0.8059 0.8059 0.8053 0.8053 0.7282 0.445 0.445 0.3119\nCall:\nVAR(y = daten_var_trans, p = 7)\n\n\nEstimation results for equation rgnp: \n===================================== \nrgnp = rgnp.l1 + ulc.l1 + gdf.l1 + rgnp.l2 + ulc.l2 + gdf.l2 + rgnp.l3 + ulc.l3 + gdf.l3 + rgnp.l4 + ulc.l4 + gdf.l4 + rgnp.l5 + ulc.l5 + gdf.l5 + rgnp.l6 + ulc.l6 + gdf.l6 + rgnp.l7 + ulc.l7 + gdf.l7 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)    \nrgnp.l1  0.931417   0.133043   7.001 4.42e-10 ***\nulc.l1  -0.131304   0.172623  -0.761  0.44886    \ngdf.l1  -0.150196   0.352155  -0.427  0.67076    \nrgnp.l2 -0.104478   0.196658  -0.531  0.59654    \nulc.l2   0.010892   0.240427   0.045  0.96397    \ngdf.l2   0.004823   0.584643   0.008  0.99344    \nrgnp.l3 -0.235903   0.190814  -1.236  0.21956    \nulc.l3  -0.384017   0.224469  -1.711  0.09057 .  \ngdf.l3   0.594908   0.548686   1.084  0.28115    \nrgnp.l4  0.049052   0.195794   0.251  0.80275    \nulc.l4   0.662939   0.222475   2.980  0.00371 ** \ngdf.l4  -1.097971   0.501966  -2.187  0.03131 *  \nrgnp.l5  0.160943   0.198686   0.810  0.42006    \nulc.l5  -0.341975   0.252484  -1.354  0.17899    \ngdf.l5   0.456388   0.544686   0.838  0.40431    \nrgnp.l6  0.199754   0.211248   0.946  0.34689    \nulc.l6   0.456815   0.280877   1.626  0.10736    \ngdf.l6  -0.381554   0.564097  -0.676  0.50052    \nrgnp.l7 -0.273171   0.143440  -1.904  0.06005 .  \nulc.l7  -0.231916   0.190687  -1.216  0.22708    \ngdf.l7   0.362701   0.326883   1.110  0.27014    \nconst    0.015874   0.005955   2.666  0.00911 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.009993 on 90 degrees of freedom\nMultiple R-Squared: 0.8752, Adjusted R-squared: 0.8461 \nF-statistic: 30.06 on 21 and 90 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation ulc: \n==================================== \nulc = rgnp.l1 + ulc.l1 + gdf.l1 + rgnp.l2 + ulc.l2 + gdf.l2 + rgnp.l3 + ulc.l3 + gdf.l3 + rgnp.l4 + ulc.l4 + gdf.l4 + rgnp.l5 + ulc.l5 + gdf.l5 + rgnp.l6 + ulc.l6 + gdf.l6 + rgnp.l7 + ulc.l7 + gdf.l7 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)    \nrgnp.l1  0.226938   0.104413   2.173 0.032372 *  \nulc.l1   1.122658   0.135476   8.287 1.04e-12 ***\ngdf.l1   0.493846   0.276376   1.787 0.077326 .  \nrgnp.l2  0.017251   0.154340   0.112 0.911255    \nulc.l2  -0.165542   0.188690  -0.877 0.382648    \ngdf.l2  -0.811071   0.458835  -1.768 0.080505 .  \nrgnp.l3  0.178705   0.149753   1.193 0.235876    \nulc.l3   0.308535   0.176166   1.751 0.083286 .  \ngdf.l3   0.233391   0.430615   0.542 0.589162    \nrgnp.l4 -0.393359   0.153661  -2.560 0.012137 *  \nulc.l4  -0.971765   0.174601  -5.566 2.67e-07 ***\ngdf.l4   0.650271   0.393949   1.651 0.102297    \nrgnp.l5  0.457259   0.155931   2.932 0.004264 ** \nulc.l5   0.957830   0.198153   4.834 5.49e-06 ***\ngdf.l5   0.048604   0.427476   0.114 0.909728    \nrgnp.l6 -0.262274   0.165790  -1.582 0.117165    \nulc.l6  -0.442885   0.220436  -2.009 0.047519 *  \ngdf.l6  -0.714731   0.442710  -1.614 0.109932    \nrgnp.l7  0.172265   0.112574   1.530 0.129463    \nulc.l7   0.223518   0.149653   1.494 0.138785    \ngdf.l7   0.182848   0.256542   0.713 0.477851    \nconst   -0.017392   0.004674  -3.721 0.000344 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.007843 on 90 degrees of freedom\nMultiple R-Squared: 0.9574, Adjusted R-squared: 0.9474 \nF-statistic: 96.29 on 21 and 90 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation gdf: \n==================================== \ngdf = rgnp.l1 + ulc.l1 + gdf.l1 + rgnp.l2 + ulc.l2 + gdf.l2 + rgnp.l3 + ulc.l3 + gdf.l3 + rgnp.l4 + ulc.l4 + gdf.l4 + rgnp.l5 + ulc.l5 + gdf.l5 + rgnp.l6 + ulc.l6 + gdf.l6 + rgnp.l7 + ulc.l7 + gdf.l7 + const \n\n         Estimate Std. Error t value Pr(&gt;|t|)    \nrgnp.l1  0.128854   0.042076   3.062  0.00290 ** \nulc.l1   0.109174   0.054594   2.000  0.04854 *  \ngdf.l1   1.345271   0.111373  12.079  &lt; 2e-16 ***\nrgnp.l2 -0.130944   0.062195  -2.105  0.03804 *  \nulc.l2  -0.155607   0.076037  -2.046  0.04363 *  \ngdf.l2  -0.402705   0.184899  -2.178  0.03202 *  \nrgnp.l3  0.123122   0.060347   2.040  0.04426 *  \nulc.l3   0.150668   0.070990   2.122  0.03655 *  \ngdf.l3   0.133933   0.173527   0.772  0.44224    \nrgnp.l4 -0.017453   0.061922  -0.282  0.77870    \nulc.l4  -0.070860   0.070360  -1.007  0.31658    \ngdf.l4  -0.578357   0.158752  -3.643  0.00045 ***\nrgnp.l5  0.070479   0.062836   1.122  0.26501    \nulc.l5   0.171962   0.079851   2.154  0.03395 *  \ngdf.l5   0.574494   0.172262   3.335  0.00124 ** \nrgnp.l6 -0.135630   0.066809  -2.030  0.04530 *  \nulc.l6  -0.134128   0.088830  -1.510  0.13456    \ngdf.l6  -0.048853   0.178401  -0.274  0.78484    \nrgnp.l7  0.066312   0.045364   1.462  0.14729    \nulc.l7   0.016334   0.060307   0.271  0.78713    \ngdf.l7  -0.113633   0.103380  -1.099  0.27462    \nconst   -0.003344   0.001883  -1.775  0.07922 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.00316 on 90 degrees of freedom\nMultiple R-Squared: 0.9851, Adjusted R-squared: 0.9816 \nF-statistic: 283.4 on 21 and 90 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           rgnp        ulc       gdf\nrgnp  9.986e-05 -4.608e-05 5.469e-06\nulc  -4.608e-05  6.151e-05 5.572e-06\ngdf   5.469e-06  5.572e-06 9.988e-06\n\nCorrelation matrix of residuals:\n        rgnp     ulc    gdf\nrgnp  1.0000 -0.5879 0.1732\nulc  -0.5879  1.0000 0.2248\ngdf   0.1732  0.2248 1.0000\n\n\nCode\nroots(R)\n\n\n [1] 0.9527839 0.9254726 0.9254726 0.9025335 0.9025335 0.8832403 0.8832403\n [8] 0.8817101 0.8817101 0.8738234 0.8738234 0.8232729 0.8232729 0.8059344\n[15] 0.8059344 0.8052588 0.8052588 0.7282462 0.4450240 0.4450240 0.3119052\n\n\nMan merkt, dass Vektorautoregression schnell sehr unübersichtlich werden kann.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#kausalität",
    "href": "ewf/zeitreihenanalyse.html#kausalität",
    "title": "Zeitreihenanalyse",
    "section": "Kausalität",
    "text": "Kausalität\nEs soll im Kontext von zwei Zeitreihen \\(x_t\\) und \\(y_t\\) untersucht werden, ob eine Kausalverbindung zwischen diesen besteht. Diese könnte entweder von \\(x\\) nach \\(y\\), umgekehrt oder in beide Richtungen verlaufen. Der Grundgedanke bei der Kausalität in diesem Fall ist, dass die Ursache der Wirkung zeitlich vorausgehen muss.\nFormal kann man schreiben \\(\\text{Var}(y_t|y_{t-1}, y_{t-2}, …, x_{t-1}, x_{t-2}, …) &lt; \\text{Var}(y_t|y_{t-1}, y_{t-2}, …) \\Rightarrow x \\rightarrow y\\). Das bedeutet so viel wie wenn die Prognose von y durch die Einbeziehung der Vergangenheit von x verbessert wird, dann ist x kausal für y.\nFür einen einfachen Fall mit AR(1) Prozessen gilt also \\[\n\\begin{bmatrix} y_{t}\\\\ x_{t} \\end{bmatrix} = \\begin{bmatrix} \\alpha_1\\\\ \\alpha_2 \\end{bmatrix} + \\begin{bmatrix} \\alpha_{11} & \\beta_{11}\\\\ \\beta_{21} & \\alpha_{21} \\end{bmatrix} \\begin{bmatrix} y_{t-1}\\\\ x_{t-1} \\end{bmatrix} + \\begin{bmatrix} u_{1t}\\\\ u_{2t} \\end{bmatrix}\n\\] bzw. \\[\n\\begin{align*}\ny_t &= \\alpha_1 + \\alpha_{11}y_{t-1} + \\beta_{11}x_{t-1} + u_{1t} \\\\\nx_t &= \\alpha_2 + \\beta_{21}y_{t-1} + \\alpha_{21}x_{t-1} + u_{2t}\n\\end{align*}\n\\]\nIm folgenden werden je zwei Zeitreihen mit verschiedenen Koeffizientenmatrizen simuliert. Es wird von einem mittelwertbereinigten Prozess ohne Konstante (\\(\\alpha_1\\), \\(\\alpha_2\\)) ausgegangen.\nDie Gleichung kann natürlich erweitert werden. Denkbar sind hier weitere Lags in die Vergangenheit oder der Einbezug gleichzeitiger Kausalität: \\(y_t = \\alpha_1 + \\alpha_{11}y_{t-1} + \\beta_{10}x_{t} + \\beta_{11}x_{t-1} + u_{1t}\\), analog für \\(x_t\\). Dabei ist der Wert der aktuellen Periode \\(y_t\\) auch vom aktuellen Wert der Periode \\(x_t\\) abhängig (und umgekehrt).\n\n\\(\\begin{bmatrix} \\alpha_{11} & \\beta_{11}\\\\ \\beta_{21} & \\alpha_{21} \\end{bmatrix} = \\begin{bmatrix} 0.7 & 0\\\\ 0 & 0.7 \\end{bmatrix}\\)\\(\\begin{bmatrix} \\alpha_{11} & \\beta_{11}\\\\ \\beta_{21} & \\alpha_{21} \\end{bmatrix} = \\begin{bmatrix} 0.7 & 0\\\\ 0.5 & 0.7 \\end{bmatrix}\\)\\(\\begin{bmatrix} \\alpha_{11} & \\beta_{11}\\\\ \\beta_{21} & \\alpha_{21} \\end{bmatrix} = \\begin{bmatrix} 0.7 & 0\\\\ 0.9 & 0.7 \\end{bmatrix}\\)\\(\\begin{bmatrix} \\alpha_{11} & \\beta_{11}\\\\ \\beta_{21} & \\alpha_{21} \\end{bmatrix} = \\begin{bmatrix} 0.7 & -0.5\\\\ 0.5 & 0.7 \\end{bmatrix}\\)\\(\\begin{bmatrix} \\alpha_{11} & \\beta_{11}\\\\ \\beta_{21} & \\alpha_{21} \\end{bmatrix} = \\begin{bmatrix} 0 & 0\\\\ 0.9 & 0 \\end{bmatrix}\\)\n\n\nIn diesem Beispiel sind beide Zeitreihen unabhängige AR(1) Prozesse.\n\n\nCode\nset.seed(3)\n\nB1 &lt;- rbind(\n  c(0.7, 0.0),\n  c(0.0, 0.7)\n  )\n\nvar &lt;- VAR.sim(B=B1, n=100, include=\"none\")\n\nvar %&gt;%\n  data.frame() %&gt;%\n  rename(\"x\" = X1, \"y\" = X2) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  coord_cartesian(ylim = c(-5, 6))+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nIn diesem Beispiel hat Zeitreihe \\(x\\) einen (geringen) Einfluss auf \\(y\\), aber nicht umgekehrt.\n\n\nCode\nset.seed(3)\n\nB1 &lt;- rbind(\n  c(0.7, 0.0),\n  c(0.5, 0.7)\n  )\n\nvar &lt;- VAR.sim(B=B1, n=100, include=\"none\")\n\nvar %&gt;%\n  data.frame() %&gt;%\n  rename(\"x\" = X1, \"y\" = X2) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  coord_cartesian(ylim = c(-5, 6))+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nIn diesem Beispiel hat Zeitreihe \\(x\\) einen Einfluss auf \\(y\\), aber nicht umgekehrt.\n\n\nCode\nset.seed(3)\n\nB1 &lt;- rbind(\n  c(0.7, 0.0),\n  c(0.9, 0.7)\n  )\n\nvar &lt;- VAR.sim(B=B1, n=100, include=\"none\")\n\nvar %&gt;%\n  data.frame() %&gt;%\n  rename(\"x\" = X1, \"y\" = X2) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  coord_cartesian(ylim = c(-5, 6))+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nIn diesem Beispiel hat Zeitreihe \\(x\\) einen Einfluss auf \\(y\\) und \\(y\\) hat gleichzeitig einen Einfluss auf \\(x\\)\n\n\nCode\nset.seed(3)\n\nB1 &lt;- rbind(\n  c(0.7, -0.5),\n  c(0.5, 0.7)\n  )\n\nvar &lt;- VAR.sim(B=B1, n=100, include=\"none\")\n\nvar %&gt;%\n  data.frame() %&gt;%\n  rename(\"x\" = X1, \"y\" = X2) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  coord_cartesian(ylim = c(-5, 6))+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nIn diesem Fall scheint eine Koeffizientenmatrix mit \\(0.5\\) statt \\(-0.5\\) zu einem nicht-stationären Prozess zu führen.\n\n\nIn diesem Beispiel ist \\(x\\) ein AR(1) Prozess mit \\(\\rho = 0\\) (weißes Rauschen) und hat einen Einfluss auf \\(y\\).\n\n\nCode\nset.seed(3)\n\nB1 &lt;- rbind(\n  c(0.0, 0.0),\n  c(0.9, 0.0)\n  )\n\nvar &lt;- VAR.sim(B=B1, n=100, include=\"none\")\n\nvar %&gt;%\n  data.frame() %&gt;%\n  rename(\"x\" = X1, \"y\" = X2) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  coord_cartesian(ylim = c(-5, 6))+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#granger-test-auf-kausalität",
    "href": "ewf/zeitreihenanalyse.html#granger-test-auf-kausalität",
    "title": "Zeitreihenanalyse",
    "section": "Granger Test auf Kausalität",
    "text": "Granger Test auf Kausalität\nIn einem Modell mit \\[\\begin{align*}\ny_t &= \\alpha_1 + \\alpha_{11}y_{t-1} +  \\alpha_{12}y_{t-2} + \\beta_{11}x_{t-1} + \\beta_{12}x_{t-2} + u_{1t} \\\\\nx_t &= \\alpha_2 + \\beta_{21}y_{t-1} + \\beta_{22}y_{t-2} + \\alpha_{21}x_{t-1} + \\alpha_{22}x_{t-2} + u_{2t}\n\\end{align*}\\] kann nun getestet werden, ob \\(x\\) einen Einfluss auf \\(y\\) hat: \\(H_0: x \\nrightarrow y\\) bzw. \\(H_0: \\beta_{11} = \\beta_{12} = 0\\). Bei verwerfen gilt \\(H_1: x \\rightarrow y\\).\nZuerst generieren wir zwei Zeitreihen \\(x\\) und \\(y\\), wobei \\(x \\rightarrow y\\) aber nicht umgekehrt. Dann werden die (restringierten) Regressionsmodelle aufgestellt. Und mit einem F-Test getestet.\n\n\nCode\nset.seed(1)\n\nn &lt;- 1000\n\nu_x &lt;- rnorm(n, mean = 0, sd = 3)\nu_y &lt;- rnorm(n, mean = 0, sd = 3)\n\nx &lt;- numeric(n)\ny &lt;- numeric(n)\n\nfor(t in 3:n){\n  x[t] &lt;- 0.7*x[t-1] + -0.3*x[t-2] + u_x[t]\n  y[t] &lt;- 0.7*y[t-1] + 0.2*y[t-2] + 0.5*x[t-1] + u_y[t]\n}\n\n#data.frame(x,y) %&gt;%\n#  mutate(t = 1:nrow(.)) %&gt;%\n#  pivot_longer(cols = -t) %&gt;%\n#  ggplot(aes(x = t, y = value, color = name))+\n#  geom_line()+\n#  #coord_cartesian(ylim = c(-5, 6))+\n#  theme_bw()+\n#  theme(legend.position = \"top\",\n#        legend.title = element_blank())\n\ndata &lt;- data.frame(x,y) %&gt;%\n  mutate(x_l1 = lag(x, n = 1),\n         y_l1 = lag(y, n = 1),\n         x_l2 = lag(x, n = 2),\n         y_l2 = lag(y, n = 2)) %&gt;%\n  drop_na()\n\nR1 &lt;- lm(x ~ x_l1 + y_l1 + x_l2 + y_l2, data = data)\nR2 &lt;- lm(y ~ x_l1 + y_l1 + x_l2 + y_l2, data = data)\n\nR1_restricted &lt;- lm(x ~ x_l1 + x_l2, data = data)\nR2_restricted &lt;- lm(y ~ y_l1 + y_l2, data = data)\n\nprint(linearHypothesis(R1, c(\"y_l1 = 0\", \"y_l2 = 0\"), restricted = R1_restricted))\n\n\nLinear hypothesis test\n\nHypothesis:\ny_l1 = 0\ny_l2 = 0\n\nModel 1: restricted model\nModel 2: x ~ x_l1 + y_l1 + x_l2 + y_l2\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    995 9596.5                           \n2    993 9592.3  2    4.2491 0.2199 0.8026\n\n\nCode\nprint(linearHypothesis(R2, c(\"x_l1 = 0\", \"x_l2 = 0\"), restricted = R2_restricted))\n\n\nLinear hypothesis test\n\nHypothesis:\nx_l1 = 0\nx_l2 = 0\n\nModel 1: restricted model\nModel 2: y ~ x_l1 + y_l1 + x_l2 + y_l2\n\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    995 13459.9                                  \n2    993  9643.9  2      3816 196.46 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWie zu erwarten verwirft der F-Test für \\(x\\) die Nullhypothese nicht (p-Wert von \\(0.8\\)). Bei \\(y\\) verwirft der F-Test die Nullhypothese nicht, das heißt \\(y\\) hängt von \\(x\\) ab, bzw. \\(x\\) hat kausalen Einfluss auf \\(y\\).",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#impuls-antwort-analyse",
    "href": "ewf/zeitreihenanalyse.html#impuls-antwort-analyse",
    "title": "Zeitreihenanalyse",
    "section": "Impuls Antwort Analyse",
    "text": "Impuls Antwort Analyse\nEin VAR(p) Modell lässt sich - ähnlich zu einem AR(p) - Modell als unendlicher (Vektor)-Moving-Average Prozess VMA(∞) darstellen: \\[\\mathbf{y}_t = \\mathbf{\\mu} + \\mathbf{u}_t + \\mathbf{B}_1 \\mathbf{u}_{t-1} + \\mathbf{B}_2 \\mathbf{u}_{t-2} + …\\] mit \\[\\mathbf{B}_h = \\begin{bmatrix} \\partial \\mathbf{y}_{1,t+h} / \\partial \\mathbf{u}_{1t} & \\ldots & \\partial \\mathbf{y}_{1,t+h} / \\partial \\mathbf{u}_{mt} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\partial \\mathbf{y}_{m,t+h} / \\partial \\mathbf{u}_{1t} & \\ldots & \\partial \\mathbf{y}_{m,t+h} / \\partial \\mathbf{u}_{mt} \\end{bmatrix}\\]\nDie Einträge können dabei wie folgt interpretiert werden: Zeile \\(i\\), Spalte \\(j\\) gibt den Effekt einer Erhöhung des \\(j\\)-ten Residuums (Schock) um eine Einheit auf die \\(i\\)-te Variable nach \\(h\\) Perioden an.\nZum Berechnen dieser Matrix \\(\\mathbf{B}_h\\) muss zuerst die Koeffizientenmatrix geschätzt werden. Es wird eines der oberen Beispiele mit \\(\\begin{bmatrix} 0.7 & 0 \\\\ 0.9 & 0.7 \\end{bmatrix}\\) verwendet, diesmal jedoch mit \\(n = 1000\\) Beobachtungen.\n\n\nCode\nset.seed(3)\n\nB1 &lt;- rbind(\n  c(0.7, 0.0),\n  c(0.9, 0.7)\n  )\n\nvar &lt;- VAR.sim(B=B1, n=1000, include=\"none\")\ncolnames(var) &lt;- c(\"x\", \"y\")\n\nR &lt;- VAR(var, p = 1, type = \"const\")\nR\n\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation x: \n====================================== \nCall:\nx = x.l1 + y.l1 + const \n\n        x.l1         y.l1        const \n 0.681844193  0.002830106 -0.002314831 \n\n\nEstimated coefficients for equation y: \n====================================== \nCall:\ny = x.l1 + y.l1 + const \n\n       x.l1        y.l1       const \n 0.87657900  0.69214118 -0.01188907 \n\n\nVergleicht man die Schätzwerte mit den tatsächlichen Werten, so werden diese ziemlich gut getroffen.\n\nImpuls Antwort Funktion\n\n\\(x \\rightarrow y\\)\\(y \\rightarrow x\\)\n\n\n\n\nCode\nplot(vars::irf(R, impulse = \"x\", response = \"y\"))\n\n\n\n\n\n\n\n\n\nEine Veränderung der Schocks in \\(x\\) führen zu einer Veränderung in \\(y\\).\n\n\n\n\nCode\nplot(vars::irf(R, impulse = \"y\", response = \"x\"))\n\n\n\n\n\n\n\n\n\nWie zu erwarten verläuft die Kurve der Impuls-Antwort-Funktion von \\(y\\) auf \\(x\\) flach. Eine Veränderung der Schocks in \\(y\\) hat also keinen Einfluss auf den Verlauf von \\(x\\).\n\n\n\n\n\nImpuls Antwort Matrix\nNun kann auch die \\(\\mathbf{B}_h\\) Matrix bestimmt werden. Hier wird \\(h = 0\\) und \\(h = 2\\) benutzt.\n\n\nCode\nB_raw &lt;- vars::irf(R, n.ahead = 5)\n#B_raw$irf\n\nB_0 &lt;- rbind(\n  (B_raw$irf$x)[1,],\n  (B_raw$irf$y)[1,]\n)\nround(B_0, 2)\n\n\n        x    y\n[1,] 1.01 0.00\n[2,] 0.00 0.99\n\n\nCode\nB_2 &lt;- rbind(\n  (B_raw$irf$x)[3,],\n  (B_raw$irf$y)[3,]\n)\nround(B_2, 2)\n\n\n        x    y\n[1,] 0.47 1.21\n[2,] 0.00 0.48\n\n\nDie Matrizen können wie folgt interpretiert werden:\n\n\n\n\nx\ny\n\n\n\n\nx\n\\(x_t \\rightarrow x_{t+h}\\)\n\\(x_t \\rightarrow y_{t+h}\\)\n\n\ny\n\\(y_t \\rightarrow x_{t+h}\\)\n\\(y_t \\rightarrow y_{t+h}\\)\n\n\n\nDie Matrix für \\(h=0\\) ist dabei denkbar einfach: Eine Erhöhung des Schocks der aktuellen Periode um \\(1\\) erhöht \\(x_t\\) aus der aktuellen Periode (bzw. \\(y_t\\)) um \\(\\approx 1\\). Addiert man also \\(1\\) zu \\(x_t\\) ist das Ergebnis \\(x_t + 1\\). Da es keine Interaktionen zwischen \\(x_t\\) und \\(y_t\\) (aus der selben Periode) gibt, sind die Werte hier \\(\\approx 0\\).\nDie Matrix für \\(h=2\\) kann wie folgt erklärt werden: Eine Erhöhung des Schocks in \\(x_t\\) um 1 führt zu einer Erhöhung von \\(x_{t+2}\\) um \\(\\approx 0.49\\). Dies folgt aus der Koeffizientenmatrix bzw. \\(\\rho = 0.7\\) und \\(\\rho^2 = 0.49 \\approx 0.47\\). Das gleiche gilt für \\(y_t\\) und \\(y_{t+2}\\). Die \\(1.2\\) ist der Einfluss auf \\(y_{t+2}\\) bei einer Erhöhung des Residuums bei \\(x_t\\) um \\(1\\). Der Effekt hat sich also verstärkt. Dies ist auch in der Impuls-Antwort-Funktion oben zu sehen, verschindet aber nach einigen Perioden wieder.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#deterministische-vs-stoachstische-trends",
    "href": "ewf/zeitreihenanalyse.html#deterministische-vs-stoachstische-trends",
    "title": "Zeitreihenanalyse",
    "section": "Deterministische vs Stoachstische Trends",
    "text": "Deterministische vs Stoachstische Trends\nIm folgenden wird ein AR(1) Prozess mit deterministischen Trend (\\(y_t = \\mu + \\delta t + \\rho y_{t-1} + u_t\\), \\(|\\rho| &lt; 1\\)) und ein Unit Root Prozess (\\(y_t = \\delta + y_{t-1} + u_t\\)) simuliert. Dabei werden jeweils die gleichen Störterme/Schocks \\(u\\) verwendet.\n\n\nCode\nset.seed(1)\n\nn &lt;- 200\nu &lt;- rnorm(n, mean = 0, sd = 2)\n\ny_det &lt;- numeric(n)\ny_stoch &lt;- numeric(n)\n\nfor(t in 2:n){\n  y_det[t] &lt;- 0.1*t + 0.5*y_det[t-1] + u[t]\n  y_stoch[t] &lt;- 0.2 + y_stoch[t-1] + u[t]\n}\n\ndata.frame(y_det, y_stoch) %&gt;%\n  rename(\"Stochastisch\" = y_stoch, \"Deterministisch\" = y_det) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#df-adf-test",
    "href": "ewf/zeitreihenanalyse.html#df-adf-test",
    "title": "Zeitreihenanalyse",
    "section": "DF & ADF Test",
    "text": "DF & ADF Test\nBeim Dickey-Fuller-Test (DF-Test) wird das Modell \\(\\Delta y_t = \\phi y_{t-1}\\) mit OLS geschätzt. Im Anschluss wird die Nullhypothese \\(H_0: \\phi = 0\\) vs \\(H_1: \\phi &lt; 0\\) getestet. Für den Fall \\(\\phi = 0\\) liegt ein Unit-Root Prozess vor, da in diesem Fall die Wirkung vergangener Schocks nicht kleiner wird.\nDer augmented Dickey-Fuller-Test (ADF-Test) werden zusätzlich die Einflüssen autokorrelierter Residuen herausgerechnet.\n\n\nCode\nset.seed(3)\n\nn &lt;- 200\nu &lt;- rnorm(n, mean = 0, sd = 10)\ny &lt;- numeric(n)\n\nfor(t in 2:n){\n  y[t] &lt;- 1 + y[t-1] + u[t]\n}\n\nplot(y, type = \"l\")\n\n\n\n\n\n\n\n\n\nCode\nR &lt;- ur.df(y, type = \"drift\", lags = 1, selectlags = \"Fixed\")\nsummary(R)\n\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression drift \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.0397  -7.6032   0.6935   7.0213  27.0116 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.661137   1.109756   1.497    0.136\nz.lag.1     -0.004556   0.008304  -0.549    0.584\nz.diff.lag   0.021567   0.071879   0.300    0.764\n\nResidual standard error: 9.887 on 195 degrees of freedom\nMultiple R-squared:  0.001871,  Adjusted R-squared:  -0.008366 \nF-statistic: 0.1828 on 2 and 195 DF,  p-value: 0.8331\n\n\nValue of test-statistic is: -0.5487 1.5678 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau2 -3.46 -2.88 -2.57\nphi1  6.52  4.63  3.81\n\n\nAchtung: Die p-Werte dürfen nicht verwendet werden. Stattdessen müssen die t-Werte mit den kritischen Werten am Ende des Outputs verglichen werden. Ist der t-Wert betragsmäßig größer als der kritische Wert, so wird die Nullhypothese (Unit-Root-Prozess) verworfen.\nWertden beim ADF-Test zu viele Lags mit einbezogen, so erhöht sich die Varianz der Schätzung und der Test verliert an Trennschärfe. In anderen Worten, wenn zu viele Lags in den ADF-Test einbezogen werden, wird die Nullhypothese eher akzeptiert, selbst wenn sie falsch ist (d.h.Wahrscheinlichkeit eines Fehlers 2. Art wird größer).Werden hingegen zu wenige Lags einbezogen, so ist die Schätzung verzerrt und der Test invalide.\nIn diesem Fall wird die korrekte Anzahl an Lags einbezogen und \\(H_0: \\phi = 0\\) (Unit-Root-Fall) wird nicht verworfen, da \\(|-0.549| &lt; |-2.88|\\).",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#nichtstationarität-trotz-ey-0",
    "href": "ewf/zeitreihenanalyse.html#nichtstationarität-trotz-ey-0",
    "title": "Zeitreihenanalyse",
    "section": "Nichtstationarität trotz E(y) = 0",
    "text": "Nichtstationarität trotz E(y) = 0\nEin Unit-Root-Prozess kann als MA(∞) geschrieben werden: \\(x_t = x_0 + u_t + u_{t-1} + … + u_1\\). Das stellt die Summe aller vergangener Schocks da. Jedes \\(u\\) hat den Erwartungswert \\(E(u_i) = 0\\). Lässt man nun \\(x_0\\) weg, kann man auch schreiben \\(E(x_t) = \\sum_{i=1}^{t} E(u_i) = 0\\). Das sieht ja erstmal nach Stationarität aus. Man muss allerdings auch die Varianz betrachten. Mit \\(Var(u_i) = \\sigma^2\\) und dem Fakt dass die Schocks iid verteilt sind folgt \\[\n\\begin{align*}\nVar(x_t) &= Var(u_t) + Var(u_{t-1}) + … + Var(u_1) \\\\\n&= \\sum_{i=1}^{t} Var(u_i) \\\\\n&= t\\sigma^2\n\\end{align*}\n\\] Somit hängt die Varianz also vom Zeitpunkt ab.\nSimuliert sieht das wie folgt aus:\n\n\nCode\nset.seed(1)\n\nn &lt;- 100\nu &lt;- numeric(n)\n\nfor(t in 1:n){\n  u[t] &lt;- rnorm(1, mean = 0, sd = t)\n}\n\ny &lt;- cumsum(u)\n\ndata.frame(y) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  ggplot(aes(x = t, y = y))+\n  geom_line()+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nNoch einmal ausdrücklich: Dieser Prozess entsteht rein aus Residuen / Schocks mit einem Erwartungswert von Null!",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#darstellung-als-differenz",
    "href": "ewf/zeitreihenanalyse.html#darstellung-als-differenz",
    "title": "Zeitreihenanalyse",
    "section": "Darstellung als Differenz",
    "text": "Darstellung als Differenz\nSimulieren von \\(\\phi\\) in \\(\\Delta y_t = \\phi y_{t-1} + u_t\\)\n\n\\(\\phi &lt; 0\\)\\(\\phi = 0\\)\n\n\n\n\nCode\nset.seed(1)\n\nn &lt;- 200\ny &lt;- numeric(n)\nu &lt;- rnorm(n, mean = 0, sd = 1)\n\nphi &lt;- -0.2\n\nfor(t in 2:n){\n  delta &lt;- phi*y[t-1] + u[t]\n  y[t] = y[t-1] + delta\n}\n\nplot(y, type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(1)\n\nn &lt;- 200\ny &lt;- numeric(n)\nu &lt;- rnorm(n, mean = 0, sd = 1)\n\nphi &lt;- 0\n\nfor(t in 2:n){\n  delta &lt;- phi*y[t-1] + u[t]\n  y[t] = y[t-1] + delta\n}\n\nplot(y, type = \"l\")",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#i2",
    "href": "ewf/zeitreihenanalyse.html#i2",
    "title": "Zeitreihenanalyse",
    "section": "I(2)",
    "text": "I(2)\nDie meisten hier behandelten Zeitreihen sind vom Integrationsgrad I(1), d.h. sie müssen einmal differenziert werden um eine stationäre Zeitreihe zu bilden. I(2) Prozesse hingegeben benötigen eine zweifache Differenzierung zur Stationarität.\n\n\nCode\nn &lt;- 100\n\nu &lt;- rnorm(n, mean = 0, sd = 3)\ny &lt;- cumsum(u + 1)\n\ny2 &lt;- cumsum(y + 1)\n\ny3 &lt;- diff(y2, differences = 2) # Zweifache Differenzierung\n\npar(mfrow = c(1, 3))\nplot(y, type = \"l\")\nplot(y2, type = \"l\")\nplot(y3, type = \"l\")",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#scheinkorrelation",
    "href": "ewf/zeitreihenanalyse.html#scheinkorrelation",
    "title": "Zeitreihenanalyse",
    "section": "Scheinkorrelation",
    "text": "Scheinkorrelation\nScheinregressionen können schnell entstehen, wenn zwei nichtstationäre Zeitreihen (Unit-Root-Prozesse) aufeinander regressiert werden. In diesem Beispiel werden zwei Zeitreihen x und y vollkommen unaghängig voneinander simuliert.\n\n\nCode\nset.seed(3)\n\nn &lt;- 100\n\nu1 &lt;- rnorm(n, mean = 0, sd = 3)\nu2 &lt;- rnorm(n, mean = 0, sd = 3)\n\ny &lt;- numeric(n)\nx &lt;- numeric(n)\n\nfor(t in 2:n){\n  y[t] &lt;- .5 + y[t-1] + u1[t]\n  x[t] &lt;- .5 + x[t-1] + u2[t]\n}\n\ndata.frame(x, y) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\nR &lt;- lm(y~x)\nsummary(R)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.634  -8.322  -2.467   8.524  26.270 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.44788    1.68546   0.859    0.392    \nx            0.79921    0.05384  14.845   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11 on 98 degrees of freedom\nMultiple R-squared:  0.6922,    Adjusted R-squared:  0.6891 \nF-statistic: 220.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nEine Regression findet trotzdem einen stark signifikanten Zusammenhang.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  },
  {
    "objectID": "ewf/zeitreihenanalyse.html#kointegration-simuliert",
    "href": "ewf/zeitreihenanalyse.html#kointegration-simuliert",
    "title": "Zeitreihenanalyse",
    "section": "Kointegration simuliert",
    "text": "Kointegration simuliert\nBei Kointegration kann eine Zeitreihe als eine Linearkombination von anderen Zeitreihen dargestellt werden. Im einfachsten Fall mit zwei Zeitreihen kann dies wie folgt aussehen.\n\n\nCode\nset.seed(3)\n\nn &lt;- 100\n\nu1 &lt;- rnorm(n, mean = 0, sd = 3)\nu2 &lt;- rnorm(n, mean = 0, sd = 5)\n\ny &lt;- numeric(n)\nx &lt;- numeric(n)\n\nfor(t in 2:n){\n  x[t] &lt;- .5 + x[t-1] + u[t]\n}\n\ny &lt;- 5 + 2*x + u2\n\ndata.frame(x,y) %&gt;%\n  mutate(t = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = -t) %&gt;%\n  ggplot(aes(x = t, y = value, color = name))+\n  geom_line()+\n  theme_bw()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nEs ist leicht erkennbar, dass y in einem gewissen Rahmen x folgt.",
    "crumbs": [
      "Empirische Wirtschaftsforschung",
      "Zeitreihenanalyse"
    ]
  }
]